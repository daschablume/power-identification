{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from models import NeuralNetwork, TrainConfig, evaluate_nn_model, save_model, load_model, plot_results\n",
    "from utils import load_data, split_data, encode_data, mapping_dict\n",
    "from pathlib import Path\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(\"Device: cuda\")\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    print(\"Device: cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data encoder...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# train_encoder = TfidfVectorizer(sublinear_tf=True, analyzer=\"char\", ngram_range=(1,3))\u001b[39;00m\n\u001b[1;32m     24\u001b[0m train_encoder \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_raw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrepare data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m encode_data(train_raw, train_encoder)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2063\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2058\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2059\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2060\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2061\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2062\u001b[0m )\n\u001b[0;32m-> 2063\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1374\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m             )\n\u001b[1;32m   1372\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1374\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1377\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1261\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1260\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1262\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1263\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:112\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    110\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# POC\n",
    "#%%\n",
    "file_list = [\n",
    "    'power-gb-train.tsv',\n",
    "    # 'power-ua-train.tsv',\n",
    "    # 'power-fr-train.tsv',\n",
    "    # 'power-nl-train.tsv',\n",
    "]\n",
    "\n",
    "full_data = load_data(folder_path=\"data/train/power/\", file_list=file_list,text_head='text_en')\n",
    "train_raw, test_raw = split_data(full_data, test_size=0.2, random_state=0)\n",
    "\n",
    "file_list = [\n",
    "    'power-gb-test.tsv',\n",
    "    # 'power-ua-train.tsv',\n",
    "    # 'power-fr-train.tsv',\n",
    "    # 'power-nl-train.tsv',\n",
    "]\n",
    "\n",
    "test_data = load_data(folder_path=\"data/test/power/\", file_list=file_list,text_head='text_en')\n",
    "\n",
    "print(\"Prepare data encoder...\")\n",
    "# train_encoder = TfidfVectorizer(sublinear_tf=True, analyzer=\"char\", ngram_range=(1,3))\n",
    "train_encoder = TfidfVectorizer(max_features=10000)\n",
    "train_encoder.fit(train_raw.texts)\n",
    "\n",
    "print(\"Prepare data...\")\n",
    "train_dataset = encode_data(train_raw, train_encoder)\n",
    "test_dataset = encode_data(test_raw, train_encoder)\n",
    "\n",
    "print(\"Train model\")\n",
    "models_dir = Path('models')\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "model_nn = NeuralNetwork(\n",
    "    input_size=len(train_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "if Path('models/model_nn.pt').exists():\n",
    "    model_nn = load_model(model_nn, 'model_nn')\n",
    "else:\n",
    "    model_nn.fit(train_dataloader, train_config)\n",
    "    save_model(model_nn, \"model_nn\")\n",
    "\n",
    "model_nn_results = evaluate_nn_model(model_nn, test_dataset)\n",
    "np.save('models/model_nn_results.npy', model_nn_results)\n",
    "print(model_nn_results)\n",
    "\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn, train_config, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power-at-train (0.758832573890686, 0.5467289686203003, 0.5984655022621155)\n",
      "power-ba-train (0.8353909254074097, 0.9308755993843079, 0.9099099040031433)\n",
      "power-be-train (0.540569007396698, 0.39673912525177, 0.5011441707611084)\n",
      "power-bg-train (0.6343283653259277, 0.48170730471611023, 0.5632798671722412)\n",
      "power-cz-train (0.6032568216323853, 0.457337886095047, 0.5)\n",
      "power-dk-train (0.6287744045257568, 0.5846599340438843, 0.6590538620948792)\n",
      "power-es-ct-train (0.7605177760124207, 0.8916256427764893, 0.8302752375602722)\n",
      "power-es-ga-train (0.6823529601097107, 0.8928571343421936, 0.7352941036224365)\n",
      "power-es-pv-train (0.6978723406791687, 0.5773195624351501, 0.6120218634605408)\n",
      "power-es-train (0.7452702522277832, 0.7231897115707397, 0.8071611523628235)\n",
      "power-fi-train (0.5883293151855469, 0.5476190447807312, 0.537286639213562)\n",
      "power-fr-train (0.699592649936676, 0.35103243589401245, 0.4465290904045105)\n",
      "power-gb-train (0.7102324366569519, 0.7643880248069763, 0.7466350197792053)\n",
      "power-gr-train (0.8299845457077026, 0.8501326441764832, 0.853528618812561)\n",
      "power-hr-train (0.6590038537979126, 0.4323641061782837, 0.4899713397026062)\n",
      "power-hu-train (0.7833655476570129, 0.6521739363670349, 0.7466063499450684)\n",
      "power-it-train (0.734215259552002, 0.359375, 0.48364487290382385)\n",
      "power-lv-train (0.6348684430122375, 0.39316239953041077, 0.4532019793987274)\n",
      "power-nl-train (0.6361386179924011, 0.3384379744529724, 0.42912620306015015)\n",
      "power-pl-train (0.639352560043335, 0.5043706297874451, 0.6181039214134216)\n",
      "power-pt-train (0.7231980562210083, 0.4203389883041382, 0.5204616785049438)\n",
      "power-rs-train (0.781697154045105, 0.4863731563091278, 0.5858585834503174)\n",
      "power-si-train (0.7072637677192688, 0.2700348496437073, 0.36643025279045105)\n",
      "power-tr-train (0.835273027420044, 0.8069053888320923, 0.8205461502075195)\n",
      "power-ua-train (0.7176901698112488, 0.3505154550075531, 0.43589743971824646)\n",
      "power-at-train (0.7692780494689941, 0.37476634979248047, 0.5164198279380798)\n",
      "power-ba-train (0.751028835773468, 0.8018433451652527, 0.8518971800804138)\n",
      "power-be-train (0.5426765084266663, 0.3550724685192108, 0.47457626461982727)\n",
      "power-bg-train (0.579104483127594, 0.2301829308271408, 0.34872978925704956)\n",
      "power-cz-train (0.6313841342926025, 0.3464163839817047, 0.4491150379180908)\n",
      "power-dk-train (0.6083481311798096, 0.4862518012523651, 0.6037735939025879)\n",
      "power-es-ct-train (0.7896440029144287, 0.866995096206665, 0.8441246747970581)\n",
      "power-es-ga-train (0.6764705777168274, 0.9285714030265808, 0.7393364906311035)\n",
      "power-es-pv-train (0.7021276354789734, 0.876288652420044, 0.7083333134651184)\n",
      "power-es-train (0.7310810685157776, 0.684692919254303, 0.7896406054496765)\n",
      "power-fi-train (0.636290967464447, 0.24725274741649628, 0.3724137842655182)\n",
      "power-fr-train (0.7041751742362976, 0.29203540086746216, 0.405322402715683)\n",
      "power-gb-train (0.7174766063690186, 0.7887057662010193, 0.7571984529495239)\n",
      "power-gr-train (0.7581143975257874, 0.6140583753585815, 0.7473769187927246)\n",
      "power-hr-train (0.6762452125549316, 0.29962074756622314, 0.41217392683029175)\n",
      "power-hu-train (0.7485493421554565, 0.5415019989013672, 0.6782178282737732)\n",
      "power-it-train (0.7366205453872681, 0.3038194477558136, 0.44416242837905884)\n",
      "power-lv-train (0.6184210777282715, 0.18803419172763824, 0.2750000059604645)\n",
      "power-nl-train (0.6379950642585754, 0.2848392128944397, 0.3887147307395935)\n",
      "power-pl-train (0.6039453744888306, 0.3723776340484619, 0.5211009383201599)\n",
      "power-pt-train (0.7328891754150391, 0.4576271176338196, 0.5504587292671204)\n",
      "power-rs-train (0.7587354183197021, 0.28721174597740173, 0.43047916889190674)\n",
      "power-si-train (0.7187329530715942, 0.203832745552063, 0.31241655349731445)\n",
      "power-tr-train (0.8281109929084778, 0.6668797731399536, 0.7836213111877441)\n",
      "power-ua-train (0.7208982706069946, 0.17820324003696442, 0.2843713164329529)\n",
      "power-at-train.tsv: Positive 41.17%\n",
      "power-ba-train.tsv: Positive 83.17%\n",
      "power-be-train.tsv: Positive 52.57%\n",
      "power-bg-train.tsv: Positive 47.17%\n",
      "power-cz-train.tsv: Positive 52.25%\n",
      "power-dk-train.tsv: Positive 62.81%\n",
      "power-es-ct-train.tsv: Positive 65.18%\n",
      "power-es-ga-train.tsv: Positive 57.50%\n",
      "power-es-pv-train.tsv: Positive 56.35%\n",
      "power-es-train.tsv: Positive 70.73%\n",
      "power-fi-train.tsv: Positive 44.59%\n",
      "power-fr-train.tsv: Positive 37.04%\n",
      "power-gb-train.tsv: Positive 56.39%\n",
      "power-gr-train.tsv: Positive 62.70%\n",
      "power-hr-train.tsv: Positive 39.73%\n",
      "power-hu-train.tsv: Positive 40.89%\n",
      "power-it-train.tsv: Positive 37.49%\n",
      "power-lv-train.tsv: Positive 33.05%\n",
      "power-nl-train.tsv: Positive 41.50%\n",
      "power-pl-train.tsv: Positive 54.80%\n",
      "power-pt-train.tsv: Positive 41.32%\n",
      "power-rs-train.tsv: Positive 27.09%\n",
      "power-si-train.tsv: Positive 37.48%\n",
      "power-tr-train.tsv: Positive 51.38%\n",
      "power-ua-train.tsv: Positive 31.21%\n"
     ]
    }
   ],
   "source": [
    "# Mass testing all countries\"s English text\n",
    "\n",
    "parent_dir = Path(\"data/train/power\")\n",
    "\n",
    "file_list = sorted([file for file in parent_dir.glob(\"*.tsv\")])\n",
    "text_en_result_list = []\n",
    "\n",
    "for file in file_list:\n",
    "\n",
    "    full_data = load_data(folder_path=parent_dir, file_list=[file.name],text_head=\"text_en\")\n",
    "    train_dev_raw, test_raw = split_data(full_data, test_size=0.2, random_state=0)\n",
    "    train_raw, dev_raw = split_data(train_dev_raw, test_size=0.2, random_state=0)\n",
    "\n",
    "    # train_encoder = TfidfVectorizer(sublinear_tf=True, analyzer=\"char\", ngram_range=(1,3))\n",
    "    train_encoder = TfidfVectorizer()\n",
    "    train_encoder.fit(train_raw.texts)\n",
    "\n",
    "    train_dataset = encode_data(train_raw, train_encoder)\n",
    "    dev_dataset = encode_data(dev_raw, train_encoder)\n",
    "    test_dataset = encode_data(test_raw, train_encoder)\n",
    "\n",
    "    train_config = TrainConfig(\n",
    "        num_epochs      = 10,\n",
    "        early_stop      = False,\n",
    "        violation_limit = 5,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "    model_nn = NeuralNetwork(\n",
    "        input_size=len(train_encoder.vocabulary_),\n",
    "        hidden_size=128,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    if Path(f\"models/model_nn_{file.stem}_en.pt\").exists():\n",
    "        model_nn = load_model(model_nn, f\"model_nn_{file.stem}_en\")\n",
    "    else:\n",
    "        model_nn.fit(train_dataloader, train_config)\n",
    "        save_model(model_nn, f\"model_nn_{file.stem}_en\")\n",
    "\n",
    "    model_nn_results = evaluate_nn_model(model_nn, test_dataset)\n",
    "    text_en_result_list.append(model_nn_results)\n",
    "    \n",
    "    np.save(f\"models/model_nn_{file.stem}_en_results.npy\", model_nn_results)\n",
    "    print(file.stem, model_nn_results)\n",
    "\n",
    "\n",
    "# Mass testing all countries's original text\n",
    "\n",
    "parent_dir = Path(\"data/train/power\")\n",
    "\n",
    "file_list = sorted([file for file in parent_dir.glob(\"*.tsv\")])\n",
    "text_ori_result_list = []\n",
    "\n",
    "for file in file_list:\n",
    "\n",
    "    full_data = load_data(folder_path=parent_dir, file_list=[file.name],text_head=\"text\")\n",
    "    train_dev_raw, test_raw = split_data(full_data, test_size=0.2, random_state=0)\n",
    "    train_raw, dev_raw = split_data(train_dev_raw, test_size=0.2, random_state=0)\n",
    "\n",
    "    # train_encoder = TfidfVectorizer(sublinear_tf=True, analyzer=\"char\", ngram_range=(1,3))\n",
    "    train_encoder = TfidfVectorizer()\n",
    "    train_encoder.fit(train_raw.texts)\n",
    "\n",
    "    train_dataset = encode_data(train_raw, train_encoder)\n",
    "    dev_dataset = encode_data(dev_raw, train_encoder)\n",
    "    test_dataset = encode_data(test_raw, train_encoder)\n",
    "\n",
    "    train_config = TrainConfig(\n",
    "        num_epochs      = 10,\n",
    "        early_stop      = False,\n",
    "        violation_limit = 5,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "    model_nn = NeuralNetwork(\n",
    "        input_size=len(train_encoder.vocabulary_),\n",
    "        hidden_size=128,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    if Path(f\"models/model_nn_{file.stem}_ori.pt\").exists():\n",
    "        model_nn = load_model(model_nn, f\"model_nn_{file.stem}_ori\")\n",
    "    else:\n",
    "        model_nn.fit(train_dataloader, train_config)\n",
    "        save_model(model_nn, f\"model_nn_{file.stem}_ori\")\n",
    "\n",
    "    model_nn_results = evaluate_nn_model(model_nn, test_dataset)\n",
    "    text_ori_result_list.append(model_nn_results)\n",
    "    \n",
    "    np.save(f\"models/model_nn_{file.stem}_ori_results.npy\", model_nn_results)\n",
    "    print(file.stem, model_nn_results)\n",
    "\n",
    "\n",
    "# Detect class imbalance\n",
    "parent_dir = Path(\"data/train/power\")\n",
    "\n",
    "file_list = sorted([file for file in parent_dir.glob(\"*.tsv\")])\n",
    "stats = []\n",
    "\n",
    "for file in file_list:\n",
    "    full_data = load_data(folder_path=parent_dir, file_list=[file.name],text_head=\"text\")\n",
    "    positive = sum(full_data.labels)\n",
    "    stats.append((positive, len(full_data), positive / len(full_data)))\n",
    "    print(f\"{file.name}: Positive {positive / len(full_data) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance of the two languages\n",
    "\n",
    "def plot_countries(country_group):\n",
    "\n",
    "    for file, en ,ori, stat in zip(file_list, text_en_result_list, text_ori_result_list, stats):\n",
    "        data = [\n",
    "            ('en', stat[2], *en),\n",
    "            ('ori', stat[2], *ori)\n",
    "        ]\n",
    "\n",
    "        country_code = file.stem.replace('power-', '').replace('-train', '')\n",
    "        country_name = mapping_dict[country_code]\n",
    "\n",
    "        if country_code in country_group:\n",
    "\n",
    "            results_df = pd.DataFrame(data, columns=[\"language\", \"positive_pct\", \"precision\", \"recall\", 'f1']).melt(id_vars=\"language\")\n",
    "\n",
    "            result_chart = alt.Chart(results_df).mark_bar().encode(\n",
    "                x = alt.X('variable:N', axis = alt.Axis(title = '', labels = False, ticks = False), sort = None, ),\n",
    "                y = alt.Y('value:Q', axis = alt.Axis(title = 'Score'), scale=alt.Scale(domain=(0, 1))),\n",
    "                column=alt.Column('language:N', title='Language', sort = None),\n",
    "                color=alt.Color('variable:N', scale=alt.Scale(scheme='category20'), title='Evaluation Metric', sort = None)\n",
    "            ).properties(\n",
    "                width=200,\n",
    "                height=300,\n",
    "                title = f\"{country_code} - {country_name} - {stat[1]} datapoints\"\n",
    "            )\n",
    "\n",
    "            result_chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result summary\n",
    "- Austria: lots of data, good precision, low recall. Worse performance on original language\n",
    "- Bosnia: Not much data, good results overall\n",
    "- Belgium: medium data, bad result\n",
    "- Bulgaria: medium data, bad\n",
    "- Czechia: medium data, Bad results\n",
    "- Denmark: medium data, bad result\n",
    "- Catalonia: less data, result quite good, precision < recall. EN and ORI are comparable\n",
    "- Galacia: same with Catalonia\n",
    "- Basque: Same with others, but worse result using EN\n",
    "- Spain: Medium-large data, everything is balanced around 0.7 - 0.8, similar perofrmance in both lang. Lots of positive labels\n",
    "- Finland: medium data, overall bad performance, lower performance in ORI\n",
    "- France: medium-large data, very low recall -> cannot capture negative class. Similar performance in both lang\n",
    "- GB: large data, balance results\n",
    "- Greece: Medium data, good result on en text\n",
    "- Croatia: Large data, bad results on both. Slightly better in English\n",
    "- Hungary: less data, quite good results on EN\n",
    "- Italy: Medium data, good precision, bad recall, low positive percentage\n",
    "- Latvia: Not much data, bad result\n",
    "- Netherlands: medium data, slight class imbalance, overall bad result\n",
    "- Poland: class balance, result is bad / so-so\n",
    "- Portugal: medium data, Slight class imbalance, high precision but low recall\n",
    "- Serbia: large data, high class imbalance, high precision, low recall\n",
    "- Slovenia: Medium-large data ,high class imbalance, high precision, low recall\n",
    "- Turkey: Large data, balance class, good overall result on English\n",
    "- Ukraine: medium-large data, high class imbalance, high precision, low recall\n",
    "\n",
    "- Low positive score: good precision, bad recall (true positive / total positive) -> can easily capture true positive and negative by guessing all to be negative, so miss lots of true positive -> the effect of class imbalance\n",
    "\n",
    "- Less data = better result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Test by country groups\n",
    "\n",
    "- Balkan\n",
    "  - Bosnia and Herzegovina (ba)\n",
    "  - Croatia (hr)\n",
    "  - Serbia (rs)\n",
    "\n",
    "- Diff\n",
    "  - Greece (gr)\n",
    "  - Bulgaria (bg)\n",
    "\n",
    "- Spanish\n",
    "  - Spain (es)\n",
    "  - Catalonia (es-ct)\n",
    "  - Galicia (es-ga)\n",
    "  - Basque Country (es-pv) [only power]\n",
    "\n",
    "- Nordic\n",
    "  - Denmark (dk) \n",
    "  - Finland (fi)\n",
    "  - Iceland (is) [only political orientation] \n",
    "  - Norway (no) [only political orientation] \n",
    "  - Sweden (se) [only political orientation] \n",
    "\n",
    "- Slavic\n",
    "  - Poland (pl)\n",
    "  - Ukraine (ua)\n",
    "  - Czechia (cz)\n",
    "  - Serbia (rs)\n",
    "  - Slovenia (si)\n",
    "\n",
    "- West German\n",
    "  - Austria (at)\n",
    "  - Great Britain (gb)\n",
    "  - The Netherlands (nl)\n",
    "  - Norway (no) [only political orientation] \n",
    "  - Sweden (se) [only political orientation] \n",
    "  - Belgium (be)\n",
    "\n",
    "\n",
    "- Romance\n",
    "  - France (fr)\n",
    "  - Portugal (pt)\n",
    "  - Italy (it)\n",
    "\n",
    "- Uralic\n",
    "  - Estonia (ee) \n",
    "  - Hungary (hu)\n",
    "\n",
    "- Baltic\n",
    "  - Latvia (lv)\n",
    "  - Lithuanian\n",
    "\n",
    "\n",
    "- Turkic\n",
    "  - Turkey (tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ba \t Bosnia and Herzegovina \t 2531 \t Zahvaljujem gospodo predsjedavajući, dame i gospodo, Sa žaljenjem mogu konstatovati da na sjednici K\n",
      "hr \t Croatia \t 10741 \t Gospodine predsjedniče, uvaženi kolega zastupnik Leko i kolega zastupnik Arlović iznosili su neke ar\n",
      "rs \t Serbia \t 15114 \t Dame i gospodo, dozvolite da u ime našeg izbornog tela prenesem pozdrave i želju da ova skupština po\n"
     ]
    }
   ],
   "source": [
    "balkans = ['ba', 'hr', 'rs']\n",
    "parent_dir = Path(\"data/train/power\")\n",
    "\n",
    "for code in balkans:\n",
    "    full_data = load_data(folder_path=parent_dir, file_list=[f\"power-{code}-train.tsv\"],text_head=\"text\")\n",
    "    print(\n",
    "        code, \"\\t\", \n",
    "        mapping_dict[code], \"\\t\", \n",
    "        len(full_data), \"\\t\",\n",
    "        full_data.texts[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-2c31ffc350554f9d831f0096bf9914de.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-2c31ffc350554f9d831f0096bf9914de.vega-embed details,\n",
       "  #altair-viz-2c31ffc350554f9d831f0096bf9914de.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-2c31ffc350554f9d831f0096bf9914de\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2c31ffc350554f9d831f0096bf9914de\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2c31ffc350554f9d831f0096bf9914de\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-68863f377eeaf60c46ba5db7d1f0d46f\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"variable\", \"scale\": {\"scheme\": \"category20\"}, \"sort\": null, \"title\": \"Evaluation Metric\", \"type\": \"nominal\"}, \"column\": {\"field\": \"language\", \"sort\": null, \"title\": \"Language\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labels\": false, \"ticks\": false, \"title\": \"\"}, \"field\": \"variable\", \"sort\": null, \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Score\"}, \"field\": \"value\", \"scale\": {\"domain\": [0, 1]}, \"type\": \"quantitative\"}}, \"height\": 300, \"title\": \"ba - Bosnia and Herzegovina - 2531 datapoints\", \"width\": 200, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-68863f377eeaf60c46ba5db7d1f0d46f\": [{\"language\": \"en\", \"variable\": \"positive_pct\", \"value\": 0.8316870802054523}, {\"language\": \"ori\", \"variable\": \"positive_pct\", \"value\": 0.8316870802054523}, {\"language\": \"en\", \"variable\": \"precision\", \"value\": 0.8353909254074097}, {\"language\": \"ori\", \"variable\": \"precision\", \"value\": 0.751028835773468}, {\"language\": \"en\", \"variable\": \"recall\", \"value\": 0.9308755993843079}, {\"language\": \"ori\", \"variable\": \"recall\", \"value\": 0.8018433451652527}, {\"language\": \"en\", \"variable\": \"f1\", \"value\": 0.9099099040031433}, {\"language\": \"ori\", \"variable\": \"f1\", \"value\": 0.8518971800804138}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-d78971936d43464797a10b586aadabb8.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-d78971936d43464797a10b586aadabb8.vega-embed details,\n",
       "  #altair-viz-d78971936d43464797a10b586aadabb8.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-d78971936d43464797a10b586aadabb8\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-d78971936d43464797a10b586aadabb8\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-d78971936d43464797a10b586aadabb8\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-73f2c451278ff698a46d7d406845078c\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"variable\", \"scale\": {\"scheme\": \"category20\"}, \"sort\": null, \"title\": \"Evaluation Metric\", \"type\": \"nominal\"}, \"column\": {\"field\": \"language\", \"sort\": null, \"title\": \"Language\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labels\": false, \"ticks\": false, \"title\": \"\"}, \"field\": \"variable\", \"sort\": null, \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Score\"}, \"field\": \"value\", \"scale\": {\"domain\": [0, 1]}, \"type\": \"quantitative\"}}, \"height\": 300, \"title\": \"hr - Croatia - 10741 datapoints\", \"width\": 200, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-73f2c451278ff698a46d7d406845078c\": [{\"language\": \"en\", \"variable\": \"positive_pct\", \"value\": 0.3972628246904385}, {\"language\": \"ori\", \"variable\": \"positive_pct\", \"value\": 0.3972628246904385}, {\"language\": \"en\", \"variable\": \"precision\", \"value\": 0.6590038537979126}, {\"language\": \"ori\", \"variable\": \"precision\", \"value\": 0.6762452125549316}, {\"language\": \"en\", \"variable\": \"recall\", \"value\": 0.4323641061782837}, {\"language\": \"ori\", \"variable\": \"recall\", \"value\": 0.29962074756622314}, {\"language\": \"en\", \"variable\": \"f1\", \"value\": 0.4899713397026062}, {\"language\": \"ori\", \"variable\": \"f1\", \"value\": 0.41217392683029175}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-75a02f5dd7ae4116bbf44e4603121dfb.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-75a02f5dd7ae4116bbf44e4603121dfb.vega-embed details,\n",
       "  #altair-viz-75a02f5dd7ae4116bbf44e4603121dfb.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-75a02f5dd7ae4116bbf44e4603121dfb\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-75a02f5dd7ae4116bbf44e4603121dfb\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-75a02f5dd7ae4116bbf44e4603121dfb\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-32a0df100d2301f1b294949a2a7d2dff\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"variable\", \"scale\": {\"scheme\": \"category20\"}, \"sort\": null, \"title\": \"Evaluation Metric\", \"type\": \"nominal\"}, \"column\": {\"field\": \"language\", \"sort\": null, \"title\": \"Language\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labels\": false, \"ticks\": false, \"title\": \"\"}, \"field\": \"variable\", \"sort\": null, \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Score\"}, \"field\": \"value\", \"scale\": {\"domain\": [0, 1]}, \"type\": \"quantitative\"}}, \"height\": 300, \"title\": \"rs - Serbia - 15114 datapoints\", \"width\": 200, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-32a0df100d2301f1b294949a2a7d2dff\": [{\"language\": \"en\", \"variable\": \"positive_pct\", \"value\": 0.2708746857218473}, {\"language\": \"ori\", \"variable\": \"positive_pct\", \"value\": 0.2708746857218473}, {\"language\": \"en\", \"variable\": \"precision\", \"value\": 0.781697154045105}, {\"language\": \"ori\", \"variable\": \"precision\", \"value\": 0.7587354183197021}, {\"language\": \"en\", \"variable\": \"recall\", \"value\": 0.4863731563091278}, {\"language\": \"ori\", \"variable\": \"recall\", \"value\": 0.28721174597740173}, {\"language\": \"en\", \"variable\": \"f1\", \"value\": 0.5858585834503174}, {\"language\": \"ori\", \"variable\": \"f1\", \"value\": 0.43047916889190674}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_countries(balkans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to handle Croatia differently\n",
    "# How to deal with class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training and tetsing on original text seems to have good effect\n",
    "- Using character-level tokens seems to work better. a vocabulary of 50000 tokens provides diminishing return.\n",
    "- Still need to deal with imbalance labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22672 5714\n",
      "Percentage of positive: 0.3698394495412844 0.364193209660483\n",
      "Prepare data encoder...\n",
      "Vocabulary 50000\n",
      "Prepare data...\n",
      "Train model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 178/178 [00:02<00:00, 70.13batch/s, batch_accuracy=0.875, loss=16.3]\n",
      "Epoch 2: 100%|██████████| 178/178 [00:02<00:00, 70.81batch/s, batch_accuracy=0.625, loss=11.5]\n",
      "Epoch 3: 100%|██████████| 178/178 [00:02<00:00, 68.61batch/s, batch_accuracy=0.812, loss=9.81]\n",
      "Epoch 4: 100%|██████████| 178/178 [00:02<00:00, 69.61batch/s, batch_accuracy=0.938, loss=6.31]\n",
      "Epoch 5: 100%|██████████| 178/178 [00:02<00:00, 67.86batch/s, batch_accuracy=1, loss=23.4]   \n",
      "Epoch 6: 100%|██████████| 178/178 [00:02<00:00, 69.49batch/s, batch_accuracy=1, loss=5.69]   \n",
      "Epoch 7: 100%|██████████| 178/178 [00:02<00:00, 72.44batch/s, batch_accuracy=1, loss=8.9]    \n",
      "Epoch 8: 100%|██████████| 178/178 [00:02<00:00, 69.93batch/s, batch_accuracy=1, loss=16.7]   \n",
      "Epoch 9: 100%|██████████| 178/178 [00:02<00:00, 70.51batch/s, batch_accuracy=1, loss=1.49]   \n",
      "Epoch 10: 100%|██████████| 178/178 [00:02<00:00, 70.12batch/s, batch_accuracy=1, loss=19.8]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7394120097160339, 0.6516097784042358, 0.6455605626106262)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-2d95e7d9dfa84242ba0a66e1b6f13c46.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-2d95e7d9dfa84242ba0a66e1b6f13c46.vega-embed details,\n",
       "  #altair-viz-2d95e7d9dfa84242ba0a66e1b6f13c46.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-2d95e7d9dfa84242ba0a66e1b6f13c46\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2d95e7d9dfa84242ba0a66e1b6f13c46\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2d95e7d9dfa84242ba0a66e1b6f13c46\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-b915bd26a64fddd40637c4b129022cd8\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-b915bd26a64fddd40637c4b129022cd8\": [{\"training_acc\": 0.8671875, \"training_loss\": 257.1566162109375, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 232.84840393066406, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 247.27066040039062, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 203.62245178222656, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 198.489013671875, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 222.47201538085938, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 261.11688232421875, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 231.84909057617188, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 221.71270751953125, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 206.9979248046875, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 245.66299438476562, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 235.3549346923828, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 245.31500244140625, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 186.66482543945312, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 186.1325225830078, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 286.9930725097656, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 228.0541229248047, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 223.6299591064453, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 199.30941772460938, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 189.56964111328125, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 188.97543334960938, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 227.52081298828125, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 225.48638916015625, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 258.3847961425781, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 225.51055908203125, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 224.4434051513672, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 254.46209716796875, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 248.78439331054688, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 241.09893798828125, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 220.22225952148438, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 179.61721801757812, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 226.48892211914062, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 194.05152893066406, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 216.80575561523438, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 266.68621826171875, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 199.7882843017578, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 236.91839599609375, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 219.98570251464844, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 265.9894714355469, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 198.37396240234375, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 195.03338623046875, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 231.55862426757812, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 227.98080444335938, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 251.39663696289062, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 168.475341796875, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 224.30113220214844, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 223.92471313476562, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 220.51480102539062, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 244.07566833496094, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 195.36912536621094, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 230.3611602783203, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 236.1835174560547, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 228.51417541503906, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 232.94906616210938, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 229.77999877929688, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 219.98716735839844, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 209.72659301757812, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 241.06622314453125, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 227.5221710205078, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 166.51766967773438, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 251.17904663085938, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 179.56666564941406, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 236.98019409179688, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 291.198486328125, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 245.22027587890625, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 204.0150604248047, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 243.76980590820312, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 226.2023162841797, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 176.30703735351562, \"iteration\": 69, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 167.97833251953125, \"iteration\": 70, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 213.46218872070312, \"iteration\": 71, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 212.70758056640625, \"iteration\": 72, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 198.33514404296875, \"iteration\": 73, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 186.84713745117188, \"iteration\": 74, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 225.46665954589844, \"iteration\": 75, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 176.0351104736328, \"iteration\": 76, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 197.6146240234375, \"iteration\": 77, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 210.88583374023438, \"iteration\": 78, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 193.31369018554688, \"iteration\": 79, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 208.62567138671875, \"iteration\": 80, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 172.25274658203125, \"iteration\": 81, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 235.7930450439453, \"iteration\": 82, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 216.68312072753906, \"iteration\": 83, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 236.3500213623047, \"iteration\": 84, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 239.27517700195312, \"iteration\": 85, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 256.1248779296875, \"iteration\": 86, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 189.427734375, \"iteration\": 87, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 200.81463623046875, \"iteration\": 88, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 240.87466430664062, \"iteration\": 89, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 218.7213134765625, \"iteration\": 90, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 208.3038330078125, \"iteration\": 91, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 249.89376831054688, \"iteration\": 92, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 199.83782958984375, \"iteration\": 93, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 245.94003295898438, \"iteration\": 94, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 205.269775390625, \"iteration\": 95, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 199.69515991210938, \"iteration\": 96, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 245.7446746826172, \"iteration\": 97, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 235.35873413085938, \"iteration\": 98, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 184.0544891357422, \"iteration\": 99, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 222.7356719970703, \"iteration\": 100, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 215.85031127929688, \"iteration\": 101, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 222.65625, \"iteration\": 102, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 213.46878051757812, \"iteration\": 103, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 193.50718688964844, \"iteration\": 104, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 200.2342529296875, \"iteration\": 105, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 242.5661163330078, \"iteration\": 106, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 272.6390380859375, \"iteration\": 107, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 172.6943359375, \"iteration\": 108, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 243.81661987304688, \"iteration\": 109, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 228.19288635253906, \"iteration\": 110, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 177.58602905273438, \"iteration\": 111, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 268.8647766113281, \"iteration\": 112, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 206.1270294189453, \"iteration\": 113, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 200.71978759765625, \"iteration\": 114, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 226.25698852539062, \"iteration\": 115, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 198.3616943359375, \"iteration\": 116, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 232.85546875, \"iteration\": 117, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 183.81805419921875, \"iteration\": 118, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 206.23634338378906, \"iteration\": 119, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 167.64712524414062, \"iteration\": 120, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 226.7408905029297, \"iteration\": 121, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 228.29510498046875, \"iteration\": 122, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 236.20587158203125, \"iteration\": 123, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 193.57754516601562, \"iteration\": 124, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 265.9777526855469, \"iteration\": 125, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 183.892333984375, \"iteration\": 126, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 224.79885864257812, \"iteration\": 127, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 151.7122802734375, \"iteration\": 128, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 258.3943176269531, \"iteration\": 129, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 246.48065185546875, \"iteration\": 130, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 186.51895141601562, \"iteration\": 131, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 211.25384521484375, \"iteration\": 132, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 211.1269073486328, \"iteration\": 133, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 209.66390991210938, \"iteration\": 134, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 243.42079162597656, \"iteration\": 135, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 218.73934936523438, \"iteration\": 136, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 208.1314697265625, \"iteration\": 137, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 228.7286376953125, \"iteration\": 138, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 208.916259765625, \"iteration\": 139, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 232.80877685546875, \"iteration\": 140, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 167.78445434570312, \"iteration\": 141, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 234.42660522460938, \"iteration\": 142, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 245.095703125, \"iteration\": 143, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 287.19232177734375, \"iteration\": 144, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 170.75074768066406, \"iteration\": 145, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 242.39239501953125, \"iteration\": 146, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 199.70877075195312, \"iteration\": 147, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 195.27772521972656, \"iteration\": 148, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 196.1549072265625, \"iteration\": 149, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 213.52581787109375, \"iteration\": 150, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 234.91412353515625, \"iteration\": 151, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 219.52650451660156, \"iteration\": 152, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 195.61004638671875, \"iteration\": 153, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 207.68203735351562, \"iteration\": 154, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 210.44297790527344, \"iteration\": 155, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 206.1919708251953, \"iteration\": 156, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 205.19741821289062, \"iteration\": 157, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 225.2873992919922, \"iteration\": 158, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 198.8037109375, \"iteration\": 159, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 232.72064208984375, \"iteration\": 160, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 210.60577392578125, \"iteration\": 161, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 223.11431884765625, \"iteration\": 162, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 214.08468627929688, \"iteration\": 163, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 210.13446044921875, \"iteration\": 164, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 238.0986328125, \"iteration\": 165, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 239.07015991210938, \"iteration\": 166, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 210.3708038330078, \"iteration\": 167, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 194.03248596191406, \"iteration\": 168, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 221.8459930419922, \"iteration\": 169, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 215.44776916503906, \"iteration\": 170, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 167.059326171875, \"iteration\": 171, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 217.41671752929688, \"iteration\": 172, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 193.45147705078125, \"iteration\": 173, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 242.5945587158203, \"iteration\": 174, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 194.55758666992188, \"iteration\": 175, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 191.42726135253906, \"iteration\": 176, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 183.04986572265625, \"iteration\": 177, \"epoch\": 1}, {\"training_acc\": 0.875, \"training_loss\": 16.281335830688477, \"iteration\": 178, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 201.86929321289062, \"iteration\": 179, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 181.45379638671875, \"iteration\": 180, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 213.30062866210938, \"iteration\": 181, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 197.9931640625, \"iteration\": 182, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 193.54937744140625, \"iteration\": 183, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 199.5960693359375, \"iteration\": 184, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 174.34466552734375, \"iteration\": 185, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 209.14588928222656, \"iteration\": 186, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 178.880615234375, \"iteration\": 187, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 225.01885986328125, \"iteration\": 188, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 178.73410034179688, \"iteration\": 189, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 161.73464965820312, \"iteration\": 190, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 232.35389709472656, \"iteration\": 191, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 221.1923828125, \"iteration\": 192, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 164.30935668945312, \"iteration\": 193, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 198.58541870117188, \"iteration\": 194, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 182.7636260986328, \"iteration\": 195, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 209.03843688964844, \"iteration\": 196, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 215.50091552734375, \"iteration\": 197, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 193.6191864013672, \"iteration\": 198, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 240.15406799316406, \"iteration\": 199, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 194.61416625976562, \"iteration\": 200, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 222.3789520263672, \"iteration\": 201, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 188.363525390625, \"iteration\": 202, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 223.40975952148438, \"iteration\": 203, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 220.26739501953125, \"iteration\": 204, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 228.28759765625, \"iteration\": 205, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 209.2979278564453, \"iteration\": 206, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 215.6466064453125, \"iteration\": 207, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 206.47244262695312, \"iteration\": 208, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 202.14242553710938, \"iteration\": 209, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 209.73768615722656, \"iteration\": 210, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 194.18673706054688, \"iteration\": 211, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 195.0482177734375, \"iteration\": 212, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 238.04197692871094, \"iteration\": 213, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 221.23880004882812, \"iteration\": 214, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 148.89923095703125, \"iteration\": 215, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 197.72140502929688, \"iteration\": 216, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 143.416015625, \"iteration\": 217, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 207.39715576171875, \"iteration\": 218, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 210.8541259765625, \"iteration\": 219, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 216.60357666015625, \"iteration\": 220, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 204.94039916992188, \"iteration\": 221, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 149.62750244140625, \"iteration\": 222, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 213.4465789794922, \"iteration\": 223, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 219.49952697753906, \"iteration\": 224, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 170.25823974609375, \"iteration\": 225, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 189.92889404296875, \"iteration\": 226, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 181.920654296875, \"iteration\": 227, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 207.28411865234375, \"iteration\": 228, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 230.5091094970703, \"iteration\": 229, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 169.81634521484375, \"iteration\": 230, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 242.26181030273438, \"iteration\": 231, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 157.46739196777344, \"iteration\": 232, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 228.32374572753906, \"iteration\": 233, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 204.09580993652344, \"iteration\": 234, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 210.5529327392578, \"iteration\": 235, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 219.3447265625, \"iteration\": 236, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 238.94171142578125, \"iteration\": 237, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 219.5980224609375, \"iteration\": 238, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 238.11489868164062, \"iteration\": 239, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 184.93368530273438, \"iteration\": 240, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 170.8607940673828, \"iteration\": 241, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 197.0147705078125, \"iteration\": 242, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 193.9783935546875, \"iteration\": 243, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 178.95733642578125, \"iteration\": 244, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 188.06832885742188, \"iteration\": 245, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 218.95822143554688, \"iteration\": 246, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 201.97885131835938, \"iteration\": 247, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 230.24993896484375, \"iteration\": 248, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 267.13714599609375, \"iteration\": 249, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 259.9639892578125, \"iteration\": 250, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 198.53453063964844, \"iteration\": 251, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 199.3006134033203, \"iteration\": 252, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 189.79415893554688, \"iteration\": 253, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 238.83319091796875, \"iteration\": 254, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 243.68479919433594, \"iteration\": 255, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 176.91738891601562, \"iteration\": 256, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 232.87281799316406, \"iteration\": 257, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 229.4290771484375, \"iteration\": 258, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 212.60012817382812, \"iteration\": 259, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 187.93443298339844, \"iteration\": 260, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 200.19949340820312, \"iteration\": 261, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 191.18453979492188, \"iteration\": 262, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 202.44757080078125, \"iteration\": 263, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 199.66043090820312, \"iteration\": 264, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 127.67800903320312, \"iteration\": 265, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 193.9575958251953, \"iteration\": 266, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 243.5139923095703, \"iteration\": 267, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 202.19503784179688, \"iteration\": 268, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 191.89501953125, \"iteration\": 269, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 230.73069763183594, \"iteration\": 270, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 256.13104248046875, \"iteration\": 271, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 234.1089324951172, \"iteration\": 272, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 196.703369140625, \"iteration\": 273, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 187.67605590820312, \"iteration\": 274, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 221.55223083496094, \"iteration\": 275, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 218.8830108642578, \"iteration\": 276, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 240.04319763183594, \"iteration\": 277, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 153.0130157470703, \"iteration\": 278, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 177.93832397460938, \"iteration\": 279, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 174.82113647460938, \"iteration\": 280, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 216.06063842773438, \"iteration\": 281, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 226.60118103027344, \"iteration\": 282, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 162.16714477539062, \"iteration\": 283, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 209.2412109375, \"iteration\": 284, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 202.87734985351562, \"iteration\": 285, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 178.95010375976562, \"iteration\": 286, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 217.76983642578125, \"iteration\": 287, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 220.73211669921875, \"iteration\": 288, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 195.3649139404297, \"iteration\": 289, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 195.49411010742188, \"iteration\": 290, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 192.05322265625, \"iteration\": 291, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 196.99676513671875, \"iteration\": 292, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 262.091552734375, \"iteration\": 293, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 186.28451538085938, \"iteration\": 294, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 243.89309692382812, \"iteration\": 295, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 220.1501007080078, \"iteration\": 296, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 184.67974853515625, \"iteration\": 297, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 189.9285430908203, \"iteration\": 298, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 196.60342407226562, \"iteration\": 299, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 225.62738037109375, \"iteration\": 300, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 245.72752380371094, \"iteration\": 301, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 180.6624755859375, \"iteration\": 302, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 191.4663543701172, \"iteration\": 303, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 216.96417236328125, \"iteration\": 304, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 208.85906982421875, \"iteration\": 305, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 245.30715942382812, \"iteration\": 306, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 190.92056274414062, \"iteration\": 307, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 216.03311157226562, \"iteration\": 308, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 237.130615234375, \"iteration\": 309, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 198.8566131591797, \"iteration\": 310, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 208.7611083984375, \"iteration\": 311, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 219.10494995117188, \"iteration\": 312, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 193.75436401367188, \"iteration\": 313, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 178.82110595703125, \"iteration\": 314, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 185.30966186523438, \"iteration\": 315, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 167.7353057861328, \"iteration\": 316, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 142.57220458984375, \"iteration\": 317, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 209.18128967285156, \"iteration\": 318, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 229.88442993164062, \"iteration\": 319, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 204.55471801757812, \"iteration\": 320, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 150.60525512695312, \"iteration\": 321, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 219.3096923828125, \"iteration\": 322, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 185.7103271484375, \"iteration\": 323, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 207.61874389648438, \"iteration\": 324, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 198.97537231445312, \"iteration\": 325, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 194.7910919189453, \"iteration\": 326, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 217.8895263671875, \"iteration\": 327, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 256.329833984375, \"iteration\": 328, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 129.11265563964844, \"iteration\": 329, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 192.32904052734375, \"iteration\": 330, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 210.66571044921875, \"iteration\": 331, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 207.40652465820312, \"iteration\": 332, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 204.5016632080078, \"iteration\": 333, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 229.21441650390625, \"iteration\": 334, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 235.89300537109375, \"iteration\": 335, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 246.9536590576172, \"iteration\": 336, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 232.7415771484375, \"iteration\": 337, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 198.54721069335938, \"iteration\": 338, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 206.30874633789062, \"iteration\": 339, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 217.480224609375, \"iteration\": 340, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 214.64588928222656, \"iteration\": 341, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 193.13601684570312, \"iteration\": 342, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 178.67633056640625, \"iteration\": 343, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 240.2106475830078, \"iteration\": 344, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 225.108642578125, \"iteration\": 345, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 230.9305877685547, \"iteration\": 346, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 216.41921997070312, \"iteration\": 347, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 196.49673461914062, \"iteration\": 348, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 184.11215209960938, \"iteration\": 349, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 158.89051818847656, \"iteration\": 350, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 250.0680389404297, \"iteration\": 351, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 219.11264038085938, \"iteration\": 352, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 233.6724090576172, \"iteration\": 353, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 209.84268188476562, \"iteration\": 354, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 287.6665344238281, \"iteration\": 355, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 11.51620864868164, \"iteration\": 356, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 193.5955352783203, \"iteration\": 357, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 232.8755645751953, \"iteration\": 358, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 182.64987182617188, \"iteration\": 359, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 156.689697265625, \"iteration\": 360, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 169.3031005859375, \"iteration\": 361, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 210.1763458251953, \"iteration\": 362, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 203.95217895507812, \"iteration\": 363, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 200.5486602783203, \"iteration\": 364, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 184.99114990234375, \"iteration\": 365, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 198.2369384765625, \"iteration\": 366, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 201.28872680664062, \"iteration\": 367, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 161.41053771972656, \"iteration\": 368, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 169.61407470703125, \"iteration\": 369, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 189.42803955078125, \"iteration\": 370, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 187.239501953125, \"iteration\": 371, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 172.30799865722656, \"iteration\": 372, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 141.01535034179688, \"iteration\": 373, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 211.6920623779297, \"iteration\": 374, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 164.42529296875, \"iteration\": 375, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 241.69642639160156, \"iteration\": 376, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 219.25624084472656, \"iteration\": 377, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 158.76358032226562, \"iteration\": 378, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 183.18072509765625, \"iteration\": 379, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 189.14987182617188, \"iteration\": 380, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 210.00677490234375, \"iteration\": 381, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 196.81796264648438, \"iteration\": 382, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 173.7718505859375, \"iteration\": 383, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 203.2697296142578, \"iteration\": 384, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 192.00955200195312, \"iteration\": 385, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 153.12515258789062, \"iteration\": 386, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 175.29376220703125, \"iteration\": 387, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 221.94076538085938, \"iteration\": 388, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 255.125244140625, \"iteration\": 389, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 239.15647888183594, \"iteration\": 390, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 182.79949951171875, \"iteration\": 391, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 216.29403686523438, \"iteration\": 392, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 203.84628295898438, \"iteration\": 393, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 191.0811767578125, \"iteration\": 394, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 220.30154418945312, \"iteration\": 395, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 204.21124267578125, \"iteration\": 396, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 186.5977783203125, \"iteration\": 397, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 197.58447265625, \"iteration\": 398, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 240.86978149414062, \"iteration\": 399, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 216.70492553710938, \"iteration\": 400, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 142.06024169921875, \"iteration\": 401, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 164.83221435546875, \"iteration\": 402, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 196.82583618164062, \"iteration\": 403, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 173.8638916015625, \"iteration\": 404, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 157.70578002929688, \"iteration\": 405, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 244.97879028320312, \"iteration\": 406, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 244.62838745117188, \"iteration\": 407, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 213.03457641601562, \"iteration\": 408, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 208.56777954101562, \"iteration\": 409, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 181.04644775390625, \"iteration\": 410, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 206.1302490234375, \"iteration\": 411, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 233.06671142578125, \"iteration\": 412, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 228.5886688232422, \"iteration\": 413, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 168.96041870117188, \"iteration\": 414, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 220.77145385742188, \"iteration\": 415, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 226.200439453125, \"iteration\": 416, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 158.4156036376953, \"iteration\": 417, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 190.30943298339844, \"iteration\": 418, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 167.5322265625, \"iteration\": 419, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 172.38340759277344, \"iteration\": 420, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 210.6646270751953, \"iteration\": 421, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 195.63868713378906, \"iteration\": 422, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 183.00653076171875, \"iteration\": 423, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 220.56307983398438, \"iteration\": 424, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 205.18771362304688, \"iteration\": 425, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 190.91921997070312, \"iteration\": 426, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 222.21160888671875, \"iteration\": 427, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 144.6840057373047, \"iteration\": 428, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 224.4229736328125, \"iteration\": 429, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 187.16925048828125, \"iteration\": 430, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 214.69583129882812, \"iteration\": 431, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 206.71044921875, \"iteration\": 432, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 237.68243408203125, \"iteration\": 433, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 200.154052734375, \"iteration\": 434, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 196.9514923095703, \"iteration\": 435, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 169.82568359375, \"iteration\": 436, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 169.128173828125, \"iteration\": 437, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 178.11215209960938, \"iteration\": 438, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 221.71322631835938, \"iteration\": 439, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 233.57286071777344, \"iteration\": 440, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 193.2991180419922, \"iteration\": 441, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 225.18670654296875, \"iteration\": 442, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 210.20628356933594, \"iteration\": 443, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 199.59036254882812, \"iteration\": 444, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 201.52354431152344, \"iteration\": 445, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 217.63461303710938, \"iteration\": 446, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 183.93289184570312, \"iteration\": 447, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 176.19119262695312, \"iteration\": 448, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 180.6527099609375, \"iteration\": 449, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 179.3388671875, \"iteration\": 450, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 177.48043823242188, \"iteration\": 451, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 197.2700958251953, \"iteration\": 452, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 152.658935546875, \"iteration\": 453, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 206.03842163085938, \"iteration\": 454, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 182.59861755371094, \"iteration\": 455, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 197.42794799804688, \"iteration\": 456, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 214.0616455078125, \"iteration\": 457, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 188.17654418945312, \"iteration\": 458, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 204.86819458007812, \"iteration\": 459, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 197.4721221923828, \"iteration\": 460, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 182.6036834716797, \"iteration\": 461, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 180.52890014648438, \"iteration\": 462, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 188.7675323486328, \"iteration\": 463, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 155.04664611816406, \"iteration\": 464, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 152.60031127929688, \"iteration\": 465, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 191.19467163085938, \"iteration\": 466, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 182.33616638183594, \"iteration\": 467, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 189.52615356445312, \"iteration\": 468, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 190.393310546875, \"iteration\": 469, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 226.85226440429688, \"iteration\": 470, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 176.5182342529297, \"iteration\": 471, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 216.58299255371094, \"iteration\": 472, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 260.05126953125, \"iteration\": 473, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 197.08944702148438, \"iteration\": 474, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 189.6209716796875, \"iteration\": 475, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 193.60610961914062, \"iteration\": 476, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 246.84263610839844, \"iteration\": 477, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 193.26919555664062, \"iteration\": 478, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 184.40670776367188, \"iteration\": 479, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 187.28866577148438, \"iteration\": 480, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 215.37863159179688, \"iteration\": 481, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 178.64295959472656, \"iteration\": 482, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 209.3800048828125, \"iteration\": 483, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 179.64585876464844, \"iteration\": 484, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 169.63632202148438, \"iteration\": 485, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 195.89743041992188, \"iteration\": 486, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 174.96986389160156, \"iteration\": 487, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 219.75991821289062, \"iteration\": 488, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 253.81304931640625, \"iteration\": 489, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 202.62152099609375, \"iteration\": 490, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 191.41058349609375, \"iteration\": 491, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 206.869140625, \"iteration\": 492, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 208.3455810546875, \"iteration\": 493, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 235.93682861328125, \"iteration\": 494, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 188.7681884765625, \"iteration\": 495, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 215.58230590820312, \"iteration\": 496, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 198.56524658203125, \"iteration\": 497, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 153.76071166992188, \"iteration\": 498, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 138.58279418945312, \"iteration\": 499, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 200.02633666992188, \"iteration\": 500, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 249.1142120361328, \"iteration\": 501, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 187.2771759033203, \"iteration\": 502, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 212.12994384765625, \"iteration\": 503, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 207.5137939453125, \"iteration\": 504, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 207.4633331298828, \"iteration\": 505, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 215.64979553222656, \"iteration\": 506, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 187.7937469482422, \"iteration\": 507, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 241.84930419921875, \"iteration\": 508, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 230.23031616210938, \"iteration\": 509, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 160.13754272460938, \"iteration\": 510, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 214.576171875, \"iteration\": 511, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 238.6040802001953, \"iteration\": 512, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 245.04762268066406, \"iteration\": 513, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 203.9348602294922, \"iteration\": 514, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 197.09860229492188, \"iteration\": 515, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 202.5240478515625, \"iteration\": 516, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 224.84405517578125, \"iteration\": 517, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 192.30575561523438, \"iteration\": 518, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 156.64633178710938, \"iteration\": 519, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 169.1019744873047, \"iteration\": 520, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 225.62945556640625, \"iteration\": 521, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 177.57913208007812, \"iteration\": 522, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 208.01254272460938, \"iteration\": 523, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 242.06199645996094, \"iteration\": 524, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 223.13729858398438, \"iteration\": 525, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 192.34457397460938, \"iteration\": 526, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 188.75408935546875, \"iteration\": 527, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 179.00811767578125, \"iteration\": 528, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 169.91717529296875, \"iteration\": 529, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 167.60482788085938, \"iteration\": 530, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 253.89303588867188, \"iteration\": 531, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 184.8729248046875, \"iteration\": 532, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 208.64405822753906, \"iteration\": 533, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 9.810107231140137, \"iteration\": 534, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 198.57052612304688, \"iteration\": 535, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 203.5093994140625, \"iteration\": 536, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 203.4717254638672, \"iteration\": 537, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 171.80783081054688, \"iteration\": 538, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 120.84332275390625, \"iteration\": 539, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 216.08892822265625, \"iteration\": 540, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 235.32647705078125, \"iteration\": 541, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 223.93527221679688, \"iteration\": 542, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 222.35423278808594, \"iteration\": 543, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 201.90313720703125, \"iteration\": 544, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 202.4549560546875, \"iteration\": 545, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 159.5543212890625, \"iteration\": 546, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 151.93617248535156, \"iteration\": 547, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 176.36093139648438, \"iteration\": 548, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 171.02365112304688, \"iteration\": 549, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 168.01864624023438, \"iteration\": 550, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 204.55712890625, \"iteration\": 551, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 192.83546447753906, \"iteration\": 552, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 212.1175537109375, \"iteration\": 553, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 166.13372802734375, \"iteration\": 554, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 178.9393310546875, \"iteration\": 555, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 212.46932983398438, \"iteration\": 556, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 179.49493408203125, \"iteration\": 557, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 195.81591796875, \"iteration\": 558, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 177.24252319335938, \"iteration\": 559, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 173.39019775390625, \"iteration\": 560, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 170.8223876953125, \"iteration\": 561, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 214.58071899414062, \"iteration\": 562, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 189.9359893798828, \"iteration\": 563, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 165.99192810058594, \"iteration\": 564, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 184.77865600585938, \"iteration\": 565, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 200.82371520996094, \"iteration\": 566, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 154.432373046875, \"iteration\": 567, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 208.48526000976562, \"iteration\": 568, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 182.49534606933594, \"iteration\": 569, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 168.60150146484375, \"iteration\": 570, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 217.22573852539062, \"iteration\": 571, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 190.74322509765625, \"iteration\": 572, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 196.46234130859375, \"iteration\": 573, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 183.29324340820312, \"iteration\": 574, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 228.01161193847656, \"iteration\": 575, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 181.5275115966797, \"iteration\": 576, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 193.36029052734375, \"iteration\": 577, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 193.56996154785156, \"iteration\": 578, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 228.7242431640625, \"iteration\": 579, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 192.56924438476562, \"iteration\": 580, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 259.4652099609375, \"iteration\": 581, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 163.2818603515625, \"iteration\": 582, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 228.05406188964844, \"iteration\": 583, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 177.35768127441406, \"iteration\": 584, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 175.35025024414062, \"iteration\": 585, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 146.70809936523438, \"iteration\": 586, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 210.26695251464844, \"iteration\": 587, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 137.38319396972656, \"iteration\": 588, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 187.858154296875, \"iteration\": 589, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 187.5574951171875, \"iteration\": 590, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 217.19033813476562, \"iteration\": 591, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 181.5053253173828, \"iteration\": 592, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 176.15567016601562, \"iteration\": 593, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 233.8889617919922, \"iteration\": 594, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 181.6790008544922, \"iteration\": 595, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 181.14120483398438, \"iteration\": 596, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 219.17005920410156, \"iteration\": 597, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 233.9630889892578, \"iteration\": 598, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 195.29835510253906, \"iteration\": 599, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 217.852783203125, \"iteration\": 600, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 229.5143280029297, \"iteration\": 601, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 207.74984741210938, \"iteration\": 602, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 184.32943725585938, \"iteration\": 603, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 195.954833984375, \"iteration\": 604, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 232.17510986328125, \"iteration\": 605, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 173.99075317382812, \"iteration\": 606, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 187.83099365234375, \"iteration\": 607, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 185.5347900390625, \"iteration\": 608, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 197.06541442871094, \"iteration\": 609, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 177.3485107421875, \"iteration\": 610, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 227.47451782226562, \"iteration\": 611, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 135.74656677246094, \"iteration\": 612, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 192.08522033691406, \"iteration\": 613, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 211.2313995361328, \"iteration\": 614, \"epoch\": 4}, {\"training_acc\": 0.875, \"training_loss\": 195.1800537109375, \"iteration\": 615, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 209.8727264404297, \"iteration\": 616, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 191.16343688964844, \"iteration\": 617, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 168.57989501953125, \"iteration\": 618, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 208.8174591064453, \"iteration\": 619, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 212.08148193359375, \"iteration\": 620, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 160.581787109375, \"iteration\": 621, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 152.30747985839844, \"iteration\": 622, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 203.45468139648438, \"iteration\": 623, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 207.00135803222656, \"iteration\": 624, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 190.04415893554688, \"iteration\": 625, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 215.00869750976562, \"iteration\": 626, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 204.24127197265625, \"iteration\": 627, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 211.51890563964844, \"iteration\": 628, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 192.24179077148438, \"iteration\": 629, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 186.90443420410156, \"iteration\": 630, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 169.95767211914062, \"iteration\": 631, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 147.27622985839844, \"iteration\": 632, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 229.41824340820312, \"iteration\": 633, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 224.1932373046875, \"iteration\": 634, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 220.60232543945312, \"iteration\": 635, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 177.56787109375, \"iteration\": 636, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 190.60678100585938, \"iteration\": 637, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 209.50169372558594, \"iteration\": 638, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 239.55853271484375, \"iteration\": 639, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 195.9628448486328, \"iteration\": 640, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 177.13229370117188, \"iteration\": 641, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 135.02874755859375, \"iteration\": 642, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 238.9774627685547, \"iteration\": 643, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 171.22348022460938, \"iteration\": 644, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 200.45697021484375, \"iteration\": 645, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 207.7997589111328, \"iteration\": 646, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 197.83969116210938, \"iteration\": 647, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 214.82418823242188, \"iteration\": 648, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 182.72027587890625, \"iteration\": 649, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 222.07611083984375, \"iteration\": 650, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 131.30162048339844, \"iteration\": 651, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 196.02761840820312, \"iteration\": 652, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 186.80441284179688, \"iteration\": 653, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 197.56216430664062, \"iteration\": 654, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 170.33837890625, \"iteration\": 655, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 198.32272338867188, \"iteration\": 656, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 208.8520050048828, \"iteration\": 657, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 183.01002502441406, \"iteration\": 658, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 189.2054443359375, \"iteration\": 659, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 215.90005493164062, \"iteration\": 660, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 184.84181213378906, \"iteration\": 661, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 219.3631591796875, \"iteration\": 662, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 150.32412719726562, \"iteration\": 663, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 179.6979217529297, \"iteration\": 664, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 228.89193725585938, \"iteration\": 665, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 189.20730590820312, \"iteration\": 666, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 206.32437133789062, \"iteration\": 667, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 170.42947387695312, \"iteration\": 668, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 243.42340087890625, \"iteration\": 669, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 169.0628204345703, \"iteration\": 670, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 131.0367431640625, \"iteration\": 671, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 160.75721740722656, \"iteration\": 672, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 219.09542846679688, \"iteration\": 673, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 168.25006103515625, \"iteration\": 674, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 210.2117919921875, \"iteration\": 675, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 190.11553955078125, \"iteration\": 676, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 205.33392333984375, \"iteration\": 677, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 196.6977996826172, \"iteration\": 678, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 146.08935546875, \"iteration\": 679, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 158.57925415039062, \"iteration\": 680, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 165.23834228515625, \"iteration\": 681, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 161.37380981445312, \"iteration\": 682, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 178.9517822265625, \"iteration\": 683, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 232.4811248779297, \"iteration\": 684, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 196.33319091796875, \"iteration\": 685, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 179.0483856201172, \"iteration\": 686, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 188.99423217773438, \"iteration\": 687, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 206.3411102294922, \"iteration\": 688, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 183.61776733398438, \"iteration\": 689, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 211.11044311523438, \"iteration\": 690, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 202.93194580078125, \"iteration\": 691, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 177.0352020263672, \"iteration\": 692, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 186.14718627929688, \"iteration\": 693, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 194.72506713867188, \"iteration\": 694, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 167.24786376953125, \"iteration\": 695, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 166.37530517578125, \"iteration\": 696, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 214.46817016601562, \"iteration\": 697, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 184.07351684570312, \"iteration\": 698, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 192.22833251953125, \"iteration\": 699, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 192.5091552734375, \"iteration\": 700, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 165.53134155273438, \"iteration\": 701, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 196.20773315429688, \"iteration\": 702, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 178.49813842773438, \"iteration\": 703, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 178.590087890625, \"iteration\": 704, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 187.58734130859375, \"iteration\": 705, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 230.41256713867188, \"iteration\": 706, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 167.61962890625, \"iteration\": 707, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 196.8659210205078, \"iteration\": 708, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 177.45957946777344, \"iteration\": 709, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 163.13555908203125, \"iteration\": 710, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 184.13002014160156, \"iteration\": 711, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 6.305412292480469, \"iteration\": 712, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 229.3853759765625, \"iteration\": 713, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 127.43673706054688, \"iteration\": 714, \"epoch\": 5}, {\"training_acc\": 0.9296875, \"training_loss\": 192.21731567382812, \"iteration\": 715, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 197.29319763183594, \"iteration\": 716, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 190.93405151367188, \"iteration\": 717, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 174.25271606445312, \"iteration\": 718, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 199.6456298828125, \"iteration\": 719, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 200.12017822265625, \"iteration\": 720, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 180.64495849609375, \"iteration\": 721, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 167.9879608154297, \"iteration\": 722, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 205.3691864013672, \"iteration\": 723, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 216.07400512695312, \"iteration\": 724, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 224.76513671875, \"iteration\": 725, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 170.87220764160156, \"iteration\": 726, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 190.4164581298828, \"iteration\": 727, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 198.33389282226562, \"iteration\": 728, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 169.4747314453125, \"iteration\": 729, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 175.21746826171875, \"iteration\": 730, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 190.13360595703125, \"iteration\": 731, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 214.92330932617188, \"iteration\": 732, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 201.11032104492188, \"iteration\": 733, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 220.25225830078125, \"iteration\": 734, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 139.59576416015625, \"iteration\": 735, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 220.13621520996094, \"iteration\": 736, \"epoch\": 5}, {\"training_acc\": 0.9296875, \"training_loss\": 173.16696166992188, \"iteration\": 737, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 196.49595642089844, \"iteration\": 738, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 171.17825317382812, \"iteration\": 739, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 171.8942108154297, \"iteration\": 740, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 166.16375732421875, \"iteration\": 741, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 225.4539337158203, \"iteration\": 742, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 201.73863220214844, \"iteration\": 743, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 226.14553833007812, \"iteration\": 744, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 181.84298706054688, \"iteration\": 745, \"epoch\": 5}, {\"training_acc\": 0.9140625, \"training_loss\": 192.002685546875, \"iteration\": 746, \"epoch\": 5}, {\"training_acc\": 0.9296875, \"training_loss\": 181.24530029296875, \"iteration\": 747, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 192.35415649414062, \"iteration\": 748, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 204.35650634765625, \"iteration\": 749, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 194.68374633789062, \"iteration\": 750, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 201.96507263183594, \"iteration\": 751, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 255.62576293945312, \"iteration\": 752, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 204.25643920898438, \"iteration\": 753, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 179.286376953125, \"iteration\": 754, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 166.08872985839844, \"iteration\": 755, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 174.75924682617188, \"iteration\": 756, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 206.08956909179688, \"iteration\": 757, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 167.0581817626953, \"iteration\": 758, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 190.95166015625, \"iteration\": 759, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 249.82614135742188, \"iteration\": 760, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 216.70172119140625, \"iteration\": 761, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 238.1296844482422, \"iteration\": 762, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 152.28439331054688, \"iteration\": 763, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 180.50790405273438, \"iteration\": 764, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 186.3435821533203, \"iteration\": 765, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 210.84078979492188, \"iteration\": 766, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 154.42630004882812, \"iteration\": 767, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 208.05633544921875, \"iteration\": 768, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 155.984130859375, \"iteration\": 769, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 209.27682495117188, \"iteration\": 770, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 219.1934814453125, \"iteration\": 771, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 186.48915100097656, \"iteration\": 772, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 174.3350372314453, \"iteration\": 773, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 219.70660400390625, \"iteration\": 774, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 180.5267333984375, \"iteration\": 775, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 169.47579956054688, \"iteration\": 776, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 183.46180725097656, \"iteration\": 777, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 138.26016235351562, \"iteration\": 778, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 181.1759033203125, \"iteration\": 779, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 158.47361755371094, \"iteration\": 780, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 179.62001037597656, \"iteration\": 781, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 192.1316375732422, \"iteration\": 782, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 215.2352294921875, \"iteration\": 783, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 204.14007568359375, \"iteration\": 784, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 160.5904541015625, \"iteration\": 785, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 191.3592529296875, \"iteration\": 786, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 201.42181396484375, \"iteration\": 787, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 152.20001220703125, \"iteration\": 788, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 188.52096557617188, \"iteration\": 789, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 227.2968292236328, \"iteration\": 790, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 185.55555725097656, \"iteration\": 791, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 172.17086791992188, \"iteration\": 792, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 175.65817260742188, \"iteration\": 793, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 180.2877655029297, \"iteration\": 794, \"epoch\": 5}, {\"training_acc\": 0.9140625, \"training_loss\": 207.74801635742188, \"iteration\": 795, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 181.82083129882812, \"iteration\": 796, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 160.8623046875, \"iteration\": 797, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 187.1844482421875, \"iteration\": 798, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 226.59219360351562, \"iteration\": 799, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 191.010986328125, \"iteration\": 800, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 151.94760131835938, \"iteration\": 801, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 218.0418701171875, \"iteration\": 802, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 227.29833984375, \"iteration\": 803, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 166.2392120361328, \"iteration\": 804, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 220.07627868652344, \"iteration\": 805, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 168.37095642089844, \"iteration\": 806, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 181.5582733154297, \"iteration\": 807, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 190.0957794189453, \"iteration\": 808, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 183.97303771972656, \"iteration\": 809, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 151.53958129882812, \"iteration\": 810, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 189.53414916992188, \"iteration\": 811, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 165.84011840820312, \"iteration\": 812, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 206.9441375732422, \"iteration\": 813, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 195.31846618652344, \"iteration\": 814, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 119.42460632324219, \"iteration\": 815, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 180.7042694091797, \"iteration\": 816, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 231.9449920654297, \"iteration\": 817, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 194.31442260742188, \"iteration\": 818, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 145.32257080078125, \"iteration\": 819, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 219.66555786132812, \"iteration\": 820, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 255.31356811523438, \"iteration\": 821, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 190.82774353027344, \"iteration\": 822, \"epoch\": 5}, {\"training_acc\": 0.921875, \"training_loss\": 196.91172790527344, \"iteration\": 823, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 209.69552612304688, \"iteration\": 824, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 160.56341552734375, \"iteration\": 825, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 166.54440307617188, \"iteration\": 826, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 180.9608154296875, \"iteration\": 827, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 145.853271484375, \"iteration\": 828, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 165.429443359375, \"iteration\": 829, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 195.7279815673828, \"iteration\": 830, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 169.29429626464844, \"iteration\": 831, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 142.60743713378906, \"iteration\": 832, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 176.1319580078125, \"iteration\": 833, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 141.64097595214844, \"iteration\": 834, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 156.74468994140625, \"iteration\": 835, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 133.20095825195312, \"iteration\": 836, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 168.2689208984375, \"iteration\": 837, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 193.92227172851562, \"iteration\": 838, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 226.40121459960938, \"iteration\": 839, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 186.37890625, \"iteration\": 840, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 224.65673828125, \"iteration\": 841, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 184.85459899902344, \"iteration\": 842, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 185.1967315673828, \"iteration\": 843, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 166.56256103515625, \"iteration\": 844, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 186.50881958007812, \"iteration\": 845, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 216.5562744140625, \"iteration\": 846, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 192.36874389648438, \"iteration\": 847, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 133.3876495361328, \"iteration\": 848, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 203.939697265625, \"iteration\": 849, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 176.40713500976562, \"iteration\": 850, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 181.57986450195312, \"iteration\": 851, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 176.4198455810547, \"iteration\": 852, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 198.39112854003906, \"iteration\": 853, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 170.8766326904297, \"iteration\": 854, \"epoch\": 5}, {\"training_acc\": 0.921875, \"training_loss\": 173.5287322998047, \"iteration\": 855, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 255.16571044921875, \"iteration\": 856, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 147.8856201171875, \"iteration\": 857, \"epoch\": 5}, {\"training_acc\": 0.921875, \"training_loss\": 193.17623901367188, \"iteration\": 858, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 181.86294555664062, \"iteration\": 859, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 227.82469177246094, \"iteration\": 860, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 192.27134704589844, \"iteration\": 861, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 181.81289672851562, \"iteration\": 862, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 187.2994384765625, \"iteration\": 863, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 194.964599609375, \"iteration\": 864, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 194.85787963867188, \"iteration\": 865, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 171.33572387695312, \"iteration\": 866, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 202.1826629638672, \"iteration\": 867, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 170.61135864257812, \"iteration\": 868, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 204.7154083251953, \"iteration\": 869, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 184.84429931640625, \"iteration\": 870, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 206.53968811035156, \"iteration\": 871, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 197.29043579101562, \"iteration\": 872, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 186.2058563232422, \"iteration\": 873, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 230.07070922851562, \"iteration\": 874, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 133.37240600585938, \"iteration\": 875, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 169.6070556640625, \"iteration\": 876, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 167.3907470703125, \"iteration\": 877, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 216.2442626953125, \"iteration\": 878, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 185.94451904296875, \"iteration\": 879, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 226.42808532714844, \"iteration\": 880, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 162.32981872558594, \"iteration\": 881, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 223.4436798095703, \"iteration\": 882, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 194.01620483398438, \"iteration\": 883, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 182.98837280273438, \"iteration\": 884, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 149.0198974609375, \"iteration\": 885, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 209.13433837890625, \"iteration\": 886, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 161.37161254882812, \"iteration\": 887, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 145.87571716308594, \"iteration\": 888, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 187.64999389648438, \"iteration\": 889, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 23.350122451782227, \"iteration\": 890, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 136.2484588623047, \"iteration\": 891, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 174.43614196777344, \"iteration\": 892, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 136.8732452392578, \"iteration\": 893, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 228.09051513671875, \"iteration\": 894, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 164.0478973388672, \"iteration\": 895, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 158.84535217285156, \"iteration\": 896, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 208.31373596191406, \"iteration\": 897, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 192.41995239257812, \"iteration\": 898, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 215.02163696289062, \"iteration\": 899, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 208.6332550048828, \"iteration\": 900, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 208.44769287109375, \"iteration\": 901, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 197.53077697753906, \"iteration\": 902, \"epoch\": 6}, {\"training_acc\": 0.9609375, \"training_loss\": 209.91680908203125, \"iteration\": 903, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 203.79293823242188, \"iteration\": 904, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 173.58786010742188, \"iteration\": 905, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 216.89749145507812, \"iteration\": 906, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 193.0584259033203, \"iteration\": 907, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 193.69842529296875, \"iteration\": 908, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 169.446044921875, \"iteration\": 909, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 158.8941650390625, \"iteration\": 910, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 164.05039978027344, \"iteration\": 911, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 193.80419921875, \"iteration\": 912, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 182.7896270751953, \"iteration\": 913, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 134.8634033203125, \"iteration\": 914, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 159.15875244140625, \"iteration\": 915, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 198.1738739013672, \"iteration\": 916, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 193.81573486328125, \"iteration\": 917, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 160.82736206054688, \"iteration\": 918, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 182.86349487304688, \"iteration\": 919, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 173.60574340820312, \"iteration\": 920, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 168.56387329101562, \"iteration\": 921, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 183.03421020507812, \"iteration\": 922, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 213.65249633789062, \"iteration\": 923, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 253.52435302734375, \"iteration\": 924, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 238.31185913085938, \"iteration\": 925, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 243.0752410888672, \"iteration\": 926, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 189.05307006835938, \"iteration\": 927, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 173.49317932128906, \"iteration\": 928, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 247.90191650390625, \"iteration\": 929, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 168.9348907470703, \"iteration\": 930, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 213.82418823242188, \"iteration\": 931, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 198.07373046875, \"iteration\": 932, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 173.199951171875, \"iteration\": 933, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 167.99407958984375, \"iteration\": 934, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 169.0810546875, \"iteration\": 935, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 165.30081176757812, \"iteration\": 936, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 192.2596893310547, \"iteration\": 937, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 207.48617553710938, \"iteration\": 938, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 173.11212158203125, \"iteration\": 939, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 213.1398162841797, \"iteration\": 940, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 144.4705810546875, \"iteration\": 941, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 192.31851196289062, \"iteration\": 942, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 165.23480224609375, \"iteration\": 943, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 159.18118286132812, \"iteration\": 944, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 177.89707946777344, \"iteration\": 945, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 212.6495361328125, \"iteration\": 946, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 222.2516632080078, \"iteration\": 947, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 198.3388214111328, \"iteration\": 948, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 222.59950256347656, \"iteration\": 949, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 164.38905334472656, \"iteration\": 950, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 158.0535888671875, \"iteration\": 951, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 218.17938232421875, \"iteration\": 952, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 182.71017456054688, \"iteration\": 953, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 163.01283264160156, \"iteration\": 954, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 173.00462341308594, \"iteration\": 955, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 218.6007080078125, \"iteration\": 956, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 197.31077575683594, \"iteration\": 957, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 153.5836639404297, \"iteration\": 958, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 135.98817443847656, \"iteration\": 959, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 187.846435546875, \"iteration\": 960, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 168.27333068847656, \"iteration\": 961, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 182.02362060546875, \"iteration\": 962, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 204.24990844726562, \"iteration\": 963, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 117.99934387207031, \"iteration\": 964, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 158.86080932617188, \"iteration\": 965, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 164.96240234375, \"iteration\": 966, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 201.7574462890625, \"iteration\": 967, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 174.4335174560547, \"iteration\": 968, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 188.35943603515625, \"iteration\": 969, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 208.37120056152344, \"iteration\": 970, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 182.1580352783203, \"iteration\": 971, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 168.84564208984375, \"iteration\": 972, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 158.70045471191406, \"iteration\": 973, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 200.34286499023438, \"iteration\": 974, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 167.99600219726562, \"iteration\": 975, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 204.2669219970703, \"iteration\": 976, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 214.02755737304688, \"iteration\": 977, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 160.55311584472656, \"iteration\": 978, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 190.21205139160156, \"iteration\": 979, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 188.4370574951172, \"iteration\": 980, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 217.19129943847656, \"iteration\": 981, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 178.16668701171875, \"iteration\": 982, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 148.9390411376953, \"iteration\": 983, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 197.80027770996094, \"iteration\": 984, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 175.53839111328125, \"iteration\": 985, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 170.1590576171875, \"iteration\": 986, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 145.30792236328125, \"iteration\": 987, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 179.12696838378906, \"iteration\": 988, \"epoch\": 6}, {\"training_acc\": 0.9609375, \"training_loss\": 190.25831604003906, \"iteration\": 989, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 177.63345336914062, \"iteration\": 990, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 173.47079467773438, \"iteration\": 991, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 208.9146270751953, \"iteration\": 992, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 163.583984375, \"iteration\": 993, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 164.51589965820312, \"iteration\": 994, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 213.33612060546875, \"iteration\": 995, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 168.4075927734375, \"iteration\": 996, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 223.6681671142578, \"iteration\": 997, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 216.43380737304688, \"iteration\": 998, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 221.99205017089844, \"iteration\": 999, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 164.64364624023438, \"iteration\": 1000, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 227.15994262695312, \"iteration\": 1001, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 207.43606567382812, \"iteration\": 1002, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 150.25050354003906, \"iteration\": 1003, \"epoch\": 6}, {\"training_acc\": 0.96875, \"training_loss\": 159.53651428222656, \"iteration\": 1004, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 222.66244506835938, \"iteration\": 1005, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 200.16180419921875, \"iteration\": 1006, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 215.0076446533203, \"iteration\": 1007, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 122.00251770019531, \"iteration\": 1008, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 222.39060974121094, \"iteration\": 1009, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 175.36061096191406, \"iteration\": 1010, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 244.25955200195312, \"iteration\": 1011, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 151.89889526367188, \"iteration\": 1012, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 189.33114624023438, \"iteration\": 1013, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 222.4578094482422, \"iteration\": 1014, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 177.37660217285156, \"iteration\": 1015, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 208.44369506835938, \"iteration\": 1016, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 198.14706420898438, \"iteration\": 1017, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 168.44644165039062, \"iteration\": 1018, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 178.74667358398438, \"iteration\": 1019, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 214.70773315429688, \"iteration\": 1020, \"epoch\": 6}, {\"training_acc\": 0.96875, \"training_loss\": 174.31234741210938, \"iteration\": 1021, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 158.78958129882812, \"iteration\": 1022, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 149.1473388671875, \"iteration\": 1023, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 131.12564086914062, \"iteration\": 1024, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 183.63363647460938, \"iteration\": 1025, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 164.05889892578125, \"iteration\": 1026, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 158.33424377441406, \"iteration\": 1027, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 202.25546264648438, \"iteration\": 1028, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 189.33604431152344, \"iteration\": 1029, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 197.99668884277344, \"iteration\": 1030, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 178.9567413330078, \"iteration\": 1031, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 202.93527221679688, \"iteration\": 1032, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 182.9723358154297, \"iteration\": 1033, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 202.5608673095703, \"iteration\": 1034, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 170.8994903564453, \"iteration\": 1035, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 223.11578369140625, \"iteration\": 1036, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 159.26486206054688, \"iteration\": 1037, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 242.5643310546875, \"iteration\": 1038, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 217.92794799804688, \"iteration\": 1039, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 140.7000732421875, \"iteration\": 1040, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 163.17178344726562, \"iteration\": 1041, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 182.68284606933594, \"iteration\": 1042, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 203.36387634277344, \"iteration\": 1043, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 173.58547973632812, \"iteration\": 1044, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 217.75440979003906, \"iteration\": 1045, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 178.2108917236328, \"iteration\": 1046, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 187.52249145507812, \"iteration\": 1047, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 177.7725830078125, \"iteration\": 1048, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 172.39414978027344, \"iteration\": 1049, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 183.23728942871094, \"iteration\": 1050, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 193.0658416748047, \"iteration\": 1051, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 159.65972900390625, \"iteration\": 1052, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 159.2603759765625, \"iteration\": 1053, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 172.99130249023438, \"iteration\": 1054, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 196.97866821289062, \"iteration\": 1055, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 172.8760528564453, \"iteration\": 1056, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 218.58399963378906, \"iteration\": 1057, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 182.91038513183594, \"iteration\": 1058, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 237.71917724609375, \"iteration\": 1059, \"epoch\": 6}, {\"training_acc\": 0.9609375, \"training_loss\": 195.9423828125, \"iteration\": 1060, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 221.92486572265625, \"iteration\": 1061, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 131.1761474609375, \"iteration\": 1062, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 144.68081665039062, \"iteration\": 1063, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 141.7029266357422, \"iteration\": 1064, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 202.3428955078125, \"iteration\": 1065, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 174.46185302734375, \"iteration\": 1066, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 197.54788208007812, \"iteration\": 1067, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 5.688394546508789, \"iteration\": 1068, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 201.68618774414062, \"iteration\": 1069, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 153.12831115722656, \"iteration\": 1070, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 167.14065551757812, \"iteration\": 1071, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 201.44992065429688, \"iteration\": 1072, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 211.2550048828125, \"iteration\": 1073, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 139.45175170898438, \"iteration\": 1074, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.29635620117188, \"iteration\": 1075, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 177.32223510742188, \"iteration\": 1076, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 153.05514526367188, \"iteration\": 1077, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 206.37496948242188, \"iteration\": 1078, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.95010375976562, \"iteration\": 1079, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.44944763183594, \"iteration\": 1080, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 206.39993286132812, \"iteration\": 1081, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 186.8196258544922, \"iteration\": 1082, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 176.89599609375, \"iteration\": 1083, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.33522033691406, \"iteration\": 1084, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 177.77305603027344, \"iteration\": 1085, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.86962890625, \"iteration\": 1086, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 241.38543701171875, \"iteration\": 1087, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 221.6439208984375, \"iteration\": 1088, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 211.3704376220703, \"iteration\": 1089, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 217.0022430419922, \"iteration\": 1090, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 167.4429931640625, \"iteration\": 1091, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 171.98577880859375, \"iteration\": 1092, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 162.18556213378906, \"iteration\": 1093, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 177.3311767578125, \"iteration\": 1094, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 181.48924255371094, \"iteration\": 1095, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 210.9281768798828, \"iteration\": 1096, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 191.42335510253906, \"iteration\": 1097, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 162.93392944335938, \"iteration\": 1098, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 148.58529663085938, \"iteration\": 1099, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 192.281005859375, \"iteration\": 1100, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.24046325683594, \"iteration\": 1101, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 162.50814819335938, \"iteration\": 1102, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 162.80479431152344, \"iteration\": 1103, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 162.1844940185547, \"iteration\": 1104, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 181.97955322265625, \"iteration\": 1105, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.299560546875, \"iteration\": 1106, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.1879119873047, \"iteration\": 1107, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 206.17233276367188, \"iteration\": 1108, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 177.19764709472656, \"iteration\": 1109, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 162.4171142578125, \"iteration\": 1110, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 186.33309936523438, \"iteration\": 1111, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 158.36952209472656, \"iteration\": 1112, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 169.23553466796875, \"iteration\": 1113, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 201.14877319335938, \"iteration\": 1114, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 176.46585083007812, \"iteration\": 1115, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 176.80470275878906, \"iteration\": 1116, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 177.45652770996094, \"iteration\": 1117, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 167.9486846923828, \"iteration\": 1118, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 226.2036895751953, \"iteration\": 1119, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 191.90972900390625, \"iteration\": 1120, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.5596923828125, \"iteration\": 1121, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 162.4727325439453, \"iteration\": 1122, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 201.68028259277344, \"iteration\": 1123, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 211.34866333007812, \"iteration\": 1124, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 186.66885375976562, \"iteration\": 1125, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 153.6168975830078, \"iteration\": 1126, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 172.38963317871094, \"iteration\": 1127, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 143.4473876953125, \"iteration\": 1128, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.27471923828125, \"iteration\": 1129, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 230.96878051757812, \"iteration\": 1130, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 206.03981018066406, \"iteration\": 1131, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 192.8101348876953, \"iteration\": 1132, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 186.53201293945312, \"iteration\": 1133, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 167.7104034423828, \"iteration\": 1134, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 186.58514404296875, \"iteration\": 1135, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 157.99026489257812, \"iteration\": 1136, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 181.594482421875, \"iteration\": 1137, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 120.58676147460938, \"iteration\": 1138, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 236.35838317871094, \"iteration\": 1139, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 182.35867309570312, \"iteration\": 1140, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 201.07102966308594, \"iteration\": 1141, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 120.65681457519531, \"iteration\": 1142, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 157.55926513671875, \"iteration\": 1143, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 167.91598510742188, \"iteration\": 1144, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 182.53091430664062, \"iteration\": 1145, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 143.71658325195312, \"iteration\": 1146, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 162.32391357421875, \"iteration\": 1147, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 241.27313232421875, \"iteration\": 1148, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 187.54818725585938, \"iteration\": 1149, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 232.1348419189453, \"iteration\": 1150, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 152.95431518554688, \"iteration\": 1151, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 206.3728790283203, \"iteration\": 1152, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 211.59024047851562, \"iteration\": 1153, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 201.32876586914062, \"iteration\": 1154, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 152.62109375, \"iteration\": 1155, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 148.466064453125, \"iteration\": 1156, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 186.61248779296875, \"iteration\": 1157, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 143.97714233398438, \"iteration\": 1158, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 191.57614135742188, \"iteration\": 1159, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 162.08056640625, \"iteration\": 1160, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.51211547851562, \"iteration\": 1161, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 181.56646728515625, \"iteration\": 1162, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 201.21109008789062, \"iteration\": 1163, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 187.6009521484375, \"iteration\": 1164, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 176.68612670898438, \"iteration\": 1165, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 186.28582763671875, \"iteration\": 1166, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 172.14076232910156, \"iteration\": 1167, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 138.89035034179688, \"iteration\": 1168, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 181.7689208984375, \"iteration\": 1169, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 186.8790740966797, \"iteration\": 1170, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 163.44033813476562, \"iteration\": 1171, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 186.76116943359375, \"iteration\": 1172, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 216.63961791992188, \"iteration\": 1173, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 177.16397094726562, \"iteration\": 1174, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 221.1634521484375, \"iteration\": 1175, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 241.68789672851562, \"iteration\": 1176, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 223.24771118164062, \"iteration\": 1177, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 171.86660766601562, \"iteration\": 1178, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 177.4654541015625, \"iteration\": 1179, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 231.86410522460938, \"iteration\": 1180, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 191.73638916015625, \"iteration\": 1181, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 177.2800750732422, \"iteration\": 1182, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 172.46298217773438, \"iteration\": 1183, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 167.4396514892578, \"iteration\": 1184, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 143.65760803222656, \"iteration\": 1185, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 211.08551025390625, \"iteration\": 1186, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 197.29306030273438, \"iteration\": 1187, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 148.3725128173828, \"iteration\": 1188, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 168.167236328125, \"iteration\": 1189, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 215.97943115234375, \"iteration\": 1190, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 201.18386840820312, \"iteration\": 1191, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 201.26898193359375, \"iteration\": 1192, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.65933227539062, \"iteration\": 1193, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 201.53451538085938, \"iteration\": 1194, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 177.29290771484375, \"iteration\": 1195, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 231.26910400390625, \"iteration\": 1196, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 176.88131713867188, \"iteration\": 1197, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 157.73703002929688, \"iteration\": 1198, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 162.6918487548828, \"iteration\": 1199, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 191.65509033203125, \"iteration\": 1200, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 181.6224822998047, \"iteration\": 1201, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 181.4284210205078, \"iteration\": 1202, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 182.49122619628906, \"iteration\": 1203, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 215.78500366210938, \"iteration\": 1204, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 148.46429443359375, \"iteration\": 1205, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 182.12498474121094, \"iteration\": 1206, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 148.34783935546875, \"iteration\": 1207, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 167.92576599121094, \"iteration\": 1208, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 143.55703735351562, \"iteration\": 1209, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 139.7352294921875, \"iteration\": 1210, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 191.21737670898438, \"iteration\": 1211, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 177.07058715820312, \"iteration\": 1212, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 172.13873291015625, \"iteration\": 1213, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 211.61227416992188, \"iteration\": 1214, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 148.40367126464844, \"iteration\": 1215, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 186.74813842773438, \"iteration\": 1216, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 198.44825744628906, \"iteration\": 1217, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 191.3201141357422, \"iteration\": 1218, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 186.61788940429688, \"iteration\": 1219, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 172.42095947265625, \"iteration\": 1220, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 221.72512817382812, \"iteration\": 1221, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 191.1772003173828, \"iteration\": 1222, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 216.1344451904297, \"iteration\": 1223, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 134.905517578125, \"iteration\": 1224, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 167.18365478515625, \"iteration\": 1225, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 148.60223388671875, \"iteration\": 1226, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 167.51068115234375, \"iteration\": 1227, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 157.37696838378906, \"iteration\": 1228, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 171.97769165039062, \"iteration\": 1229, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 154.09246826171875, \"iteration\": 1230, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 187.0099334716797, \"iteration\": 1231, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 176.4589080810547, \"iteration\": 1232, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.30776977539062, \"iteration\": 1233, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 216.185546875, \"iteration\": 1234, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 206.8678436279297, \"iteration\": 1235, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 231.3118896484375, \"iteration\": 1236, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 252.522705078125, \"iteration\": 1237, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 212.82113647460938, \"iteration\": 1238, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 196.63961791992188, \"iteration\": 1239, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 143.97805786132812, \"iteration\": 1240, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 182.8450927734375, \"iteration\": 1241, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 172.4998321533203, \"iteration\": 1242, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 158.00958251953125, \"iteration\": 1243, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 211.47137451171875, \"iteration\": 1244, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 172.09771728515625, \"iteration\": 1245, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 8.896157264709473, \"iteration\": 1246, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 143.28817749023438, \"iteration\": 1247, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 210.84371948242188, \"iteration\": 1248, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 143.54501342773438, \"iteration\": 1249, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 221.052978515625, \"iteration\": 1250, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.306884765625, \"iteration\": 1251, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 196.14183044433594, \"iteration\": 1252, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 148.3375244140625, \"iteration\": 1253, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 148.00863647460938, \"iteration\": 1254, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 167.33755493164062, \"iteration\": 1255, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.23800659179688, \"iteration\": 1256, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 226.14332580566406, \"iteration\": 1257, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 177.38455200195312, \"iteration\": 1258, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 152.8096923828125, \"iteration\": 1259, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.47000122070312, \"iteration\": 1260, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.34158325195312, \"iteration\": 1261, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.38241577148438, \"iteration\": 1262, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.3385009765625, \"iteration\": 1263, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.2140655517578, \"iteration\": 1264, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 195.8843994140625, \"iteration\": 1265, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 143.11819458007812, \"iteration\": 1266, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 196.03118896484375, \"iteration\": 1267, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 206.13589477539062, \"iteration\": 1268, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 147.88656616210938, \"iteration\": 1269, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.11343383789062, \"iteration\": 1270, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 205.6100616455078, \"iteration\": 1271, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 205.70468139648438, \"iteration\": 1272, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 211.01004028320312, \"iteration\": 1273, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 220.64151000976562, \"iteration\": 1274, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 191.111083984375, \"iteration\": 1275, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.33709716796875, \"iteration\": 1276, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 235.847900390625, \"iteration\": 1277, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.4613494873047, \"iteration\": 1278, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.23440551757812, \"iteration\": 1279, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 138.66766357421875, \"iteration\": 1280, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 162.04078674316406, \"iteration\": 1281, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 201.0400390625, \"iteration\": 1282, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 166.89239501953125, \"iteration\": 1283, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.15557861328125, \"iteration\": 1284, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.17666625976562, \"iteration\": 1285, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.36154174804688, \"iteration\": 1286, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 205.7654266357422, \"iteration\": 1287, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 220.70083618164062, \"iteration\": 1288, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 138.66644287109375, \"iteration\": 1289, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.28622436523438, \"iteration\": 1290, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 195.88258361816406, \"iteration\": 1291, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.77243041992188, \"iteration\": 1292, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 200.80319213867188, \"iteration\": 1293, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.19403076171875, \"iteration\": 1294, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.12994384765625, \"iteration\": 1295, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 216.0552978515625, \"iteration\": 1296, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 166.95864868164062, \"iteration\": 1297, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.36473083496094, \"iteration\": 1298, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.41610717773438, \"iteration\": 1299, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 215.7444305419922, \"iteration\": 1300, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.32179260253906, \"iteration\": 1301, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 191.16616821289062, \"iteration\": 1302, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 230.6820068359375, \"iteration\": 1303, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 240.85536193847656, \"iteration\": 1304, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 171.7642822265625, \"iteration\": 1305, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 191.44027709960938, \"iteration\": 1306, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 129.57363891601562, \"iteration\": 1307, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 133.9906463623047, \"iteration\": 1308, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 152.5196990966797, \"iteration\": 1309, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 143.513427734375, \"iteration\": 1310, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 230.79916381835938, \"iteration\": 1311, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.3288116455078, \"iteration\": 1312, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 200.9136962890625, \"iteration\": 1313, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.42042541503906, \"iteration\": 1314, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.2842559814453, \"iteration\": 1315, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.36398315429688, \"iteration\": 1316, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 106.66226959228516, \"iteration\": 1317, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.25155639648438, \"iteration\": 1318, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 195.83782958984375, \"iteration\": 1319, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.31668090820312, \"iteration\": 1320, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 205.73062133789062, \"iteration\": 1321, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 190.90066528320312, \"iteration\": 1322, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 161.99148559570312, \"iteration\": 1323, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.11593627929688, \"iteration\": 1324, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 129.34585571289062, \"iteration\": 1325, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 166.79576110839844, \"iteration\": 1326, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 134.04110717773438, \"iteration\": 1327, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.2913360595703, \"iteration\": 1328, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 195.95443725585938, \"iteration\": 1329, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 205.74261474609375, \"iteration\": 1330, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 215.56805419921875, \"iteration\": 1331, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 215.7166290283203, \"iteration\": 1332, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.4407196044922, \"iteration\": 1333, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 161.94699096679688, \"iteration\": 1334, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 205.719970703125, \"iteration\": 1335, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.14344787597656, \"iteration\": 1336, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.23486328125, \"iteration\": 1337, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 167.10226440429688, \"iteration\": 1338, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 200.8163604736328, \"iteration\": 1339, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 152.63038635253906, \"iteration\": 1340, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 240.87872314453125, \"iteration\": 1341, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 215.79251098632812, \"iteration\": 1342, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 220.59884643554688, \"iteration\": 1343, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 138.4529571533203, \"iteration\": 1344, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.54457092285156, \"iteration\": 1345, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 195.9182586669922, \"iteration\": 1346, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.38055419921875, \"iteration\": 1347, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 171.5579833984375, \"iteration\": 1348, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 236.21775817871094, \"iteration\": 1349, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.1981658935547, \"iteration\": 1350, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 152.50662231445312, \"iteration\": 1351, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.32632446289062, \"iteration\": 1352, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.35699462890625, \"iteration\": 1353, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 166.7161407470703, \"iteration\": 1354, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 205.683349609375, \"iteration\": 1355, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 172.013671875, \"iteration\": 1356, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 167.11074829101562, \"iteration\": 1357, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 191.1803436279297, \"iteration\": 1358, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 191.18804931640625, \"iteration\": 1359, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 148.0062255859375, \"iteration\": 1360, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.55897521972656, \"iteration\": 1361, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 200.90072631835938, \"iteration\": 1362, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 143.08047485351562, \"iteration\": 1363, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 240.899658203125, \"iteration\": 1364, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 185.984130859375, \"iteration\": 1365, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 147.7989501953125, \"iteration\": 1366, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.36431884765625, \"iteration\": 1367, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 138.481201171875, \"iteration\": 1368, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 153.54481506347656, \"iteration\": 1369, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 195.99647521972656, \"iteration\": 1370, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 236.05787658691406, \"iteration\": 1371, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.2733917236328, \"iteration\": 1372, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 221.68881225585938, \"iteration\": 1373, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 220.8981475830078, \"iteration\": 1374, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.82122802734375, \"iteration\": 1375, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.71847534179688, \"iteration\": 1376, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 191.42684936523438, \"iteration\": 1377, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.29196166992188, \"iteration\": 1378, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 220.62515258789062, \"iteration\": 1379, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 167.50096130371094, \"iteration\": 1380, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 200.86026000976562, \"iteration\": 1381, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.287353515625, \"iteration\": 1382, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 196.19125366210938, \"iteration\": 1383, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 192.25184631347656, \"iteration\": 1384, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 171.5399169921875, \"iteration\": 1385, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.55313110351562, \"iteration\": 1386, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 200.90130615234375, \"iteration\": 1387, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 246.10882568359375, \"iteration\": 1388, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 167.40257263183594, \"iteration\": 1389, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 215.95437622070312, \"iteration\": 1390, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.46548461914062, \"iteration\": 1391, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 195.92161560058594, \"iteration\": 1392, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 196.19032287597656, \"iteration\": 1393, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 161.875732421875, \"iteration\": 1394, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 157.44541931152344, \"iteration\": 1395, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 153.1387176513672, \"iteration\": 1396, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 195.95120239257812, \"iteration\": 1397, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 200.9739990234375, \"iteration\": 1398, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 147.8156280517578, \"iteration\": 1399, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 120.52528381347656, \"iteration\": 1400, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 215.66357421875, \"iteration\": 1401, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.47702026367188, \"iteration\": 1402, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 196.1018829345703, \"iteration\": 1403, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 191.02093505859375, \"iteration\": 1404, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.16836547851562, \"iteration\": 1405, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 215.6581573486328, \"iteration\": 1406, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 196.23126220703125, \"iteration\": 1407, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 210.70028686523438, \"iteration\": 1408, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 143.13754272460938, \"iteration\": 1409, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 176.44708251953125, \"iteration\": 1410, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 152.56459045410156, \"iteration\": 1411, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 190.9567108154297, \"iteration\": 1412, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 138.77310180664062, \"iteration\": 1413, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 235.9208221435547, \"iteration\": 1414, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 236.88217163085938, \"iteration\": 1415, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 186.30813598632812, \"iteration\": 1416, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 200.822509765625, \"iteration\": 1417, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 226.083740234375, \"iteration\": 1418, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.53472900390625, \"iteration\": 1419, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 172.54275512695312, \"iteration\": 1420, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 181.1749725341797, \"iteration\": 1421, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 171.67391967773438, \"iteration\": 1422, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 241.01699829101562, \"iteration\": 1423, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 16.671998977661133, \"iteration\": 1424, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 201.439453125, \"iteration\": 1425, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 176.4131622314453, \"iteration\": 1426, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 195.81378173828125, \"iteration\": 1427, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 147.97128295898438, \"iteration\": 1428, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.83822631835938, \"iteration\": 1429, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 230.73052978515625, \"iteration\": 1430, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 205.80572509765625, \"iteration\": 1431, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 167.20272827148438, \"iteration\": 1432, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 167.2454376220703, \"iteration\": 1433, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.27468872070312, \"iteration\": 1434, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 200.71400451660156, \"iteration\": 1435, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 190.84339904785156, \"iteration\": 1436, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 191.29302978515625, \"iteration\": 1437, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 133.7824249267578, \"iteration\": 1438, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 205.56390380859375, \"iteration\": 1439, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 200.78262329101562, \"iteration\": 1440, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 210.81570434570312, \"iteration\": 1441, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 205.5944366455078, \"iteration\": 1442, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 115.63990020751953, \"iteration\": 1443, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.3442840576172, \"iteration\": 1444, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 133.88681030273438, \"iteration\": 1445, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 210.80862426757812, \"iteration\": 1446, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 157.23403930664062, \"iteration\": 1447, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 205.62606811523438, \"iteration\": 1448, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 215.79046630859375, \"iteration\": 1449, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 186.28599548339844, \"iteration\": 1450, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.68763732910156, \"iteration\": 1451, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 176.5675048828125, \"iteration\": 1452, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 166.73190307617188, \"iteration\": 1453, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 176.42950439453125, \"iteration\": 1454, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 196.37139892578125, \"iteration\": 1455, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.9451904296875, \"iteration\": 1456, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 215.68508911132812, \"iteration\": 1457, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 158.0275115966797, \"iteration\": 1458, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 245.8375244140625, \"iteration\": 1459, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.14662170410156, \"iteration\": 1460, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 129.39112854003906, \"iteration\": 1461, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 166.6427001953125, \"iteration\": 1462, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 230.56930541992188, \"iteration\": 1463, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 195.79376220703125, \"iteration\": 1464, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 143.03350830078125, \"iteration\": 1465, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 161.87664794921875, \"iteration\": 1466, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 176.21575927734375, \"iteration\": 1467, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 152.3979949951172, \"iteration\": 1468, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 147.71644592285156, \"iteration\": 1469, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 251.37612915039062, \"iteration\": 1470, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.08909606933594, \"iteration\": 1471, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 161.95465087890625, \"iteration\": 1472, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 195.78607177734375, \"iteration\": 1473, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 210.5548095703125, \"iteration\": 1474, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 205.74459838867188, \"iteration\": 1475, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 235.77569580078125, \"iteration\": 1476, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 157.32437133789062, \"iteration\": 1477, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 186.13217163085938, \"iteration\": 1478, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 152.46212768554688, \"iteration\": 1479, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 186.1688232421875, \"iteration\": 1480, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 166.83181762695312, \"iteration\": 1481, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 215.9906463623047, \"iteration\": 1482, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 220.670166015625, \"iteration\": 1483, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.08717346191406, \"iteration\": 1484, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.2090606689453, \"iteration\": 1485, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 185.93275451660156, \"iteration\": 1486, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.67208862304688, \"iteration\": 1487, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.09243774414062, \"iteration\": 1488, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.7689666748047, \"iteration\": 1489, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 190.88755798339844, \"iteration\": 1490, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 200.6441650390625, \"iteration\": 1491, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 176.3118438720703, \"iteration\": 1492, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 166.77120971679688, \"iteration\": 1493, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 120.478759765625, \"iteration\": 1494, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 200.67971801757812, \"iteration\": 1495, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 191.41909790039062, \"iteration\": 1496, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 161.95919799804688, \"iteration\": 1497, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 129.1611328125, \"iteration\": 1498, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.0838165283203, \"iteration\": 1499, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 210.82789611816406, \"iteration\": 1500, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 166.70974731445312, \"iteration\": 1501, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 138.3824920654297, \"iteration\": 1502, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.68136596679688, \"iteration\": 1503, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 210.55050659179688, \"iteration\": 1504, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 195.8551483154297, \"iteration\": 1505, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.52975463867188, \"iteration\": 1506, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 176.48641967773438, \"iteration\": 1507, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 152.95692443847656, \"iteration\": 1508, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.09518432617188, \"iteration\": 1509, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 195.81285095214844, \"iteration\": 1510, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 167.137451171875, \"iteration\": 1511, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 124.6274642944336, \"iteration\": 1512, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 166.62493896484375, \"iteration\": 1513, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.07681274414062, \"iteration\": 1514, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.671142578125, \"iteration\": 1515, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 166.72421264648438, \"iteration\": 1516, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 240.98043823242188, \"iteration\": 1517, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 129.15150451660156, \"iteration\": 1518, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 186.36492919921875, \"iteration\": 1519, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 161.82138061523438, \"iteration\": 1520, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 190.77862548828125, \"iteration\": 1521, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 176.7550811767578, \"iteration\": 1522, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 147.87954711914062, \"iteration\": 1523, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.5460662841797, \"iteration\": 1524, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 166.59878540039062, \"iteration\": 1525, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 190.8804931640625, \"iteration\": 1526, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 166.9254608154297, \"iteration\": 1527, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 201.31007385253906, \"iteration\": 1528, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 176.2315673828125, \"iteration\": 1529, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 157.27874755859375, \"iteration\": 1530, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.46434020996094, \"iteration\": 1531, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 191.02944946289062, \"iteration\": 1532, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 186.07211303710938, \"iteration\": 1533, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 191.15744018554688, \"iteration\": 1534, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 200.79698181152344, \"iteration\": 1535, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 241.7769012451172, \"iteration\": 1536, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 220.56822204589844, \"iteration\": 1537, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 196.0115509033203, \"iteration\": 1538, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 251.41139221191406, \"iteration\": 1539, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.55673217773438, \"iteration\": 1540, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.55636596679688, \"iteration\": 1541, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 205.60813903808594, \"iteration\": 1542, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 166.81781005859375, \"iteration\": 1543, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.40040588378906, \"iteration\": 1544, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.28268432617188, \"iteration\": 1545, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 210.78536987304688, \"iteration\": 1546, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 185.92625427246094, \"iteration\": 1547, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 195.9501495361328, \"iteration\": 1548, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.06100463867188, \"iteration\": 1549, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 200.72076416015625, \"iteration\": 1550, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 190.9858856201172, \"iteration\": 1551, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 185.9908447265625, \"iteration\": 1552, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.0303955078125, \"iteration\": 1553, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 188.68698120117188, \"iteration\": 1554, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 176.36053466796875, \"iteration\": 1555, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 182.08148193359375, \"iteration\": 1556, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 172.16433715820312, \"iteration\": 1557, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 185.9835205078125, \"iteration\": 1558, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 201.36550903320312, \"iteration\": 1559, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 157.73880004882812, \"iteration\": 1560, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.75299072265625, \"iteration\": 1561, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 232.03793334960938, \"iteration\": 1562, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 223.37948608398438, \"iteration\": 1563, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 166.70901489257812, \"iteration\": 1564, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 186.0094757080078, \"iteration\": 1565, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 167.69668579101562, \"iteration\": 1566, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 236.05718994140625, \"iteration\": 1567, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 182.19818115234375, \"iteration\": 1568, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 192.47909545898438, \"iteration\": 1569, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 178.54440307617188, \"iteration\": 1570, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 143.79302978515625, \"iteration\": 1571, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 207.2205810546875, \"iteration\": 1572, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 202.68496704101562, \"iteration\": 1573, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 211.44863891601562, \"iteration\": 1574, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 177.09605407714844, \"iteration\": 1575, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 236.9952392578125, \"iteration\": 1576, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 201.0821533203125, \"iteration\": 1577, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 176.45535278320312, \"iteration\": 1578, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 162.27200317382812, \"iteration\": 1579, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 158.0238037109375, \"iteration\": 1580, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 186.38523864746094, \"iteration\": 1581, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 162.11033630371094, \"iteration\": 1582, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 231.01129150390625, \"iteration\": 1583, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.82766723632812, \"iteration\": 1584, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 162.2059783935547, \"iteration\": 1585, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.21669006347656, \"iteration\": 1586, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 158.1785888671875, \"iteration\": 1587, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 215.79742431640625, \"iteration\": 1588, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 226.0164337158203, \"iteration\": 1589, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 201.07118225097656, \"iteration\": 1590, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 157.42807006835938, \"iteration\": 1591, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.92483520507812, \"iteration\": 1592, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 138.84039306640625, \"iteration\": 1593, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 148.00274658203125, \"iteration\": 1594, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 177.22103881835938, \"iteration\": 1595, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 191.0465087890625, \"iteration\": 1596, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 230.96322631835938, \"iteration\": 1597, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 171.67367553710938, \"iteration\": 1598, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 181.17538452148438, \"iteration\": 1599, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 186.08303833007812, \"iteration\": 1600, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 191.82659912109375, \"iteration\": 1601, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 1.4911859035491943, \"iteration\": 1602, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 200.7372283935547, \"iteration\": 1603, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 124.7149429321289, \"iteration\": 1604, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 210.65484619140625, \"iteration\": 1605, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 176.32774353027344, \"iteration\": 1606, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 120.07349395751953, \"iteration\": 1607, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 220.6481475830078, \"iteration\": 1608, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 192.1453399658203, \"iteration\": 1609, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 176.28369140625, \"iteration\": 1610, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 225.936767578125, \"iteration\": 1611, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 161.88720703125, \"iteration\": 1612, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.7283477783203, \"iteration\": 1613, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 157.24009704589844, \"iteration\": 1614, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 225.66148376464844, \"iteration\": 1615, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 166.84304809570312, \"iteration\": 1616, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.05728149414062, \"iteration\": 1617, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 167.89108276367188, \"iteration\": 1618, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.19332885742188, \"iteration\": 1619, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 167.29864501953125, \"iteration\": 1620, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 195.9708251953125, \"iteration\": 1621, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 211.1371612548828, \"iteration\": 1622, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.42868041992188, \"iteration\": 1623, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 205.588134765625, \"iteration\": 1624, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 221.8431854248047, \"iteration\": 1625, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 220.63491821289062, \"iteration\": 1626, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 195.8583984375, \"iteration\": 1627, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 161.8277587890625, \"iteration\": 1628, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 190.9490966796875, \"iteration\": 1629, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.051025390625, \"iteration\": 1630, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.3150634765625, \"iteration\": 1631, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 138.55099487304688, \"iteration\": 1632, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 245.8017578125, \"iteration\": 1633, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 129.42405700683594, \"iteration\": 1634, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 166.7791748046875, \"iteration\": 1635, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 190.796875, \"iteration\": 1636, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 143.41433715820312, \"iteration\": 1637, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 200.65293884277344, \"iteration\": 1638, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 201.96475219726562, \"iteration\": 1639, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 166.69871520996094, \"iteration\": 1640, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 129.1239013671875, \"iteration\": 1641, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 148.5279541015625, \"iteration\": 1642, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 220.66021728515625, \"iteration\": 1643, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.16171264648438, \"iteration\": 1644, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 200.6846923828125, \"iteration\": 1645, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 182.23184204101562, \"iteration\": 1646, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 200.63893127441406, \"iteration\": 1647, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 172.83370971679688, \"iteration\": 1648, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 157.27896118164062, \"iteration\": 1649, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.74026489257812, \"iteration\": 1650, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 153.1566162109375, \"iteration\": 1651, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 191.35426330566406, \"iteration\": 1652, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 152.656982421875, \"iteration\": 1653, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.1189422607422, \"iteration\": 1654, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 143.06336975097656, \"iteration\": 1655, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.33621215820312, \"iteration\": 1656, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 129.1408233642578, \"iteration\": 1657, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 172.06265258789062, \"iteration\": 1658, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 129.7010498046875, \"iteration\": 1659, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.49375915527344, \"iteration\": 1660, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.20242309570312, \"iteration\": 1661, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 190.9558868408203, \"iteration\": 1662, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 152.51504516601562, \"iteration\": 1663, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 190.87057495117188, \"iteration\": 1664, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 162.63388061523438, \"iteration\": 1665, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 241.63845825195312, \"iteration\": 1666, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.38717651367188, \"iteration\": 1667, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.4314422607422, \"iteration\": 1668, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 166.72671508789062, \"iteration\": 1669, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 152.74514770507812, \"iteration\": 1670, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 220.589111328125, \"iteration\": 1671, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 152.64019775390625, \"iteration\": 1672, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 172.703125, \"iteration\": 1673, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 191.15646362304688, \"iteration\": 1674, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 221.1375274658203, \"iteration\": 1675, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.0924072265625, \"iteration\": 1676, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 143.4383544921875, \"iteration\": 1677, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.17880249023438, \"iteration\": 1678, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 220.7230682373047, \"iteration\": 1679, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 147.88925170898438, \"iteration\": 1680, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 167.65972900390625, \"iteration\": 1681, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.89706420898438, \"iteration\": 1682, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 176.32357788085938, \"iteration\": 1683, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 147.94142150878906, \"iteration\": 1684, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.3831787109375, \"iteration\": 1685, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.48068237304688, \"iteration\": 1686, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 200.78627014160156, \"iteration\": 1687, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 185.95468139648438, \"iteration\": 1688, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 230.68063354492188, \"iteration\": 1689, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 205.5752716064453, \"iteration\": 1690, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.91656494140625, \"iteration\": 1691, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 201.081298828125, \"iteration\": 1692, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.2618865966797, \"iteration\": 1693, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.43894958496094, \"iteration\": 1694, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 221.7327880859375, \"iteration\": 1695, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 201.07859802246094, \"iteration\": 1696, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.577392578125, \"iteration\": 1697, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 195.6974334716797, \"iteration\": 1698, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 177.29286193847656, \"iteration\": 1699, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 125.15302276611328, \"iteration\": 1700, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 115.46238708496094, \"iteration\": 1701, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 220.77268981933594, \"iteration\": 1702, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 200.63954162597656, \"iteration\": 1703, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 163.64129638671875, \"iteration\": 1704, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.0823516845703, \"iteration\": 1705, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.18826293945312, \"iteration\": 1706, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 157.16639709472656, \"iteration\": 1707, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 133.82247924804688, \"iteration\": 1708, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 162.15640258789062, \"iteration\": 1709, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 226.6366729736328, \"iteration\": 1710, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 205.63839721679688, \"iteration\": 1711, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 157.31753540039062, \"iteration\": 1712, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 176.3253631591797, \"iteration\": 1713, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 201.72055053710938, \"iteration\": 1714, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 195.69662475585938, \"iteration\": 1715, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 190.98924255371094, \"iteration\": 1716, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.01922607421875, \"iteration\": 1717, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 176.3044891357422, \"iteration\": 1718, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 216.13851928710938, \"iteration\": 1719, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 195.77398681640625, \"iteration\": 1720, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.29026794433594, \"iteration\": 1721, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 102.70976257324219, \"iteration\": 1722, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.7467803955078, \"iteration\": 1723, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 152.49578857421875, \"iteration\": 1724, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 172.21856689453125, \"iteration\": 1725, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.89913940429688, \"iteration\": 1726, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 212.0753173828125, \"iteration\": 1727, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.7633056640625, \"iteration\": 1728, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.25799560546875, \"iteration\": 1729, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 200.83102416992188, \"iteration\": 1730, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 195.74090576171875, \"iteration\": 1731, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.52334594726562, \"iteration\": 1732, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.39573669433594, \"iteration\": 1733, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 205.6628875732422, \"iteration\": 1734, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 191.03982543945312, \"iteration\": 1735, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 190.93692016601562, \"iteration\": 1736, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 205.89019775390625, \"iteration\": 1737, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 157.42051696777344, \"iteration\": 1738, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 152.54005432128906, \"iteration\": 1739, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 235.75106811523438, \"iteration\": 1740, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 157.60922241210938, \"iteration\": 1741, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.0423583984375, \"iteration\": 1742, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 190.9342041015625, \"iteration\": 1743, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.94410705566406, \"iteration\": 1744, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 236.49154663085938, \"iteration\": 1745, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 187.99386596679688, \"iteration\": 1746, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 176.75582885742188, \"iteration\": 1747, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.83700561523438, \"iteration\": 1748, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 252.1228485107422, \"iteration\": 1749, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 191.270751953125, \"iteration\": 1750, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 192.02845764160156, \"iteration\": 1751, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 182.40032958984375, \"iteration\": 1752, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 230.67816162109375, \"iteration\": 1753, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 162.0214385986328, \"iteration\": 1754, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.75352478027344, \"iteration\": 1755, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 241.20965576171875, \"iteration\": 1756, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 246.03269958496094, \"iteration\": 1757, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 200.91366577148438, \"iteration\": 1758, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.40989685058594, \"iteration\": 1759, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 161.86436462402344, \"iteration\": 1760, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 242.24166870117188, \"iteration\": 1761, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 205.64688110351562, \"iteration\": 1762, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 195.72845458984375, \"iteration\": 1763, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 162.27537536621094, \"iteration\": 1764, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 196.6484375, \"iteration\": 1765, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 182.0223388671875, \"iteration\": 1766, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 210.96034240722656, \"iteration\": 1767, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 235.96926879882812, \"iteration\": 1768, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 167.238037109375, \"iteration\": 1769, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 186.07803344726562, \"iteration\": 1770, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 166.7799072265625, \"iteration\": 1771, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 236.01458740234375, \"iteration\": 1772, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 176.49819946289062, \"iteration\": 1773, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 181.44102478027344, \"iteration\": 1774, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 152.47894287109375, \"iteration\": 1775, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 171.6231231689453, \"iteration\": 1776, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 196.26730346679688, \"iteration\": 1777, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 201.16128540039062, \"iteration\": 1778, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 134.10491943359375, \"iteration\": 1779, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 19.806529998779297, \"iteration\": 1780, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# POC\n",
    "#%%\n",
    "balkan_file_list = [\n",
    "    'power-ba-train.tsv',\n",
    "    'power-rs-train.tsv',\n",
    "    'power-hr-train.tsv',\n",
    "]\n",
    "\n",
    "balkan_data = load_data(folder_path=\"data/train/power/\", file_list=balkan_file_list,text_head='text')\n",
    "train_raw, test_raw = split_data(balkan_data, test_size=0.2, random_state=0)\n",
    "\n",
    "print(len(train_raw), len(test_raw))\n",
    "print(\"Percentage of positive:\", sum(train_raw.labels) / len(train_raw), sum(test_raw.labels) / len(test_raw))\n",
    "\n",
    "print(\"Prepare data encoder...\")\n",
    "# train_encoder = TfidfVectorizer(sublinear_tf=True, analyzer=\"char\", ngram_range=(1,3))\n",
    "train_encoder = TfidfVectorizer(max_features=50000, analyzer=\"char\", ngram_range=(3,5))\n",
    "train_encoder.fit(train_raw.texts)\n",
    "print(\"Vocabulary\", len(train_encoder.vocabulary_))\n",
    "\n",
    "print(\"Prepare data...\")\n",
    "train_dataset = encode_data(train_raw, train_encoder)\n",
    "test_dataset = encode_data(test_raw, train_encoder)\n",
    "\n",
    "print(\"Train model\")\n",
    "models_dir = Path('models')\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "model_nn_balkan = NeuralNetwork(\n",
    "    input_size=len(train_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "if Path('models/model_nn_balkan.pt').exists() and USE_CACHE:\n",
    "    model_nn_balkan = load_model(model_nn_balkan, 'model_nn_balkan')\n",
    "else:\n",
    "    model_nn_balkan.fit(train_dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn_balkan, \"model_nn_balkan\")\n",
    "\n",
    "model_nn_balkan_results = evaluate_nn_model(model_nn_balkan, test_dataset)\n",
    "np.save('models/model_nn_balkan_results.npy', model_nn_balkan_results)\n",
    "print(model_nn_balkan_results)\n",
    "\n",
    "model_nn_balkan.cpu()\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn_balkan, train_config, train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-identification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

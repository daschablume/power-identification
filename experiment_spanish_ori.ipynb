{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from models import NeuralNetwork, TrainConfig, save_model, load_model, plot_results, evaluate\n",
    "from utils import load_data, split_data, encode_data, mapping_dict\n",
    "from pathlib import Path\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(\"Device: cuda\")\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    print(\"Device: cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "balkan_file_list = [\n",
    "    'power-es-train.tsv',\n",
    "    'power-es-ct-train.tsv',\n",
    "    'power-es-ga-train.tsv',\n",
    "    'power-es-pv-train.tsv'\n",
    "]\n",
    "data = load_data(folder_path=\"data/train/power/\", file_list=balkan_file_list,text_head='text')\n",
    "train_raw, test_raw = split_data(data, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data encoder...\n",
      "Prepare data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prepare data encoder...\")\n",
    "tfidf_encoder = TfidfVectorizer(max_features=50000)\n",
    "tfidf_encoder.fit(train_raw.texts)\n",
    "\n",
    "print(\"Prepare data...\")\n",
    "train_data_nn = encode_data(train_raw, tfidf_encoder)\n",
    "test_data_nn = encode_data(test_raw, tfidf_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 68/68 [00:01<00:00, 55.83batch/s, batch_accuracy=0.692, loss=0.573]\n",
      "Epoch 2: 100%|██████████| 68/68 [00:01<00:00, 65.83batch/s, batch_accuracy=0.925, loss=0.238]\n",
      "Epoch 3: 100%|██████████| 68/68 [00:01<00:00, 67.35batch/s, batch_accuracy=0.963, loss=0.108] \n",
      "Epoch 4: 100%|██████████| 68/68 [00:01<00:00, 65.93batch/s, batch_accuracy=1, loss=0.0037]    \n",
      "Epoch 5: 100%|██████████| 68/68 [00:00<00:00, 68.66batch/s, batch_accuracy=1, loss=0.00108]   \n",
      "Epoch 6: 100%|██████████| 68/68 [00:01<00:00, 66.58batch/s, batch_accuracy=1, loss=0.00056] \n",
      "Epoch 7: 100%|██████████| 68/68 [00:01<00:00, 65.61batch/s, batch_accuracy=1, loss=0.000561]\n",
      "Epoch 8: 100%|██████████| 68/68 [00:01<00:00, 67.77batch/s, batch_accuracy=1, loss=0.000137]\n",
      "Epoch 9: 100%|██████████| 68/68 [00:01<00:00, 66.44batch/s, batch_accuracy=1, loss=0.000113]\n",
      "Epoch 10: 100%|██████████| 68/68 [00:01<00:00, 63.82batch/s, batch_accuracy=1, loss=4.58e-5] \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m     y_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([test[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m test \u001b[38;5;129;01min\u001b[39;00m test_data_nn])\u001b[38;5;241m.\u001b[39mto(model_nn\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     37\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model_nn\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m\"\u001b[39m, roc_auc_score(y_test, y_pred))\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Plot training accuracy and loss side-by-side\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1775\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m \n\u001b[1;32m   1614\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1775\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1547\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1547\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/metrics/_classification.py:100\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m--> 100\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/multiclass.py:316\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/multiclass.py:171\u001b[0m, in \u001b[0;36mis_multilabel\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    169\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, VisibleDeprecationWarning)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_y_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (VisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/validation.py:1007\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1011\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/_array_api.py:746\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    744\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/torch/_tensor.py:1087\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1087\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# POC\n",
    "\n",
    "print(\"Train model\")\n",
    "models_dir = Path('models/es')\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    "    \n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_data_nn, batch_size=128, shuffle=True)\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "model_nn = NeuralNetwork(\n",
    "    input_size=len(tfidf_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn.pt').exists() and USE_CACHE:\n",
    "    model_nn = load_model(model_nn, models_dir, 'model_nn')\n",
    "else:\n",
    "    model_nn.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn, models_dir, \"model_nn\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # X_test = torch.stack([dta[0] for dta in test])\n",
    "    X_test = torch.stack([test[0] for test in test_data_nn]).to(model_nn.device)\n",
    "    y_test = torch.stack([test[1] for test in test_data_nn]).to(model_nn.device)\n",
    "    y_pred = model_nn.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7920384351407, 0.8911196911196911, 0.8386627906976745, None)\n",
      "AUC 0.7377409155186933\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-b2bde2d0a9c244a5bf80b933e234620c.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-b2bde2d0a9c244a5bf80b933e234620c.vega-embed details,\n",
       "  #altair-viz-b2bde2d0a9c244a5bf80b933e234620c.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-b2bde2d0a9c244a5bf80b933e234620c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-b2bde2d0a9c244a5bf80b933e234620c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-b2bde2d0a9c244a5bf80b933e234620c\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-227908d3b27182eef837c4356bba9307\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-227908d3b27182eef837c4356bba9307\": [{\"training_acc\": 0.7109375, \"training_loss\": 0.6803210973739624, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6826960444450378, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.6703847646713257, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.669221043586731, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6758524179458618, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.660586953163147, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.6482424736022949, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6655183434486389, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.665712296962738, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.656507134437561, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.631695568561554, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6517184376716614, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.6175194978713989, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.6279063820838928, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.6237537264823914, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.631354570388794, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6549241542816162, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6275005340576172, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6240757703781128, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6093565225601196, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6248562335968018, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5758126378059387, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5977789759635925, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.644931435585022, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6032754778862, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6174694895744324, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6063753366470337, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6147897243499756, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.617459237575531, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6274415254592896, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6643644571304321, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6046468019485474, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5887259840965271, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5778266191482544, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6369066834449768, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6545960903167725, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.678104043006897, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.626852810382843, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6424845457077026, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6004585027694702, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6359367966651917, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5904704332351685, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5811240673065186, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5950472354888916, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6692647337913513, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6245142221450806, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5720794796943665, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6193346381187439, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.565793514251709, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6014859080314636, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6069905757904053, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.5831478834152222, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5701751112937927, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5419409275054932, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.5940279364585876, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.5792874097824097, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.581805408000946, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6664373278617859, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6167398691177368, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.5950675010681152, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5520966649055481, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6319442987442017, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5616581439971924, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5661489367485046, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6089973449707031, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.5579832792282104, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5436521172523499, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.6915887850467289, \"training_loss\": 0.5734806656837463, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.5620393753051758, \"iteration\": 69, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 0.538304328918457, \"iteration\": 70, \"epoch\": 2}, {\"training_acc\": 0.6796875, \"training_loss\": 0.519277811050415, \"iteration\": 71, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 0.5289770364761353, \"iteration\": 72, \"epoch\": 2}, {\"training_acc\": 0.6640625, \"training_loss\": 0.5218638181686401, \"iteration\": 73, \"epoch\": 2}, {\"training_acc\": 0.6796875, \"training_loss\": 0.5043621063232422, \"iteration\": 74, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 0.502998411655426, \"iteration\": 75, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 0.4843023419380188, \"iteration\": 76, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5160952210426331, \"iteration\": 77, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 0.5139615535736084, \"iteration\": 78, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4405631124973297, \"iteration\": 79, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.43887442350387573, \"iteration\": 80, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 0.4809682369232178, \"iteration\": 81, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4454861879348755, \"iteration\": 82, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.4487818777561188, \"iteration\": 83, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.44772791862487793, \"iteration\": 84, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.42561423778533936, \"iteration\": 85, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.43232786655426025, \"iteration\": 86, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4052460193634033, \"iteration\": 87, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4136447012424469, \"iteration\": 88, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4371013641357422, \"iteration\": 89, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.43011340498924255, \"iteration\": 90, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.4059846103191376, \"iteration\": 91, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.364908367395401, \"iteration\": 92, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.43991196155548096, \"iteration\": 93, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.36674270033836365, \"iteration\": 94, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.4202890396118164, \"iteration\": 95, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.337342232465744, \"iteration\": 96, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3805743455886841, \"iteration\": 97, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.34684431552886963, \"iteration\": 98, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.41973236203193665, \"iteration\": 99, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3708803951740265, \"iteration\": 100, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3404490351676941, \"iteration\": 101, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.325639545917511, \"iteration\": 102, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.3070700168609619, \"iteration\": 103, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.31297069787979126, \"iteration\": 104, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2877114713191986, \"iteration\": 105, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3420652151107788, \"iteration\": 106, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.39129361510276794, \"iteration\": 107, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2884349822998047, \"iteration\": 108, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.37316134572029114, \"iteration\": 109, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.32848310470581055, \"iteration\": 110, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.31575143337249756, \"iteration\": 111, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2692936956882477, \"iteration\": 112, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.35010260343551636, \"iteration\": 113, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.33684009313583374, \"iteration\": 114, \"epoch\": 2}, {\"training_acc\": 0.953125, \"training_loss\": 0.2662925720214844, \"iteration\": 115, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.23585274815559387, \"iteration\": 116, \"epoch\": 2}, {\"training_acc\": 0.953125, \"training_loss\": 0.21854600310325623, \"iteration\": 117, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23699162900447845, \"iteration\": 118, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2629801034927368, \"iteration\": 119, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.28606969118118286, \"iteration\": 120, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.22812025249004364, \"iteration\": 121, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2587776184082031, \"iteration\": 122, \"epoch\": 2}, {\"training_acc\": 0.96875, \"training_loss\": 0.19070489704608917, \"iteration\": 123, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.26504889130592346, \"iteration\": 124, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.236724853515625, \"iteration\": 125, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26757946610450745, \"iteration\": 126, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22221800684928894, \"iteration\": 127, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2479885369539261, \"iteration\": 128, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3127422034740448, \"iteration\": 129, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25541990995407104, \"iteration\": 130, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1996842473745346, \"iteration\": 131, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.21526911854743958, \"iteration\": 132, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2318314015865326, \"iteration\": 133, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.23367901146411896, \"iteration\": 134, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.19992601871490479, \"iteration\": 135, \"epoch\": 2}, {\"training_acc\": 0.9252336448598131, \"training_loss\": 0.23789681494235992, \"iteration\": 136, \"epoch\": 2}, {\"training_acc\": 0.96875, \"training_loss\": 0.12129897624254227, \"iteration\": 137, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08270452916622162, \"iteration\": 138, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13667148351669312, \"iteration\": 139, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10106640309095383, \"iteration\": 140, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10553772002458572, \"iteration\": 141, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08334463834762573, \"iteration\": 142, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09306903928518295, \"iteration\": 143, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07486022263765335, \"iteration\": 144, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08742876350879669, \"iteration\": 145, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.10047414898872375, \"iteration\": 146, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08799055218696594, \"iteration\": 147, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05845344066619873, \"iteration\": 148, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.0838639959692955, \"iteration\": 149, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.0863237977027893, \"iteration\": 150, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08468319475650787, \"iteration\": 151, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05770324915647507, \"iteration\": 152, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07785497605800629, \"iteration\": 153, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06872774660587311, \"iteration\": 154, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.06081709265708923, \"iteration\": 155, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.03906560689210892, \"iteration\": 156, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.10533810406923294, \"iteration\": 157, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07746170461177826, \"iteration\": 158, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07950825244188309, \"iteration\": 159, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06014568358659744, \"iteration\": 160, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0711362212896347, \"iteration\": 161, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11441632360219955, \"iteration\": 162, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.050610508769750595, \"iteration\": 163, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.09413382411003113, \"iteration\": 164, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.0698612779378891, \"iteration\": 165, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04981141909956932, \"iteration\": 166, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0743095725774765, \"iteration\": 167, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.04475835710763931, \"iteration\": 168, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.046873800456523895, \"iteration\": 169, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.038554299622774124, \"iteration\": 170, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.04326228052377701, \"iteration\": 171, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10397637635469437, \"iteration\": 172, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.06523632258176804, \"iteration\": 173, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09359614551067352, \"iteration\": 174, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.04797980934381485, \"iteration\": 175, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05101855844259262, \"iteration\": 176, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.06743073463439941, \"iteration\": 177, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.039448656141757965, \"iteration\": 178, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04314791411161423, \"iteration\": 179, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0906304121017456, \"iteration\": 180, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.040770355612039566, \"iteration\": 181, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.10555858165025711, \"iteration\": 182, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.09367097914218903, \"iteration\": 183, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.044069401919841766, \"iteration\": 184, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.025235552340745926, \"iteration\": 185, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.062032029032707214, \"iteration\": 186, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07059723138809204, \"iteration\": 187, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.050836771726608276, \"iteration\": 188, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06458215415477753, \"iteration\": 189, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05245083197951317, \"iteration\": 190, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1128021627664566, \"iteration\": 191, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06143123656511307, \"iteration\": 192, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07855141162872314, \"iteration\": 193, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06922835856676102, \"iteration\": 194, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0382375568151474, \"iteration\": 195, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.021761272102594376, \"iteration\": 196, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08797972649335861, \"iteration\": 197, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.036395348608493805, \"iteration\": 198, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09671613574028015, \"iteration\": 199, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.066060870885849, \"iteration\": 200, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.055700019001960754, \"iteration\": 201, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08280687779188156, \"iteration\": 202, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03528986871242523, \"iteration\": 203, \"epoch\": 3}, {\"training_acc\": 0.9626168224299065, \"training_loss\": 0.10828552395105362, \"iteration\": 204, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02560487948358059, \"iteration\": 205, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0164487324655056, \"iteration\": 206, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010681552812457085, \"iteration\": 207, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010368850082159042, \"iteration\": 208, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.019545529037714005, \"iteration\": 209, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.012912943959236145, \"iteration\": 210, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008449654094874859, \"iteration\": 211, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010041342116892338, \"iteration\": 212, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.01075076125562191, \"iteration\": 213, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.01295769028365612, \"iteration\": 214, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009239653125405312, \"iteration\": 215, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006572228856384754, \"iteration\": 216, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006621713284403086, \"iteration\": 217, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008345156908035278, \"iteration\": 218, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009550508111715317, \"iteration\": 219, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006656655576080084, \"iteration\": 220, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0055902330204844475, \"iteration\": 221, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.011058114469051361, \"iteration\": 222, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008352930657565594, \"iteration\": 223, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005652843974530697, \"iteration\": 224, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008815200999379158, \"iteration\": 225, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.012605229392647743, \"iteration\": 226, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008362054824829102, \"iteration\": 227, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007858745753765106, \"iteration\": 228, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.012493492104113102, \"iteration\": 229, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.013978080824017525, \"iteration\": 230, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.030883589759469032, \"iteration\": 231, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0065588029101490974, \"iteration\": 232, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006369035225361586, \"iteration\": 233, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.015397035516798496, \"iteration\": 234, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007327597588300705, \"iteration\": 235, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006825454533100128, \"iteration\": 236, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005208193324506283, \"iteration\": 237, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007155950181186199, \"iteration\": 238, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007576627656817436, \"iteration\": 239, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0070979236625134945, \"iteration\": 240, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007175259757786989, \"iteration\": 241, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008487819693982601, \"iteration\": 242, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021424107253551483, \"iteration\": 243, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.036599788814783096, \"iteration\": 244, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.011036255396902561, \"iteration\": 245, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0065109627321362495, \"iteration\": 246, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0063037872314453125, \"iteration\": 247, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004953661002218723, \"iteration\": 248, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010989168658852577, \"iteration\": 249, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.01094411127269268, \"iteration\": 250, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005748779512941837, \"iteration\": 251, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0056715961545705795, \"iteration\": 252, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0049598487094044685, \"iteration\": 253, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.015557070262730122, \"iteration\": 254, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005046924576163292, \"iteration\": 255, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0040581561625003815, \"iteration\": 256, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005334198009222746, \"iteration\": 257, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005332694388926029, \"iteration\": 258, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008684396743774414, \"iteration\": 259, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008129858411848545, \"iteration\": 260, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0072080232203006744, \"iteration\": 261, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005144190974533558, \"iteration\": 262, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00816403515636921, \"iteration\": 263, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004779463168233633, \"iteration\": 264, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02945045381784439, \"iteration\": 265, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.033313918858766556, \"iteration\": 266, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0045221541076898575, \"iteration\": 267, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009203361347317696, \"iteration\": 268, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00410187104716897, \"iteration\": 269, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009583951905369759, \"iteration\": 270, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006016809958964586, \"iteration\": 271, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0036985394544899464, \"iteration\": 272, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004421703051775694, \"iteration\": 273, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002945191692560911, \"iteration\": 274, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0027004643343389034, \"iteration\": 275, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.003743210108950734, \"iteration\": 276, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0023094245698302984, \"iteration\": 277, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0025417027063667774, \"iteration\": 278, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0024024290032684803, \"iteration\": 279, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0038349246606230736, \"iteration\": 280, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017174241365864873, \"iteration\": 281, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0030039562843739986, \"iteration\": 282, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0024787764996290207, \"iteration\": 283, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0022650016471743584, \"iteration\": 284, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002497226931154728, \"iteration\": 285, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0029648728668689728, \"iteration\": 286, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0026089977473020554, \"iteration\": 287, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0022273838985711336, \"iteration\": 288, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0054527269676327705, \"iteration\": 289, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0015709074214100838, \"iteration\": 290, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0037528362590819597, \"iteration\": 291, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.003905169665813446, \"iteration\": 292, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017701723845675588, \"iteration\": 293, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0015753768384456635, \"iteration\": 294, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002230533864349127, \"iteration\": 295, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002133703324943781, \"iteration\": 296, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001922912197187543, \"iteration\": 297, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002402543555945158, \"iteration\": 298, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017918222583830357, \"iteration\": 299, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0021305265836417675, \"iteration\": 300, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0033217351883649826, \"iteration\": 301, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001612585736438632, \"iteration\": 302, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0016483780927956104, \"iteration\": 303, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002276758896186948, \"iteration\": 304, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019812823738902807, \"iteration\": 305, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002114449627697468, \"iteration\": 306, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0034754525404423475, \"iteration\": 307, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0016136500053107738, \"iteration\": 308, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0018915657419711351, \"iteration\": 309, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001993133220821619, \"iteration\": 310, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001586718368344009, \"iteration\": 311, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001708060735836625, \"iteration\": 312, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019166292622685432, \"iteration\": 313, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0016814642585814, \"iteration\": 314, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001679883454926312, \"iteration\": 315, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0020168081391602755, \"iteration\": 316, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019293244695290923, \"iteration\": 317, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0014019578229635954, \"iteration\": 318, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0014465481508523226, \"iteration\": 319, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0011219300795346498, \"iteration\": 320, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0022030132822692394, \"iteration\": 321, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001466413727030158, \"iteration\": 322, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0024291230365633965, \"iteration\": 323, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00160586042329669, \"iteration\": 324, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0015906791668385267, \"iteration\": 325, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013689168263226748, \"iteration\": 326, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0016174657503142953, \"iteration\": 327, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021264996379613876, \"iteration\": 328, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0015897833509370685, \"iteration\": 329, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013652623165398836, \"iteration\": 330, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013076835311949253, \"iteration\": 331, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013589372392743826, \"iteration\": 332, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001372791826725006, \"iteration\": 333, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0015043846797198057, \"iteration\": 334, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013617908116430044, \"iteration\": 335, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0012156420852988958, \"iteration\": 336, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0016432574484497309, \"iteration\": 337, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001533714821562171, \"iteration\": 338, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001154817407950759, \"iteration\": 339, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0010750711662694812, \"iteration\": 340, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0009556474396958947, \"iteration\": 341, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010030068224295974, \"iteration\": 342, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011885127751156688, \"iteration\": 343, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007586423307657242, \"iteration\": 344, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011305779917165637, \"iteration\": 345, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010465309023857117, \"iteration\": 346, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011356713948771358, \"iteration\": 347, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009389531915076077, \"iteration\": 348, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010134929325431585, \"iteration\": 349, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009894537506625056, \"iteration\": 350, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000987925217486918, \"iteration\": 351, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008698783349245787, \"iteration\": 352, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011350694112479687, \"iteration\": 353, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008555930107831955, \"iteration\": 354, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007755276747047901, \"iteration\": 355, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006024726899340749, \"iteration\": 356, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011649492662400007, \"iteration\": 357, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007089631399139762, \"iteration\": 358, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007420048932544887, \"iteration\": 359, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008510972838848829, \"iteration\": 360, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007361657917499542, \"iteration\": 361, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008239847375079989, \"iteration\": 362, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000912848801817745, \"iteration\": 363, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007223807624541223, \"iteration\": 364, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005521755083464086, \"iteration\": 365, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005939225084148347, \"iteration\": 366, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006956857396289706, \"iteration\": 367, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006048032082617283, \"iteration\": 368, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006988752167671919, \"iteration\": 369, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006565838702954352, \"iteration\": 370, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008417543140240014, \"iteration\": 371, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.001331416773609817, \"iteration\": 372, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008543932344764471, \"iteration\": 373, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005808782298117876, \"iteration\": 374, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009455459075979888, \"iteration\": 375, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008571046055294573, \"iteration\": 376, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006346461013890803, \"iteration\": 377, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.001171710784547031, \"iteration\": 378, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007291823858395219, \"iteration\": 379, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005136982072144747, \"iteration\": 380, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006462145829573274, \"iteration\": 381, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006679173675365746, \"iteration\": 382, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0004254001541994512, \"iteration\": 383, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005301150376908481, \"iteration\": 384, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000515824998728931, \"iteration\": 385, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005092925857752562, \"iteration\": 386, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006257508648559451, \"iteration\": 387, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0004921905929222703, \"iteration\": 388, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006153612048365176, \"iteration\": 389, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007186708971858025, \"iteration\": 390, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0004053209850098938, \"iteration\": 391, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007735105464234948, \"iteration\": 392, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007231088820844889, \"iteration\": 393, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0004693936789408326, \"iteration\": 394, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006083773914724588, \"iteration\": 395, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000995458452962339, \"iteration\": 396, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00043530965922400355, \"iteration\": 397, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005742079811170697, \"iteration\": 398, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005481722764670849, \"iteration\": 399, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00044395533041097224, \"iteration\": 400, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006153729627840221, \"iteration\": 401, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006268297438509762, \"iteration\": 402, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006331017939373851, \"iteration\": 403, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0004455048474483192, \"iteration\": 404, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00043960046605207026, \"iteration\": 405, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005305034574121237, \"iteration\": 406, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008769499254412949, \"iteration\": 407, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005599138676188886, \"iteration\": 408, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005533166695386171, \"iteration\": 409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00044303323375061154, \"iteration\": 410, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00043730964534915984, \"iteration\": 411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003830056230071932, \"iteration\": 412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002061521867290139, \"iteration\": 413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004356566059868783, \"iteration\": 414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00033096561674028635, \"iteration\": 415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00034145585959777236, \"iteration\": 416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006141569465398788, \"iteration\": 417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004079817153979093, \"iteration\": 418, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002933030482381582, \"iteration\": 419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00036912172799929976, \"iteration\": 420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000553978024981916, \"iteration\": 421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004018847248516977, \"iteration\": 422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00042528356425464153, \"iteration\": 423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000361608894309029, \"iteration\": 424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004147175350226462, \"iteration\": 425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00032294398988597095, \"iteration\": 426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000329423084622249, \"iteration\": 427, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00029656634433194995, \"iteration\": 428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003907825448550284, \"iteration\": 429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002775518223643303, \"iteration\": 430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003540205070748925, \"iteration\": 431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003456331614870578, \"iteration\": 432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00041019608033820987, \"iteration\": 433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00038502796087414026, \"iteration\": 434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00035498797660693526, \"iteration\": 435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00025199560332112014, \"iteration\": 436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003584673977456987, \"iteration\": 437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00021340462262742221, \"iteration\": 438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003417064726818353, \"iteration\": 439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005573774687945843, \"iteration\": 440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00029476426425389946, \"iteration\": 441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002590917283669114, \"iteration\": 442, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002861225511878729, \"iteration\": 443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00023051643802318722, \"iteration\": 444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00025332768564112484, \"iteration\": 445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00031364301685243845, \"iteration\": 446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003141924971714616, \"iteration\": 447, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004148153238929808, \"iteration\": 448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003576553426682949, \"iteration\": 449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00034852823591791093, \"iteration\": 450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002465267898514867, \"iteration\": 451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00027093704557046294, \"iteration\": 452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000244117749389261, \"iteration\": 453, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002863963018171489, \"iteration\": 454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00027094330289401114, \"iteration\": 455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003300142125226557, \"iteration\": 456, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00028778635896742344, \"iteration\": 457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00020313118875492364, \"iteration\": 458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00026721670292317867, \"iteration\": 459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000268591073108837, \"iteration\": 460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002581515582278371, \"iteration\": 461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002386773267062381, \"iteration\": 462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000216390224522911, \"iteration\": 463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00030608155066147447, \"iteration\": 464, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00021166150690987706, \"iteration\": 465, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00021207213285379112, \"iteration\": 466, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00029793099383823574, \"iteration\": 467, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003104559436906129, \"iteration\": 468, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002416877105133608, \"iteration\": 469, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00022977503249421716, \"iteration\": 470, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00015835570229683071, \"iteration\": 471, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00030793360201641917, \"iteration\": 472, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00027897581458091736, \"iteration\": 473, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00022862854530103505, \"iteration\": 474, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003695013001561165, \"iteration\": 475, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005613731918856502, \"iteration\": 476, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00023737575975246727, \"iteration\": 477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00035004992969334126, \"iteration\": 478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00022339833958540112, \"iteration\": 479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00027669843984767795, \"iteration\": 480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00018895023094955832, \"iteration\": 481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00023461236560251564, \"iteration\": 482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004532294115051627, \"iteration\": 483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001869199040811509, \"iteration\": 484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00022947764955461025, \"iteration\": 485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00020247903012204915, \"iteration\": 486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00021159871539566666, \"iteration\": 487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00022693623031955212, \"iteration\": 488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00026471258024685085, \"iteration\": 489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000255649967584759, \"iteration\": 490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00014684187772218138, \"iteration\": 491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001567946164868772, \"iteration\": 492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00020684718037955463, \"iteration\": 493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001639718102524057, \"iteration\": 494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00017496017972007394, \"iteration\": 495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00020315279834903777, \"iteration\": 496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00020979743567295372, \"iteration\": 497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00014366036339197308, \"iteration\": 498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00019334297394379973, \"iteration\": 499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001636251254240051, \"iteration\": 500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015141481708269566, \"iteration\": 501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016711563512217253, \"iteration\": 502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00018042849842458963, \"iteration\": 503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00018772204930428416, \"iteration\": 504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016214558854699135, \"iteration\": 505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001540051307529211, \"iteration\": 506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016263479483313859, \"iteration\": 507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001396708976244554, \"iteration\": 508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001892120053526014, \"iteration\": 509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00017411488806828856, \"iteration\": 510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013421266339719296, \"iteration\": 511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00012003205483779311, \"iteration\": 512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016061923815868795, \"iteration\": 513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001922172959893942, \"iteration\": 514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013250236224848777, \"iteration\": 515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00017078597738873214, \"iteration\": 516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00024851952912285924, \"iteration\": 517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 8.043512207223102e-05, \"iteration\": 518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00020497346122283489, \"iteration\": 519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013453549763653427, \"iteration\": 520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011557529796846211, \"iteration\": 521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001717032864689827, \"iteration\": 522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008382693631574512, \"iteration\": 523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001357285655103624, \"iteration\": 524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001655799860600382, \"iteration\": 525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013017469609621912, \"iteration\": 526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00012873714149463922, \"iteration\": 527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001361007016384974, \"iteration\": 528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015910627553239465, \"iteration\": 529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00017818788182921708, \"iteration\": 530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00014208279026206583, \"iteration\": 531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001362581388093531, \"iteration\": 532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00017099635442718863, \"iteration\": 533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011887352593475953, \"iteration\": 534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00022942530631553382, \"iteration\": 535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00014135590754449368, \"iteration\": 536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011669759987853467, \"iteration\": 537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013648258754983544, \"iteration\": 538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016454003343824297, \"iteration\": 539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002085185842588544, \"iteration\": 540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013045279774814844, \"iteration\": 541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 9.819114347919822e-05, \"iteration\": 542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00012797207455150783, \"iteration\": 543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001365465286653489, \"iteration\": 544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001249023189302534, \"iteration\": 545, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.861736907623708e-05, \"iteration\": 546, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00012634893937502056, \"iteration\": 547, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010046252282336354, \"iteration\": 548, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00012743767001666129, \"iteration\": 549, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.093017095234245e-05, \"iteration\": 550, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010667333845049143, \"iteration\": 551, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005105868331156671, \"iteration\": 552, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.019814024213701e-05, \"iteration\": 553, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011423861724324524, \"iteration\": 554, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.670474926475435e-05, \"iteration\": 555, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00012857388355769217, \"iteration\": 556, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011042926780646667, \"iteration\": 557, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.736174251884222e-05, \"iteration\": 558, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011879926751134917, \"iteration\": 559, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001118931540986523, \"iteration\": 560, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.657577564008534e-05, \"iteration\": 561, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010738426499301568, \"iteration\": 562, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00013746168406214565, \"iteration\": 563, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011544657172635198, \"iteration\": 564, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.019099525175989e-05, \"iteration\": 565, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010852994455490261, \"iteration\": 566, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001325399789493531, \"iteration\": 567, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.911094144219533e-05, \"iteration\": 568, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.683653479442e-05, \"iteration\": 569, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00015395722584798932, \"iteration\": 570, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00016637299268040806, \"iteration\": 571, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011751509737223387, \"iteration\": 572, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011792589793913066, \"iteration\": 573, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011299899779260159, \"iteration\": 574, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.513243665220216e-05, \"iteration\": 575, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002009450545301661, \"iteration\": 576, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010676030069589615, \"iteration\": 577, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.633735706098378e-05, \"iteration\": 578, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001274560927413404, \"iteration\": 579, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010039028711616993, \"iteration\": 580, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.933907520258799e-05, \"iteration\": 581, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.748529606033117e-05, \"iteration\": 582, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00014726820518262684, \"iteration\": 583, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.66588345868513e-05, \"iteration\": 584, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.61670055706054e-05, \"iteration\": 585, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.603305468568578e-05, \"iteration\": 586, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00014078422100283206, \"iteration\": 587, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011071842163801193, \"iteration\": 588, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001470260031055659, \"iteration\": 589, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.298684017267078e-05, \"iteration\": 590, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.447587217437103e-05, \"iteration\": 591, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.05442250566557e-05, \"iteration\": 592, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010291620856150985, \"iteration\": 593, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.614971011411399e-05, \"iteration\": 594, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.275588334072381e-05, \"iteration\": 595, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.1348953244742e-05, \"iteration\": 596, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.484450907213613e-05, \"iteration\": 597, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00024602306075394154, \"iteration\": 598, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.66909428825602e-05, \"iteration\": 599, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.492426422890276e-05, \"iteration\": 600, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.537962614558637e-05, \"iteration\": 601, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.862836693879217e-05, \"iteration\": 602, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.470512693747878e-05, \"iteration\": 603, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.876411498524249e-05, \"iteration\": 604, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00013307161862030625, \"iteration\": 605, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.88353499956429e-05, \"iteration\": 606, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.427436841884628e-05, \"iteration\": 607, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.551047358196229e-05, \"iteration\": 608, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00012161405174992979, \"iteration\": 609, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010152174218092114, \"iteration\": 610, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.618597687222064e-05, \"iteration\": 611, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011316197196720168, \"iteration\": 612, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.735500937793404e-05, \"iteration\": 613, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.439209391828626e-05, \"iteration\": 614, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.944035314721987e-05, \"iteration\": 615, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.276439282577485e-05, \"iteration\": 616, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.354043580358848e-05, \"iteration\": 617, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.576316081918776e-05, \"iteration\": 618, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.475732854800299e-05, \"iteration\": 619, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00011631810775725171, \"iteration\": 620, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.521965406136587e-05, \"iteration\": 621, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00011560845450730994, \"iteration\": 622, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.13980955677107e-05, \"iteration\": 623, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.020276825642213e-05, \"iteration\": 624, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.772043707314879e-05, \"iteration\": 625, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 8.041624096222222e-05, \"iteration\": 626, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 8.492221968481317e-05, \"iteration\": 627, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00011294065188849345, \"iteration\": 628, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.474099402315915e-05, \"iteration\": 629, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 9.099829912884161e-05, \"iteration\": 630, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 8.660746243549511e-05, \"iteration\": 631, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.9722126277629286e-05, \"iteration\": 632, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 9.329815657110885e-05, \"iteration\": 633, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.868652806384489e-05, \"iteration\": 634, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.689884660067037e-05, \"iteration\": 635, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 9.139355825027451e-05, \"iteration\": 636, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 9.045851766131818e-05, \"iteration\": 637, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.358539733104408e-05, \"iteration\": 638, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.384947821265087e-05, \"iteration\": 639, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.5671698646619916e-05, \"iteration\": 640, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.567506352439523e-05, \"iteration\": 641, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.548788223881274e-05, \"iteration\": 642, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.628186772810295e-05, \"iteration\": 643, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.062522490741685e-05, \"iteration\": 644, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00010295806714566424, \"iteration\": 645, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.43882415513508e-05, \"iteration\": 646, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.041385495336726e-05, \"iteration\": 647, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.092977517051622e-05, \"iteration\": 648, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00010352411482017487, \"iteration\": 649, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.801926090498455e-05, \"iteration\": 650, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.376377132255584e-05, \"iteration\": 651, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.235027442220598e-05, \"iteration\": 652, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.410952144302428e-05, \"iteration\": 653, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.028327698004432e-05, \"iteration\": 654, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.865432831342332e-05, \"iteration\": 655, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.139420318300836e-05, \"iteration\": 656, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.229077164083719e-05, \"iteration\": 657, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.865668208571151e-05, \"iteration\": 658, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.650653813267127e-05, \"iteration\": 659, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.224410026334226e-05, \"iteration\": 660, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.9980939365923405e-05, \"iteration\": 661, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.731277971994132e-05, \"iteration\": 662, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.930279101245105e-05, \"iteration\": 663, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.561633199453354e-05, \"iteration\": 664, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.07627332303673e-05, \"iteration\": 665, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.4898380767554045e-05, \"iteration\": 666, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.911121843382716e-05, \"iteration\": 667, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 8.2967228081543e-05, \"iteration\": 668, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.5602246245834976e-05, \"iteration\": 669, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 9.34073468670249e-05, \"iteration\": 670, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.134278762852773e-05, \"iteration\": 671, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013931820285506546, \"iteration\": 672, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.201202631928027e-05, \"iteration\": 673, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.785560279036872e-05, \"iteration\": 674, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002911131887231022, \"iteration\": 675, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.403633986134082e-05, \"iteration\": 676, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.849954348173924e-05, \"iteration\": 677, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001643122814130038, \"iteration\": 678, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.296300125541165e-05, \"iteration\": 679, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.580530367093161e-05, \"iteration\": 680, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(precision_recall_fscore_support(y_test.cpu(), y_pred.cpu(), average='binary'))\n",
    "print(\"AUC\", roc_auc_score(y_test.cpu(), y_pred.cpu()))\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn, train_config, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 68/68 [00:01<00:00, 66.30batch/s, batch_accuracy=0.766, loss=0.513]\n",
      "Epoch 2: 100%|██████████| 68/68 [00:01<00:00, 67.60batch/s, batch_accuracy=0.888, loss=0.299]\n",
      "Epoch 3: 100%|██████████| 68/68 [00:01<00:00, 64.59batch/s, batch_accuracy=0.981, loss=0.0727]\n",
      "Epoch 4: 100%|██████████| 68/68 [00:00<00:00, 69.09batch/s, batch_accuracy=1, loss=0.00646]   \n",
      "Epoch 5: 100%|██████████| 68/68 [00:01<00:00, 64.87batch/s, batch_accuracy=1, loss=0.00153]   \n",
      "Epoch 6: 100%|██████████| 68/68 [00:00<00:00, 68.21batch/s, batch_accuracy=1, loss=0.000908]\n",
      "Epoch 7: 100%|██████████| 68/68 [00:01<00:00, 66.47batch/s, batch_accuracy=1, loss=0.000394]\n",
      "Epoch 8: 100%|██████████| 68/68 [00:01<00:00, 67.40batch/s, batch_accuracy=1, loss=0.000119]\n",
      "Epoch 9: 100%|██████████| 68/68 [00:00<00:00, 68.12batch/s, batch_accuracy=1, loss=5.77e-5] \n",
      "Epoch 10: 100%|██████████| 68/68 [00:01<00:00, 64.79batch/s, batch_accuracy=1, loss=3.31e-5] \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m     y_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([test[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m test \u001b[38;5;129;01min\u001b[39;00m test_data_nn])\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     37\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model_nn\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m\"\u001b[39m, roc_auc_score(y_test, y_pred))\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Plot training accuracy and loss side-by-side\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1775\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m \n\u001b[1;32m   1614\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1775\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1547\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1547\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/metrics/_classification.py:101\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    100\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_pred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/multiclass.py:316\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/multiclass.py:171\u001b[0m, in \u001b[0;36mis_multilabel\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    169\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, VisibleDeprecationWarning)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_y_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (VisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/validation.py:1007\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1011\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/_array_api.py:746\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    744\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/torch/_tensor.py:1087\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1087\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# Parameters finding\n",
    "\n",
    "print(\"Train model\")\n",
    "models_dir = Path('models/es')\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    "    optimizer_params= {\"lr\": 0.0001, \"weight_decay\": 0.001, }\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_data_nn, batch_size=128, shuffle=True)\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "model_nn = NeuralNetwork(\n",
    "    input_size=len(tfidf_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn.pt').exists() and USE_CACHE:\n",
    "    model_nn = load_model(model_nn, models_dir, 'model_nn')\n",
    "else:\n",
    "    model_nn.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn, models_dir, \"model_nn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7905866302864939, 0.894980694980695, 0.8395508873596523, None)\n",
      "AUC 0.7369279332242294\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-527a2793e6bb44c4be486a61ceed4292.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-527a2793e6bb44c4be486a61ceed4292.vega-embed details,\n",
       "  #altair-viz-527a2793e6bb44c4be486a61ceed4292.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-527a2793e6bb44c4be486a61ceed4292\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-527a2793e6bb44c4be486a61ceed4292\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-527a2793e6bb44c4be486a61ceed4292\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-1143bb6168c960b7769b46409364be86\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-1143bb6168c960b7769b46409364be86\": [{\"training_acc\": 0.296875, \"training_loss\": 0.6986286640167236, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.6955863833427429, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.6918444633483887, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6896940469741821, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.6832690238952637, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6819185018539429, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6814204454421997, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6747112274169922, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6716499924659729, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6712485551834106, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.6563429832458496, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6652751564979553, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.6395778656005859, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.638873815536499, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.6094156503677368, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6378786563873291, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6441314220428467, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.6182419657707214, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.64261394739151, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6145334243774414, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5912804007530212, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6379726529121399, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6103554964065552, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6348658204078674, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6514264941215515, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6072041988372803, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6261935234069824, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6264703273773193, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.635642409324646, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5385249257087708, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6504406929016113, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6142644882202148, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5838675498962402, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5725641250610352, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6112500429153442, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6600485444068909, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6709449887275696, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5727938413619995, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.5870940685272217, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6304337978363037, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5795499086380005, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5947862863540649, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6307539939880371, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6072944402694702, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5916414260864258, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5524493455886841, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5357431173324585, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.5841332077980042, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.5694915652275085, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5920523405075073, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6353374719619751, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.5984618067741394, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6329892873764038, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.5812817811965942, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6117972731590271, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.5882954597473145, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6245149374008179, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.5864356756210327, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5493335127830505, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6136796474456787, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5657626390457153, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5470066666603088, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6053394675254822, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5669927000999451, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.539569616317749, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5286548733711243, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.5968584418296814, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.7663551401869159, \"training_loss\": 0.5129559636116028, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.5079891085624695, \"iteration\": 69, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.4842928647994995, \"iteration\": 70, \"epoch\": 2}, {\"training_acc\": 0.6171875, \"training_loss\": 0.5389997959136963, \"iteration\": 71, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 0.48678165674209595, \"iteration\": 72, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 0.449948787689209, \"iteration\": 73, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 0.5044339895248413, \"iteration\": 74, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 0.4535222053527832, \"iteration\": 75, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.43306177854537964, \"iteration\": 76, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 0.5341272354125977, \"iteration\": 77, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5035998821258545, \"iteration\": 78, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.48815280199050903, \"iteration\": 79, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 0.49583160877227783, \"iteration\": 80, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.4908895492553711, \"iteration\": 81, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 0.4872516989707947, \"iteration\": 82, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.45406579971313477, \"iteration\": 83, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3900549113750458, \"iteration\": 84, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5534589290618896, \"iteration\": 85, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.45369821786880493, \"iteration\": 86, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4629574716091156, \"iteration\": 87, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.5051403045654297, \"iteration\": 88, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4276997745037079, \"iteration\": 89, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3936094641685486, \"iteration\": 90, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.35403940081596375, \"iteration\": 91, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4247821271419525, \"iteration\": 92, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 0.46113473176956177, \"iteration\": 93, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3924880623817444, \"iteration\": 94, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4315584897994995, \"iteration\": 95, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.39048081636428833, \"iteration\": 96, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3992581367492676, \"iteration\": 97, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3601134419441223, \"iteration\": 98, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.37502503395080566, \"iteration\": 99, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.39382627606391907, \"iteration\": 100, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.37298911809921265, \"iteration\": 101, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.3431675434112549, \"iteration\": 102, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3911827802658081, \"iteration\": 103, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3363203704357147, \"iteration\": 104, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2802603542804718, \"iteration\": 105, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.28307396173477173, \"iteration\": 106, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.347533255815506, \"iteration\": 107, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.35489964485168457, \"iteration\": 108, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2758442759513855, \"iteration\": 109, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3489580750465393, \"iteration\": 110, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.284509539604187, \"iteration\": 111, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.3096195459365845, \"iteration\": 112, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3269461989402771, \"iteration\": 113, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3294130265712738, \"iteration\": 114, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2930738031864166, \"iteration\": 115, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2820918560028076, \"iteration\": 116, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.29128915071487427, \"iteration\": 117, \"epoch\": 2}, {\"training_acc\": 0.9765625, \"training_loss\": 0.2479003518819809, \"iteration\": 118, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2767876982688904, \"iteration\": 119, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2664225697517395, \"iteration\": 120, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.24240919947624207, \"iteration\": 121, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2803340554237366, \"iteration\": 122, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.25977635383605957, \"iteration\": 123, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.21837012469768524, \"iteration\": 124, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22690892219543457, \"iteration\": 125, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3116537928581238, \"iteration\": 126, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3229234218597412, \"iteration\": 127, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.24264438450336456, \"iteration\": 128, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.20922115445137024, \"iteration\": 129, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22054529190063477, \"iteration\": 130, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3317375183105469, \"iteration\": 131, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.24893103539943695, \"iteration\": 132, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3128588795661926, \"iteration\": 133, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2613123059272766, \"iteration\": 134, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1694018840789795, \"iteration\": 135, \"epoch\": 2}, {\"training_acc\": 0.8878504672897196, \"training_loss\": 0.2991824150085449, \"iteration\": 136, \"epoch\": 2}, {\"training_acc\": 0.984375, \"training_loss\": 0.10317613184452057, \"iteration\": 137, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10001687705516815, \"iteration\": 138, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10721368342638016, \"iteration\": 139, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.1305091381072998, \"iteration\": 140, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1207844465970993, \"iteration\": 141, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.06880813091993332, \"iteration\": 142, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07851755619049072, \"iteration\": 143, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09564551711082458, \"iteration\": 144, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.05421280860900879, \"iteration\": 145, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.09061986207962036, \"iteration\": 146, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05815735086798668, \"iteration\": 147, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06739208102226257, \"iteration\": 148, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07358969748020172, \"iteration\": 149, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.11153483390808105, \"iteration\": 150, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08692793548107147, \"iteration\": 151, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08448310196399689, \"iteration\": 152, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.07553467154502869, \"iteration\": 153, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.074989914894104, \"iteration\": 154, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.06433553248643875, \"iteration\": 155, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05791207402944565, \"iteration\": 156, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.08610500395298004, \"iteration\": 157, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07772761583328247, \"iteration\": 158, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.08746891468763351, \"iteration\": 159, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06866340339183807, \"iteration\": 160, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.058929286897182465, \"iteration\": 161, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.047976814210414886, \"iteration\": 162, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.059992026537656784, \"iteration\": 163, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.07111155986785889, \"iteration\": 164, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0864771381020546, \"iteration\": 165, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06414526700973511, \"iteration\": 166, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05859358236193657, \"iteration\": 167, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1096034049987793, \"iteration\": 168, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.06545912474393845, \"iteration\": 169, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05790441855788231, \"iteration\": 170, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.09851932525634766, \"iteration\": 171, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08373291045427322, \"iteration\": 172, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.08249441534280777, \"iteration\": 173, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07196033000946045, \"iteration\": 174, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05786726996302605, \"iteration\": 175, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.060499705374240875, \"iteration\": 176, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.12467936426401138, \"iteration\": 177, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.08453969657421112, \"iteration\": 178, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.045262712985277176, \"iteration\": 179, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.02175009250640869, \"iteration\": 180, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.12049929052591324, \"iteration\": 181, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.082389697432518, \"iteration\": 182, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06953351199626923, \"iteration\": 183, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.10786528140306473, \"iteration\": 184, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06428027153015137, \"iteration\": 185, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0632556676864624, \"iteration\": 186, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.0660490021109581, \"iteration\": 187, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.09286882728338242, \"iteration\": 188, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07308496534824371, \"iteration\": 189, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.057277992367744446, \"iteration\": 190, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05061454325914383, \"iteration\": 191, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04727371037006378, \"iteration\": 192, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.07763087749481201, \"iteration\": 193, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0419468879699707, \"iteration\": 194, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08834019303321838, \"iteration\": 195, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.05346653610467911, \"iteration\": 196, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07302296161651611, \"iteration\": 197, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.08076288551092148, \"iteration\": 198, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.05758291110396385, \"iteration\": 199, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.0338776633143425, \"iteration\": 200, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03575470298528671, \"iteration\": 201, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0351114422082901, \"iteration\": 202, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06709858030080795, \"iteration\": 203, \"epoch\": 3}, {\"training_acc\": 0.9813084112149533, \"training_loss\": 0.07272452861070633, \"iteration\": 204, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.015142693184316158, \"iteration\": 205, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.022550048306584358, \"iteration\": 206, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.013413352891802788, \"iteration\": 207, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010270390659570694, \"iteration\": 208, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009047465398907661, \"iteration\": 209, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009111758321523666, \"iteration\": 210, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008652877062559128, \"iteration\": 211, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009819556027650833, \"iteration\": 212, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009908394888043404, \"iteration\": 213, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008701710030436516, \"iteration\": 214, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005970625206828117, \"iteration\": 215, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010729851201176643, \"iteration\": 216, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009637586772441864, \"iteration\": 217, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.011517943814396858, \"iteration\": 218, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.012750387191772461, \"iteration\": 219, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03364724665880203, \"iteration\": 220, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009599347598850727, \"iteration\": 221, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00930387619882822, \"iteration\": 222, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007652217987924814, \"iteration\": 223, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00818509329110384, \"iteration\": 224, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.01071527972817421, \"iteration\": 225, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007643541321158409, \"iteration\": 226, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006209120154380798, \"iteration\": 227, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006694134324789047, \"iteration\": 228, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008729858323931694, \"iteration\": 229, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008621765300631523, \"iteration\": 230, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007915588095784187, \"iteration\": 231, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008087524212896824, \"iteration\": 232, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007558990269899368, \"iteration\": 233, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0048198155127465725, \"iteration\": 234, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006940990686416626, \"iteration\": 235, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008964982815086842, \"iteration\": 236, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.020496321842074394, \"iteration\": 237, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00625386368483305, \"iteration\": 238, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006775100715458393, \"iteration\": 239, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008430473506450653, \"iteration\": 240, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007855158299207687, \"iteration\": 241, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004635332152247429, \"iteration\": 242, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005213902331888676, \"iteration\": 243, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028896339237689972, \"iteration\": 244, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004342624451965094, \"iteration\": 245, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0048522986471652985, \"iteration\": 246, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0133387241512537, \"iteration\": 247, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005592747591435909, \"iteration\": 248, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005348917096853256, \"iteration\": 249, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.023321008309721947, \"iteration\": 250, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03255327790975571, \"iteration\": 251, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004771750420331955, \"iteration\": 252, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0053751165978610516, \"iteration\": 253, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.023152170702815056, \"iteration\": 254, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03533797711133957, \"iteration\": 255, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03409944847226143, \"iteration\": 256, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009108314290642738, \"iteration\": 257, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006780488416552544, \"iteration\": 258, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004965685307979584, \"iteration\": 259, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.038071971386671066, \"iteration\": 260, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004283265210688114, \"iteration\": 261, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009278618730604649, \"iteration\": 262, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00494498573243618, \"iteration\": 263, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01632273755967617, \"iteration\": 264, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006326553877443075, \"iteration\": 265, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005784193053841591, \"iteration\": 266, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006069160997867584, \"iteration\": 267, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006179279647767544, \"iteration\": 268, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005295764654874802, \"iteration\": 269, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006431487388908863, \"iteration\": 270, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004929749760776758, \"iteration\": 271, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006461662240326405, \"iteration\": 272, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0039562261663377285, \"iteration\": 273, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.003528895787894726, \"iteration\": 274, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0030069032218307257, \"iteration\": 275, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0031778812408447266, \"iteration\": 276, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0038313427940011024, \"iteration\": 277, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002979402896016836, \"iteration\": 278, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0026165787130594254, \"iteration\": 279, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002312588272616267, \"iteration\": 280, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0021437271498143673, \"iteration\": 281, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00235110055655241, \"iteration\": 282, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0029609315097332, \"iteration\": 283, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002801281400024891, \"iteration\": 284, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0026925127021968365, \"iteration\": 285, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002455832902342081, \"iteration\": 286, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0024868263863027096, \"iteration\": 287, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002829145174473524, \"iteration\": 288, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0024169539101421833, \"iteration\": 289, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002344326116144657, \"iteration\": 290, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0024017789401113987, \"iteration\": 291, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.03353241831064224, \"iteration\": 292, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002530988771468401, \"iteration\": 293, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0029456266202032566, \"iteration\": 294, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002395689021795988, \"iteration\": 295, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0027090939693152905, \"iteration\": 296, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002228583674877882, \"iteration\": 297, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01788601279258728, \"iteration\": 298, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0023308400996029377, \"iteration\": 299, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0016845092177391052, \"iteration\": 300, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002302377950400114, \"iteration\": 301, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002790616825222969, \"iteration\": 302, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0026318994350731373, \"iteration\": 303, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002404953818768263, \"iteration\": 304, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017244420014321804, \"iteration\": 305, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.003142391098663211, \"iteration\": 306, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002800250891596079, \"iteration\": 307, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019212590996176004, \"iteration\": 308, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002065474633127451, \"iteration\": 309, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019828148651868105, \"iteration\": 310, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0018685481045395136, \"iteration\": 311, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0015904231695458293, \"iteration\": 312, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0025261002592742443, \"iteration\": 313, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0022800799924880266, \"iteration\": 314, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00172330136410892, \"iteration\": 315, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001693691243417561, \"iteration\": 316, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02141835354268551, \"iteration\": 317, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017589785857126117, \"iteration\": 318, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.007438258733600378, \"iteration\": 319, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017389249987900257, \"iteration\": 320, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017654445255175233, \"iteration\": 321, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001646108808927238, \"iteration\": 322, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017911514732986689, \"iteration\": 323, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001440912252292037, \"iteration\": 324, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0020173697266727686, \"iteration\": 325, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017913004849106073, \"iteration\": 326, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001613939180970192, \"iteration\": 327, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0023800574708729982, \"iteration\": 328, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.014878617599606514, \"iteration\": 329, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019715451635420322, \"iteration\": 330, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019448522944003344, \"iteration\": 331, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001766512868925929, \"iteration\": 332, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0016316724941134453, \"iteration\": 333, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019978214986622334, \"iteration\": 334, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0011805933900177479, \"iteration\": 335, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001719620544463396, \"iteration\": 336, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017762852367013693, \"iteration\": 337, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013816438149660826, \"iteration\": 338, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00201603164896369, \"iteration\": 339, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0015329974703490734, \"iteration\": 340, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00135410123039037, \"iteration\": 341, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0014712922275066376, \"iteration\": 342, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0013826382346451283, \"iteration\": 343, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0013356504496186972, \"iteration\": 344, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0012721713865175843, \"iteration\": 345, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.001210222253575921, \"iteration\": 346, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011226336937397718, \"iteration\": 347, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011557700345292687, \"iteration\": 348, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010213537607342005, \"iteration\": 349, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0015048440545797348, \"iteration\": 350, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0014379997737705708, \"iteration\": 351, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0012668571434915066, \"iteration\": 352, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009223348461091518, \"iteration\": 353, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010246152523905039, \"iteration\": 354, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009346306324005127, \"iteration\": 355, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009047452476806939, \"iteration\": 356, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006352562922984362, \"iteration\": 357, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0012375691439956427, \"iteration\": 358, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.001102095004171133, \"iteration\": 359, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009625354432500899, \"iteration\": 360, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011541894637048244, \"iteration\": 361, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0013455607695505023, \"iteration\": 362, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009141485788859427, \"iteration\": 363, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000955779105424881, \"iteration\": 364, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005361807532608509, \"iteration\": 365, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009319260716438293, \"iteration\": 366, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009934804402291775, \"iteration\": 367, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.002370396163314581, \"iteration\": 368, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0012020342983305454, \"iteration\": 369, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011897331569343805, \"iteration\": 370, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008789370767772198, \"iteration\": 371, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011361410142853856, \"iteration\": 372, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011063211131840944, \"iteration\": 373, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0012768697924911976, \"iteration\": 374, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011619448196142912, \"iteration\": 375, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008472722256556153, \"iteration\": 376, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008001656387932599, \"iteration\": 377, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010817942675203085, \"iteration\": 378, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011049378663301468, \"iteration\": 379, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007746006594970822, \"iteration\": 380, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010285057360306382, \"iteration\": 381, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009121708571910858, \"iteration\": 382, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010364297777414322, \"iteration\": 383, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008694206480868161, \"iteration\": 384, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009768574964255095, \"iteration\": 385, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011024351697415113, \"iteration\": 386, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007335299160331488, \"iteration\": 387, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007765907794237137, \"iteration\": 388, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.001340362592600286, \"iteration\": 389, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009391012135893106, \"iteration\": 390, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006675053737126291, \"iteration\": 391, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000907570996787399, \"iteration\": 392, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008378371130675077, \"iteration\": 393, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007792818360030651, \"iteration\": 394, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008226645877584815, \"iteration\": 395, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009531811811029911, \"iteration\": 396, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007308081840164959, \"iteration\": 397, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009875395335257053, \"iteration\": 398, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006720455130562186, \"iteration\": 399, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008963937289081514, \"iteration\": 400, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010494920425117016, \"iteration\": 401, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000910636386834085, \"iteration\": 402, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00085174769628793, \"iteration\": 403, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007784337503835559, \"iteration\": 404, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003453741315752268, \"iteration\": 405, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008478393428958952, \"iteration\": 406, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004330966155976057, \"iteration\": 407, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009079238516278565, \"iteration\": 408, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005285241641104221, \"iteration\": 409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006039237487129867, \"iteration\": 410, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000678007083479315, \"iteration\": 411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005787656409665942, \"iteration\": 412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0021036553662270308, \"iteration\": 413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005313379224389791, \"iteration\": 414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000552683079149574, \"iteration\": 415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001548652769997716, \"iteration\": 416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006326604052446783, \"iteration\": 417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005841758102178574, \"iteration\": 418, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000740017625503242, \"iteration\": 419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005087904282845557, \"iteration\": 420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000472040381282568, \"iteration\": 421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00046219039359129965, \"iteration\": 422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005786636611446738, \"iteration\": 423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005186483031138778, \"iteration\": 424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005419491790235043, \"iteration\": 425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007495006429962814, \"iteration\": 426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014327194076031446, \"iteration\": 427, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00038352844421751797, \"iteration\": 428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005283274222165346, \"iteration\": 429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006462535820901394, \"iteration\": 430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005412583705037832, \"iteration\": 431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006144446670077741, \"iteration\": 432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005999936838634312, \"iteration\": 433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003985235234722495, \"iteration\": 434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005333362496457994, \"iteration\": 435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00042054918594658375, \"iteration\": 436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003918442816939205, \"iteration\": 437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005860151723027229, \"iteration\": 438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005183548200875521, \"iteration\": 439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005671901744790375, \"iteration\": 440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004929642309434712, \"iteration\": 441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005336734466254711, \"iteration\": 442, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0019772136583924294, \"iteration\": 443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00041698370478115976, \"iteration\": 444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00041362515185028315, \"iteration\": 445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004329920921009034, \"iteration\": 446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00048111614887602627, \"iteration\": 447, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00043730097240768373, \"iteration\": 448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010509609710425138, \"iteration\": 449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005037016817368567, \"iteration\": 450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005620199372060597, \"iteration\": 451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005027881124988198, \"iteration\": 452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003991984995082021, \"iteration\": 453, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00047678640112280846, \"iteration\": 454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010101227089762688, \"iteration\": 455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00038395868614315987, \"iteration\": 456, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004778674046974629, \"iteration\": 457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004011638811789453, \"iteration\": 458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004569675074890256, \"iteration\": 459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00035835328162647784, \"iteration\": 460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00035500485682860017, \"iteration\": 461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00037711014738306403, \"iteration\": 462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005743484362028539, \"iteration\": 463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003859158605337143, \"iteration\": 464, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004058997437823564, \"iteration\": 465, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003773996140807867, \"iteration\": 466, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003884650650434196, \"iteration\": 467, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003186674148309976, \"iteration\": 468, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003540234756655991, \"iteration\": 469, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00034703806159086525, \"iteration\": 470, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003044534532818943, \"iteration\": 471, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005272420821711421, \"iteration\": 472, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003954421845264733, \"iteration\": 473, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00043752777855843306, \"iteration\": 474, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00027796978247351944, \"iteration\": 475, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00039375474443659186, \"iteration\": 476, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003047312202397734, \"iteration\": 477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003562564088497311, \"iteration\": 478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002831623423844576, \"iteration\": 479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00023965499713085592, \"iteration\": 480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00026210976648144424, \"iteration\": 481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002930017071776092, \"iteration\": 482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002859549131244421, \"iteration\": 483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002233348786830902, \"iteration\": 484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002501817070879042, \"iteration\": 485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00022083894873503596, \"iteration\": 486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002567424380686134, \"iteration\": 487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00027567590586841106, \"iteration\": 488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00021612385171465576, \"iteration\": 489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00029323098715394735, \"iteration\": 490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00023287514341063797, \"iteration\": 491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00031611043959856033, \"iteration\": 492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00019482229254208505, \"iteration\": 493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002515424566809088, \"iteration\": 494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001540438533993438, \"iteration\": 495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00022824265761300921, \"iteration\": 496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00020994397345930338, \"iteration\": 497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00020469349692575634, \"iteration\": 498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016313031665049493, \"iteration\": 499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005279977340251207, \"iteration\": 500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00020558219694066793, \"iteration\": 501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00021906766050960869, \"iteration\": 502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00021704439132008702, \"iteration\": 503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00017573474906384945, \"iteration\": 504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00024603973724879324, \"iteration\": 505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00023762689670547843, \"iteration\": 506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008387499838136137, \"iteration\": 507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001763290783856064, \"iteration\": 508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016152815078385174, \"iteration\": 509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002116412651957944, \"iteration\": 510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001661514979787171, \"iteration\": 511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003531081893015653, \"iteration\": 512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001721618027659133, \"iteration\": 513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00012801861157640815, \"iteration\": 514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016841675096657127, \"iteration\": 515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00017481890972703695, \"iteration\": 516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000201379822101444, \"iteration\": 517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00014049244055058807, \"iteration\": 518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00018153811106458306, \"iteration\": 519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001642508723307401, \"iteration\": 520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015457668632734567, \"iteration\": 521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013634080823976547, \"iteration\": 522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00018555554561316967, \"iteration\": 523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00018878761329688132, \"iteration\": 524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008754520677030087, \"iteration\": 525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015932691167108715, \"iteration\": 526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015624947263859212, \"iteration\": 527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016280669660773128, \"iteration\": 528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001315587287535891, \"iteration\": 529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015719086513854563, \"iteration\": 530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013608818699140102, \"iteration\": 531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001299138821195811, \"iteration\": 532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001515583717264235, \"iteration\": 533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00017270821263082325, \"iteration\": 534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001220500998897478, \"iteration\": 535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015926106425467879, \"iteration\": 536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003484275075607002, \"iteration\": 537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001467958209104836, \"iteration\": 538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013345656043384224, \"iteration\": 539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013205796130932868, \"iteration\": 540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 9.846623288467526e-05, \"iteration\": 541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004532609018497169, \"iteration\": 542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015315573546104133, \"iteration\": 543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011861418170155957, \"iteration\": 544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 8.341972716152668e-05, \"iteration\": 545, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004213008505757898, \"iteration\": 546, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010967940761474892, \"iteration\": 547, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.9346594652161e-05, \"iteration\": 548, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.747333388077095e-05, \"iteration\": 549, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00013379153097048402, \"iteration\": 550, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.215858881361783e-05, \"iteration\": 551, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011262675980105996, \"iteration\": 552, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001066696277121082, \"iteration\": 553, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001434231671737507, \"iteration\": 554, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000307342445012182, \"iteration\": 555, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.88896975084208e-05, \"iteration\": 556, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001010771666187793, \"iteration\": 557, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010229693725705147, \"iteration\": 558, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.289501081686467e-05, \"iteration\": 559, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003292241890449077, \"iteration\": 560, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010296406253473833, \"iteration\": 561, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010179881064686924, \"iteration\": 562, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00013594036863651127, \"iteration\": 563, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.139656711136922e-05, \"iteration\": 564, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.04584235791117e-05, \"iteration\": 565, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.01513742771931e-05, \"iteration\": 566, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.320614870171994e-05, \"iteration\": 567, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00012844600132666528, \"iteration\": 568, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.145877265837044e-05, \"iteration\": 569, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00012324578710831702, \"iteration\": 570, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.58050921629183e-05, \"iteration\": 571, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.972391515271738e-05, \"iteration\": 572, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.530234142905101e-05, \"iteration\": 573, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.612243305426091e-05, \"iteration\": 574, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00015526342031080276, \"iteration\": 575, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.37281477288343e-05, \"iteration\": 576, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000238813940086402, \"iteration\": 577, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.26458674762398e-05, \"iteration\": 578, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.358937338925898e-05, \"iteration\": 579, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.071230043540709e-05, \"iteration\": 580, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.523951014969498e-05, \"iteration\": 581, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.816503784852102e-05, \"iteration\": 582, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.988312816247344e-05, \"iteration\": 583, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.778246072120965e-05, \"iteration\": 584, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.687483594054356e-05, \"iteration\": 585, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.128603803925216e-05, \"iteration\": 586, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.039895040681586e-05, \"iteration\": 587, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.182350913761184e-05, \"iteration\": 588, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.73802021285519e-05, \"iteration\": 589, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.723016071598977e-05, \"iteration\": 590, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.658989539369941e-05, \"iteration\": 591, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.8381399867357686e-05, \"iteration\": 592, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 4.5730041165370494e-05, \"iteration\": 593, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 4.921275103697553e-05, \"iteration\": 594, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.1132362538483e-05, \"iteration\": 595, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.205442620441318e-05, \"iteration\": 596, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.563050505472347e-05, \"iteration\": 597, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.6759519793558866e-05, \"iteration\": 598, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.371125295525417e-05, \"iteration\": 599, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.061453991103917e-05, \"iteration\": 600, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.085614535957575e-05, \"iteration\": 601, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.079124093754217e-05, \"iteration\": 602, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.01432730036322e-05, \"iteration\": 603, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.624568727100268e-05, \"iteration\": 604, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.8728503063321114e-05, \"iteration\": 605, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005144464084878564, \"iteration\": 606, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.098613034235314e-05, \"iteration\": 607, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.588279211428016e-05, \"iteration\": 608, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.777727245818824e-05, \"iteration\": 609, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.46359582990408e-05, \"iteration\": 610, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.489754741778597e-05, \"iteration\": 611, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.765056994277984e-05, \"iteration\": 612, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.075082299299538e-05, \"iteration\": 613, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00012403307482600212, \"iteration\": 614, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.039155075792223e-05, \"iteration\": 615, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.665008236770518e-05, \"iteration\": 616, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.841067857341841e-05, \"iteration\": 617, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.508236401714385e-05, \"iteration\": 618, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.850464367540553e-05, \"iteration\": 619, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.894765515928157e-05, \"iteration\": 620, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.383032112149522e-05, \"iteration\": 621, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.572226993739605e-05, \"iteration\": 622, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.17018376942724e-05, \"iteration\": 623, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.384934386005625e-05, \"iteration\": 624, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.512893742183223e-05, \"iteration\": 625, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.514202009886503e-05, \"iteration\": 626, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.344890425680205e-05, \"iteration\": 627, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.761266947956756e-05, \"iteration\": 628, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.11397811351344e-05, \"iteration\": 629, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.595156108029187e-05, \"iteration\": 630, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.8602119932183996e-05, \"iteration\": 631, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.469634222914465e-05, \"iteration\": 632, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.848768003284931e-05, \"iteration\": 633, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.343540786067024e-05, \"iteration\": 634, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.39531504525803e-05, \"iteration\": 635, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.5686633054865524e-05, \"iteration\": 636, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.973026105086319e-05, \"iteration\": 637, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.754159428761341e-05, \"iteration\": 638, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.4897428122349083e-05, \"iteration\": 639, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.367519042920321e-05, \"iteration\": 640, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.0253748263930902e-05, \"iteration\": 641, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020478139049373567, \"iteration\": 642, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.320304237306118e-05, \"iteration\": 643, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014941071276552975, \"iteration\": 644, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.047713734209538e-05, \"iteration\": 645, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.7388108214363456e-05, \"iteration\": 646, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.6318473576102406e-05, \"iteration\": 647, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.372972034616396e-05, \"iteration\": 648, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 2.983597914862912e-05, \"iteration\": 649, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.0870902012102306e-05, \"iteration\": 650, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.711251676781103e-05, \"iteration\": 651, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.51270243502222e-05, \"iteration\": 652, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.642533556558192e-05, \"iteration\": 653, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.8063546273624524e-05, \"iteration\": 654, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003770048206206411, \"iteration\": 655, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.525321153574623e-05, \"iteration\": 656, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.479133010841906e-05, \"iteration\": 657, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.222145798848942e-05, \"iteration\": 658, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.062962347641587e-05, \"iteration\": 659, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.045121022500098e-05, \"iteration\": 660, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.257255179458298e-05, \"iteration\": 661, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.48414430138655e-05, \"iteration\": 662, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.786972542409785e-05, \"iteration\": 663, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.185407695127651e-05, \"iteration\": 664, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.863992606056854e-05, \"iteration\": 665, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001944204414030537, \"iteration\": 666, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.866002953145653e-05, \"iteration\": 667, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.2049650801345706e-05, \"iteration\": 668, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.4607655834406614e-05, \"iteration\": 669, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 2.646089342306368e-05, \"iteration\": 670, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.8110414607217535e-05, \"iteration\": 671, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.190292409271933e-05, \"iteration\": 672, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 2.7346028218744323e-05, \"iteration\": 673, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.301766264485195e-05, \"iteration\": 674, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.8738027797080576e-05, \"iteration\": 675, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.678298137150705e-05, \"iteration\": 676, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014511775225400925, \"iteration\": 677, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.6233202738221735e-05, \"iteration\": 678, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.257767720147967e-05, \"iteration\": 679, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.30873517668806e-05, \"iteration\": 680, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    # X_test = torch.stack([dta[0] for dta in test])\n",
    "    X_test = torch.stack([test[0] for test in test_data_nn]).cpu()\n",
    "    y_test = torch.stack([test[1] for test in test_data_nn]).cpu()\n",
    "    y_pred = model_nn.predict(X_test).cpu()\n",
    "\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, y_pred, average='binary'))\n",
    "print(\"AUC\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn, train_config, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 68/68 [00:01<00:00, 65.46batch/s, batch_accuracy=0.71, loss=0.557] \n",
      "Epoch 2: 100%|██████████| 68/68 [00:00<00:00, 68.76batch/s, batch_accuracy=0.925, loss=0.236]\n",
      "Epoch 3: 100%|██████████| 68/68 [00:01<00:00, 65.81batch/s, batch_accuracy=0.981, loss=0.0583]\n",
      "Epoch 4: 100%|██████████| 68/68 [00:01<00:00, 64.70batch/s, batch_accuracy=1, loss=0.00669]   \n",
      "Epoch 5: 100%|██████████| 68/68 [00:00<00:00, 69.72batch/s, batch_accuracy=1, loss=0.00225]   \n",
      "Epoch 6: 100%|██████████| 68/68 [00:01<00:00, 66.71batch/s, batch_accuracy=1, loss=0.000935]\n",
      "Epoch 7: 100%|██████████| 68/68 [00:01<00:00, 65.51batch/s, batch_accuracy=1, loss=0.00058] \n",
      "Epoch 8: 100%|██████████| 68/68 [00:01<00:00, 67.36batch/s, batch_accuracy=1, loss=0.000313]\n",
      "Epoch 9: 100%|██████████| 68/68 [00:00<00:00, 69.05batch/s, batch_accuracy=1, loss=0.000333]\n",
      "Epoch 10: 100%|██████████| 68/68 [00:01<00:00, 65.12batch/s, batch_accuracy=1, loss=0.000172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7850340136054422, 0.8911196911196911, 0.8347197106690778, None)\n",
      "AUC 0.7288245917875547\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-ceaf133641ef4cde89a1980b319cf10b.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-ceaf133641ef4cde89a1980b319cf10b.vega-embed details,\n",
       "  #altair-viz-ceaf133641ef4cde89a1980b319cf10b.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-ceaf133641ef4cde89a1980b319cf10b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ceaf133641ef4cde89a1980b319cf10b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ceaf133641ef4cde89a1980b319cf10b\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-9d2aa62391603ae00793e0086c890fca\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-9d2aa62391603ae00793e0086c890fca\": [{\"training_acc\": 0.3671875, \"training_loss\": 0.707765519618988, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.421875, \"training_loss\": 0.7007054090499878, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.3046875, \"training_loss\": 0.7067726850509644, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.3046875, \"training_loss\": 0.7029781937599182, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6981666684150696, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.6926208734512329, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.6839613914489746, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6803147792816162, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.6674129962921143, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6725323796272278, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6586006879806519, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6627907752990723, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.6463340520858765, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6426091194152832, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.622307538986206, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.6225244998931885, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6246500015258789, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6191498041152954, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.6046748161315918, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5841372013092041, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6148691177368164, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.5995777249336243, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5509166717529297, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5518054962158203, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6297862529754639, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6292279958724976, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6311209201812744, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.673254132270813, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5514872074127197, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.6073454022407532, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5695065259933472, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.605176568031311, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5256893634796143, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6284579038619995, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6238141655921936, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6160122752189636, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5787124037742615, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6478602886199951, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6303199529647827, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.607231855392456, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5749804973602295, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5947442650794983, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.587134599685669, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6665222644805908, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5707423686981201, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6019124388694763, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6102710962295532, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6253674030303955, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6007370948791504, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6041160821914673, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5513614416122437, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.602717399597168, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5936654806137085, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6175050735473633, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6158348321914673, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5354348421096802, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5579391717910767, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6145448684692383, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6579384803771973, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6330274343490601, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5816612839698792, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6390902996063232, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.558632493019104, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6121245622634888, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5742056369781494, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.5610265731811523, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6041435599327087, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.7102803738317757, \"training_loss\": 0.5572130084037781, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5170924663543701, \"iteration\": 69, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5079098343849182, \"iteration\": 70, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.49657097458839417, \"iteration\": 71, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 0.5434731245040894, \"iteration\": 72, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5155358910560608, \"iteration\": 73, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 0.5280935764312744, \"iteration\": 74, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5097376704216003, \"iteration\": 75, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.49266332387924194, \"iteration\": 76, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 0.514336884021759, \"iteration\": 77, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 0.5562918186187744, \"iteration\": 78, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.4832555055618286, \"iteration\": 79, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5159193873405457, \"iteration\": 80, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.4915670156478882, \"iteration\": 81, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4408951997756958, \"iteration\": 82, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4669075310230255, \"iteration\": 83, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4336779713630676, \"iteration\": 84, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.457923948764801, \"iteration\": 85, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4237115979194641, \"iteration\": 86, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4504109025001526, \"iteration\": 87, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4303037226200104, \"iteration\": 88, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.4271898567676544, \"iteration\": 89, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4046010375022888, \"iteration\": 90, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.45255836844444275, \"iteration\": 91, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4049305319786072, \"iteration\": 92, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4308776259422302, \"iteration\": 93, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3678237199783325, \"iteration\": 94, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3762577176094055, \"iteration\": 95, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.36894696950912476, \"iteration\": 96, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4231288433074951, \"iteration\": 97, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.31024521589279175, \"iteration\": 98, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.34070688486099243, \"iteration\": 99, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.35760968923568726, \"iteration\": 100, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3425459861755371, \"iteration\": 101, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3776289224624634, \"iteration\": 102, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.38115593791007996, \"iteration\": 103, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.33312293887138367, \"iteration\": 104, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3528215289115906, \"iteration\": 105, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.29882878065109253, \"iteration\": 106, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.30933839082717896, \"iteration\": 107, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2868507504463196, \"iteration\": 108, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.326484739780426, \"iteration\": 109, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3003993630409241, \"iteration\": 110, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.25990843772888184, \"iteration\": 111, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.21973717212677002, \"iteration\": 112, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4300886392593384, \"iteration\": 113, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2637263536453247, \"iteration\": 114, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2763711214065552, \"iteration\": 115, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.28681981563568115, \"iteration\": 116, \"epoch\": 2}, {\"training_acc\": 0.9609375, \"training_loss\": 0.21853527426719666, \"iteration\": 117, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2914801836013794, \"iteration\": 118, \"epoch\": 2}, {\"training_acc\": 0.96875, \"training_loss\": 0.24322164058685303, \"iteration\": 119, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2894172966480255, \"iteration\": 120, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2583303153514862, \"iteration\": 121, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.26192307472229004, \"iteration\": 122, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.24446508288383484, \"iteration\": 123, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.18710564076900482, \"iteration\": 124, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.30032986402511597, \"iteration\": 125, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.24086341261863708, \"iteration\": 126, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.28301024436950684, \"iteration\": 127, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.24079616367816925, \"iteration\": 128, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2761119604110718, \"iteration\": 129, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.272918701171875, \"iteration\": 130, \"epoch\": 2}, {\"training_acc\": 0.953125, \"training_loss\": 0.20399950444698334, \"iteration\": 131, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.21845193207263947, \"iteration\": 132, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.23921746015548706, \"iteration\": 133, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2633892893791199, \"iteration\": 134, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23656222224235535, \"iteration\": 135, \"epoch\": 2}, {\"training_acc\": 0.9252336448598131, \"training_loss\": 0.2361331731081009, \"iteration\": 136, \"epoch\": 2}, {\"training_acc\": 0.96875, \"training_loss\": 0.21578583121299744, \"iteration\": 137, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.16646471619606018, \"iteration\": 138, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.07863187789916992, \"iteration\": 139, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11338470131158829, \"iteration\": 140, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07924018800258636, \"iteration\": 141, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.10612853616476059, \"iteration\": 142, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.10258449614048004, \"iteration\": 143, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.05704925209283829, \"iteration\": 144, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15192198753356934, \"iteration\": 145, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.1163846105337143, \"iteration\": 146, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.048979252576828, \"iteration\": 147, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.06153403967618942, \"iteration\": 148, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.05457974970340729, \"iteration\": 149, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07757459580898285, \"iteration\": 150, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.08592592179775238, \"iteration\": 151, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07389071583747864, \"iteration\": 152, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06094874441623688, \"iteration\": 153, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.06593506783246994, \"iteration\": 154, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.05427441745996475, \"iteration\": 155, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08119553327560425, \"iteration\": 156, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.12277662009000778, \"iteration\": 157, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.038152847439050674, \"iteration\": 158, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.051019348204135895, \"iteration\": 159, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06405514478683472, \"iteration\": 160, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08678685873746872, \"iteration\": 161, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.09753464162349701, \"iteration\": 162, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.036611415445804596, \"iteration\": 163, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0916600152850151, \"iteration\": 164, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10984696447849274, \"iteration\": 165, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08157017827033997, \"iteration\": 166, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07477263361215591, \"iteration\": 167, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06107810512185097, \"iteration\": 168, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.06039058417081833, \"iteration\": 169, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0601351298391819, \"iteration\": 170, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.06110730767250061, \"iteration\": 171, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.08928432315587997, \"iteration\": 172, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06059391796588898, \"iteration\": 173, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04827284440398216, \"iteration\": 174, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06789202988147736, \"iteration\": 175, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06589119881391525, \"iteration\": 176, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.09823504090309143, \"iteration\": 177, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.1053038239479065, \"iteration\": 178, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.12264757603406906, \"iteration\": 179, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07452919334173203, \"iteration\": 180, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07838158309459686, \"iteration\": 181, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.028440814465284348, \"iteration\": 182, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.045127417892217636, \"iteration\": 183, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07835037261247635, \"iteration\": 184, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08048677444458008, \"iteration\": 185, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09071411192417145, \"iteration\": 186, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06839919835329056, \"iteration\": 187, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.05782085657119751, \"iteration\": 188, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.0477713905274868, \"iteration\": 189, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.030794762074947357, \"iteration\": 190, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.06860668957233429, \"iteration\": 191, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.038897521793842316, \"iteration\": 192, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07790879905223846, \"iteration\": 193, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0602860301733017, \"iteration\": 194, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.05050273239612579, \"iteration\": 195, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06388860940933228, \"iteration\": 196, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.04609619081020355, \"iteration\": 197, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.07130568474531174, \"iteration\": 198, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026394572108983994, \"iteration\": 199, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.049087293446063995, \"iteration\": 200, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05111120268702507, \"iteration\": 201, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05330336093902588, \"iteration\": 202, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.021799128502607346, \"iteration\": 203, \"epoch\": 3}, {\"training_acc\": 0.9813084112149533, \"training_loss\": 0.0583062507212162, \"iteration\": 204, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.09394620358943939, \"iteration\": 205, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.013280135579407215, \"iteration\": 206, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.015728995203971863, \"iteration\": 207, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.012318822555243969, \"iteration\": 208, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010045863687992096, \"iteration\": 209, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008782529272139072, \"iteration\": 210, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010017977096140385, \"iteration\": 211, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010144438594579697, \"iteration\": 212, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009712578728795052, \"iteration\": 213, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.01160636730492115, \"iteration\": 214, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.013580916449427605, \"iteration\": 215, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.014091688208281994, \"iteration\": 216, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008070404641330242, \"iteration\": 217, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.011179765686392784, \"iteration\": 218, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.012123661115765572, \"iteration\": 219, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.022802911698818207, \"iteration\": 220, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.011034168303012848, \"iteration\": 221, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03130485117435455, \"iteration\": 222, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006798503454774618, \"iteration\": 223, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.013413868844509125, \"iteration\": 224, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04358287155628204, \"iteration\": 225, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007185942959040403, \"iteration\": 226, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008114904165267944, \"iteration\": 227, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00936506874859333, \"iteration\": 228, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009246394969522953, \"iteration\": 229, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008310828357934952, \"iteration\": 230, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007478001527488232, \"iteration\": 231, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03542812913656235, \"iteration\": 232, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007941675372421741, \"iteration\": 233, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010865708813071251, \"iteration\": 234, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008819702081382275, \"iteration\": 235, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005393295548856258, \"iteration\": 236, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008364055305719376, \"iteration\": 237, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008042801171541214, \"iteration\": 238, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008261550217866898, \"iteration\": 239, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.01025870069861412, \"iteration\": 240, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007227742113173008, \"iteration\": 241, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005011014640331268, \"iteration\": 242, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010881500318646431, \"iteration\": 243, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005918877199292183, \"iteration\": 244, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009017959237098694, \"iteration\": 245, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0038849133998155594, \"iteration\": 246, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005273538641631603, \"iteration\": 247, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01573753170669079, \"iteration\": 248, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0038440793287009, \"iteration\": 249, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007766519207507372, \"iteration\": 250, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004590948112308979, \"iteration\": 251, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0052974289283156395, \"iteration\": 252, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0077975415624678135, \"iteration\": 253, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.003229195484891534, \"iteration\": 254, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004622108303010464, \"iteration\": 255, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03055100329220295, \"iteration\": 256, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005773676093667746, \"iteration\": 257, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00527679268270731, \"iteration\": 258, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007699673064053059, \"iteration\": 259, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006817088928073645, \"iteration\": 260, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007015650160610676, \"iteration\": 261, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005387949291616678, \"iteration\": 262, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004133470356464386, \"iteration\": 263, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005069038365036249, \"iteration\": 264, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.003902906086295843, \"iteration\": 265, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005385889671742916, \"iteration\": 266, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.044189807027578354, \"iteration\": 267, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.003333727130666375, \"iteration\": 268, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.04587208852171898, \"iteration\": 269, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006020619533956051, \"iteration\": 270, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004213398322463036, \"iteration\": 271, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006692285183817148, \"iteration\": 272, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.03331684321165085, \"iteration\": 273, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.02994578331708908, \"iteration\": 274, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002588547533378005, \"iteration\": 275, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0029535291250795126, \"iteration\": 276, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.003387457923963666, \"iteration\": 277, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002777175046503544, \"iteration\": 278, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.003135150298476219, \"iteration\": 279, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001807359978556633, \"iteration\": 280, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.003367641242220998, \"iteration\": 281, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002737743780016899, \"iteration\": 282, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0026350044645369053, \"iteration\": 283, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002741837175562978, \"iteration\": 284, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0033902046270668507, \"iteration\": 285, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00232747127301991, \"iteration\": 286, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0032031077425926924, \"iteration\": 287, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0025663087144494057, \"iteration\": 288, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0029860076028853655, \"iteration\": 289, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0024128463119268417, \"iteration\": 290, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0029149390757083893, \"iteration\": 291, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0024608077947050333, \"iteration\": 292, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002768808975815773, \"iteration\": 293, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002459667157381773, \"iteration\": 294, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0026048640720546246, \"iteration\": 295, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0022661809343844652, \"iteration\": 296, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002739987801760435, \"iteration\": 297, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0023004759568721056, \"iteration\": 298, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002530822530388832, \"iteration\": 299, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00359726557508111, \"iteration\": 300, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002118583070114255, \"iteration\": 301, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0023347162641584873, \"iteration\": 302, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002294421661645174, \"iteration\": 303, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002578369341790676, \"iteration\": 304, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0025591859593987465, \"iteration\": 305, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017062216065824032, \"iteration\": 306, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0021316553466022015, \"iteration\": 307, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019472226267680526, \"iteration\": 308, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02210826985538006, \"iteration\": 309, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002003297209739685, \"iteration\": 310, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0024477746337652206, \"iteration\": 311, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0021878499537706375, \"iteration\": 312, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017419427167624235, \"iteration\": 313, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0026851058937609196, \"iteration\": 314, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0027161315083503723, \"iteration\": 315, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0016377458814531565, \"iteration\": 316, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001972891390323639, \"iteration\": 317, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0020640878938138485, \"iteration\": 318, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0022398142609745264, \"iteration\": 319, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0021559824235737324, \"iteration\": 320, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0022652181796729565, \"iteration\": 321, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0021086730994284153, \"iteration\": 322, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019525156822055578, \"iteration\": 323, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001850576838478446, \"iteration\": 324, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0026208022609353065, \"iteration\": 325, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0021854802034795284, \"iteration\": 326, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0020298506133258343, \"iteration\": 327, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002057543955743313, \"iteration\": 328, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0014992122305557132, \"iteration\": 329, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0021014120429754257, \"iteration\": 330, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0016982960514724255, \"iteration\": 331, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0018640882335603237, \"iteration\": 332, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0027254088781774044, \"iteration\": 333, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0023957155644893646, \"iteration\": 334, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017586301546543837, \"iteration\": 335, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001978652086108923, \"iteration\": 336, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001507904496975243, \"iteration\": 337, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0015515434788540006, \"iteration\": 338, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001940779504366219, \"iteration\": 339, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0022510786075145006, \"iteration\": 340, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0903802365064621, \"iteration\": 341, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0019053822616115212, \"iteration\": 342, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0013896713498979807, \"iteration\": 343, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0013196859508752823, \"iteration\": 344, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011207166826352477, \"iteration\": 345, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0014333054423332214, \"iteration\": 346, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0014381224755197763, \"iteration\": 347, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0013125159312039614, \"iteration\": 348, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.001583680510520935, \"iteration\": 349, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0013232672354206443, \"iteration\": 350, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.001608995022252202, \"iteration\": 351, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0014533557696267962, \"iteration\": 352, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0018279100768268108, \"iteration\": 353, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011434764601290226, \"iteration\": 354, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.001372259110212326, \"iteration\": 355, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007740184664726257, \"iteration\": 356, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.001298612216487527, \"iteration\": 357, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009476307313889265, \"iteration\": 358, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011599473655223846, \"iteration\": 359, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00082686502719298, \"iteration\": 360, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0012910119257867336, \"iteration\": 361, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0012017879635095596, \"iteration\": 362, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0015187510289251804, \"iteration\": 363, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009037985000759363, \"iteration\": 364, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011442999821156263, \"iteration\": 365, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0012863632291555405, \"iteration\": 366, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00142127915751189, \"iteration\": 367, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011800378561019897, \"iteration\": 368, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011451435275375843, \"iteration\": 369, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0012448682682588696, \"iteration\": 370, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0015298685757443309, \"iteration\": 371, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011470438912510872, \"iteration\": 372, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010348106734454632, \"iteration\": 373, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0013237267266958952, \"iteration\": 374, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009605698869563639, \"iteration\": 375, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008904022397473454, \"iteration\": 376, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010938108898699284, \"iteration\": 377, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007549646543338895, \"iteration\": 378, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011332251597195864, \"iteration\": 379, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008809895953163505, \"iteration\": 380, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009359191753901541, \"iteration\": 381, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011253959964960814, \"iteration\": 382, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0013989644357934594, \"iteration\": 383, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009490849915891886, \"iteration\": 384, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010132694151252508, \"iteration\": 385, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008007173892110586, \"iteration\": 386, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011008585570380092, \"iteration\": 387, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007504851673729718, \"iteration\": 388, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005098491907119751, \"iteration\": 389, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009567092056386173, \"iteration\": 390, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008628895157016814, \"iteration\": 391, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008550598286092281, \"iteration\": 392, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006801183917559683, \"iteration\": 393, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00088959198910743, \"iteration\": 394, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0011073254281654954, \"iteration\": 395, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007842546910978854, \"iteration\": 396, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000846283626742661, \"iteration\": 397, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008669578819535673, \"iteration\": 398, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009252470335923135, \"iteration\": 399, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005663961637765169, \"iteration\": 400, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010036815656349063, \"iteration\": 401, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0012756355572491884, \"iteration\": 402, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006739584496244788, \"iteration\": 403, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009771058103069663, \"iteration\": 404, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010369964875280857, \"iteration\": 405, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008793100714683533, \"iteration\": 406, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008042125264182687, \"iteration\": 407, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009352393681183457, \"iteration\": 408, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05079648643732071, \"iteration\": 409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000745636411011219, \"iteration\": 410, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006416801479645073, \"iteration\": 411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007329710060730577, \"iteration\": 412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006911177188158035, \"iteration\": 413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006836857064627111, \"iteration\": 414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000717277405783534, \"iteration\": 415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006179935298860073, \"iteration\": 416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007952031446620822, \"iteration\": 417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009392938809469342, \"iteration\": 418, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008968135807663202, \"iteration\": 419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007459206972271204, \"iteration\": 420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006518635782413185, \"iteration\": 421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005335661116987467, \"iteration\": 422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006391322240233421, \"iteration\": 423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007429654942825437, \"iteration\": 424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008424533298239112, \"iteration\": 425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00044663200969807804, \"iteration\": 426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007433965802192688, \"iteration\": 427, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006097245495766401, \"iteration\": 428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008097882382571697, \"iteration\": 429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006064994377084076, \"iteration\": 430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006564004579558969, \"iteration\": 431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005994143430143595, \"iteration\": 432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008654126431792974, \"iteration\": 433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004216766101308167, \"iteration\": 434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007729625795036554, \"iteration\": 435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007131212623789907, \"iteration\": 436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007658139802515507, \"iteration\": 437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000705546815879643, \"iteration\": 438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006164453225210309, \"iteration\": 439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005915170186199248, \"iteration\": 440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006212624721229076, \"iteration\": 441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006275475607253611, \"iteration\": 442, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008628600626252592, \"iteration\": 443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005124341696500778, \"iteration\": 444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008307293173857033, \"iteration\": 445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007017628522589803, \"iteration\": 446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005223259213380516, \"iteration\": 447, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005099494010210037, \"iteration\": 448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005933952634222806, \"iteration\": 449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006030871882103384, \"iteration\": 450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005112153012305498, \"iteration\": 451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005719151813536882, \"iteration\": 452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000551216013263911, \"iteration\": 453, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00041485403198748827, \"iteration\": 454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00041282083839178085, \"iteration\": 455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006550619145855308, \"iteration\": 456, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005880561657249928, \"iteration\": 457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004367032088339329, \"iteration\": 458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012974805431440473, \"iteration\": 459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006149894325062633, \"iteration\": 460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007057149196043611, \"iteration\": 461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003805115702562034, \"iteration\": 462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006034295656718314, \"iteration\": 463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004826499498449266, \"iteration\": 464, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000506758107803762, \"iteration\": 465, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00046569519327022135, \"iteration\": 466, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006011433433741331, \"iteration\": 467, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007136750500649214, \"iteration\": 468, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006007438059896231, \"iteration\": 469, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004216349043417722, \"iteration\": 470, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005379109643399715, \"iteration\": 471, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00044983296538703144, \"iteration\": 472, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000608696136623621, \"iteration\": 473, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008918424136936665, \"iteration\": 474, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004395312862470746, \"iteration\": 475, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005800262442789972, \"iteration\": 476, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.05359101668000221, \"iteration\": 477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00043139004264958203, \"iteration\": 478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00046430452493950725, \"iteration\": 479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00038438348565250635, \"iteration\": 480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004779829178005457, \"iteration\": 481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004956488846801221, \"iteration\": 482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003613899170886725, \"iteration\": 483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004522798990365118, \"iteration\": 484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000555231177713722, \"iteration\": 485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00033428912865929306, \"iteration\": 486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004333097895141691, \"iteration\": 487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00039950473001226783, \"iteration\": 488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003261392121203244, \"iteration\": 489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00037658147630281746, \"iteration\": 490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003264291153755039, \"iteration\": 491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00030778683139942586, \"iteration\": 492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006864331662654877, \"iteration\": 493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00041126657743006945, \"iteration\": 494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003830701461993158, \"iteration\": 495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005415392806753516, \"iteration\": 496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00042360738734714687, \"iteration\": 497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00033573826658539474, \"iteration\": 498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004618637321982533, \"iteration\": 499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003615549358073622, \"iteration\": 500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004933092859573662, \"iteration\": 501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004948450950905681, \"iteration\": 502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005819630459882319, \"iteration\": 503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003578751056920737, \"iteration\": 504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003464213223196566, \"iteration\": 505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004224887234158814, \"iteration\": 506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000260435655945912, \"iteration\": 507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00026095932116732, \"iteration\": 508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00032897567143663764, \"iteration\": 509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003471152449492365, \"iteration\": 510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003795341763179749, \"iteration\": 511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003184647939633578, \"iteration\": 512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004196041845716536, \"iteration\": 513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004700907738879323, \"iteration\": 514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00023020803928375244, \"iteration\": 515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002967171894852072, \"iteration\": 516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004289879580028355, \"iteration\": 517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002984798629768193, \"iteration\": 518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003897932474501431, \"iteration\": 519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00039090507198125124, \"iteration\": 520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003414954408071935, \"iteration\": 521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00024571162066422403, \"iteration\": 522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004526124685071409, \"iteration\": 523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003578830510377884, \"iteration\": 524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00031725873122923076, \"iteration\": 525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00026941957185044885, \"iteration\": 526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00022647151490673423, \"iteration\": 527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003295555943623185, \"iteration\": 528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00027072810917161405, \"iteration\": 529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003358118119649589, \"iteration\": 530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00028763298178091645, \"iteration\": 531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00039508604095317423, \"iteration\": 532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00036125368205830455, \"iteration\": 533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004693716182373464, \"iteration\": 534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00026255359989590943, \"iteration\": 535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00033095068647526205, \"iteration\": 536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00030156291904859245, \"iteration\": 537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002692968992050737, \"iteration\": 538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00027008610777556896, \"iteration\": 539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004578987427521497, \"iteration\": 540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008152925292961299, \"iteration\": 541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006879449356347322, \"iteration\": 542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003063884796574712, \"iteration\": 543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003126003430224955, \"iteration\": 544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.07778248935937881, \"iteration\": 545, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00027280437643639743, \"iteration\": 546, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003958780725952238, \"iteration\": 547, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015671199653297663, \"iteration\": 548, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011042292462661862, \"iteration\": 549, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004067543428391218, \"iteration\": 550, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001143661793321371, \"iteration\": 551, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003076053224503994, \"iteration\": 552, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0021095960400998592, \"iteration\": 553, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0034267655573785305, \"iteration\": 554, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010076481848955154, \"iteration\": 555, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014152223011478782, \"iteration\": 556, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008056153310462832, \"iteration\": 557, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003832452930510044, \"iteration\": 558, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004478047485463321, \"iteration\": 559, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005901927361264825, \"iteration\": 560, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00032059173099696636, \"iteration\": 561, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00045777446939609945, \"iteration\": 562, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003019269206561148, \"iteration\": 563, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00038587316521443427, \"iteration\": 564, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002754189190454781, \"iteration\": 565, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003591555287130177, \"iteration\": 566, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00029458344215527177, \"iteration\": 567, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00042725057573989034, \"iteration\": 568, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00040283380076289177, \"iteration\": 569, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00039906700840219855, \"iteration\": 570, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00035574351204559207, \"iteration\": 571, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007697298424318433, \"iteration\": 572, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000954957096837461, \"iteration\": 573, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006596292369067669, \"iteration\": 574, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003075355780310929, \"iteration\": 575, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004905590903945267, \"iteration\": 576, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005446781869977713, \"iteration\": 577, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003499002195894718, \"iteration\": 578, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003674054460134357, \"iteration\": 579, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00030301028164103627, \"iteration\": 580, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00033509329659864306, \"iteration\": 581, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003426773473620415, \"iteration\": 582, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00021338893566280603, \"iteration\": 583, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00034578621853142977, \"iteration\": 584, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00046795696835033596, \"iteration\": 585, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00026608954067341983, \"iteration\": 586, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002950109774246812, \"iteration\": 587, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00031176029006019235, \"iteration\": 588, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003516517172101885, \"iteration\": 589, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00036019523395225406, \"iteration\": 590, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00028997284243814647, \"iteration\": 591, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00033251155400648713, \"iteration\": 592, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003013464156538248, \"iteration\": 593, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00035047641722485423, \"iteration\": 594, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00042077104444615543, \"iteration\": 595, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001591627369634807, \"iteration\": 596, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002644008200149983, \"iteration\": 597, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00032024289248511195, \"iteration\": 598, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002842099347617477, \"iteration\": 599, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00033617421286180615, \"iteration\": 600, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003222425584681332, \"iteration\": 601, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00038393907016143203, \"iteration\": 602, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00021380091493483633, \"iteration\": 603, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00029664073372259736, \"iteration\": 604, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00041739194421097636, \"iteration\": 605, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001955864718183875, \"iteration\": 606, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00019698627875186503, \"iteration\": 607, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00022447752417065203, \"iteration\": 608, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003177438920829445, \"iteration\": 609, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002764553646557033, \"iteration\": 610, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001588804880157113, \"iteration\": 611, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003328929014969617, \"iteration\": 612, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.054242148995399475, \"iteration\": 613, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022755864483769983, \"iteration\": 614, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024732714518904686, \"iteration\": 615, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018742744578048587, \"iteration\": 616, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001643701398279518, \"iteration\": 617, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002818995271809399, \"iteration\": 618, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003286953433416784, \"iteration\": 619, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00026087730657309294, \"iteration\": 620, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003098729648627341, \"iteration\": 621, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023329751275014132, \"iteration\": 622, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019724560843314976, \"iteration\": 623, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016327298362739384, \"iteration\": 624, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000172366329934448, \"iteration\": 625, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00044013006845489144, \"iteration\": 626, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00037957075983285904, \"iteration\": 627, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002154498506570235, \"iteration\": 628, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003036324633285403, \"iteration\": 629, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002951564674731344, \"iteration\": 630, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002642505569383502, \"iteration\": 631, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018569405074231327, \"iteration\": 632, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002503221621736884, \"iteration\": 633, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00036803336115553975, \"iteration\": 634, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024682850926183164, \"iteration\": 635, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003405288443900645, \"iteration\": 636, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003153781872242689, \"iteration\": 637, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00029290970996953547, \"iteration\": 638, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001851847191574052, \"iteration\": 639, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00033634601277299225, \"iteration\": 640, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001774415432009846, \"iteration\": 641, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017958377429749817, \"iteration\": 642, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022980367066338658, \"iteration\": 643, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027870808844454587, \"iteration\": 644, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00025026442017406225, \"iteration\": 645, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020222048624418676, \"iteration\": 646, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002818386419676244, \"iteration\": 647, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019756756955757737, \"iteration\": 648, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00026242033345624804, \"iteration\": 649, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002589519717730582, \"iteration\": 650, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000299978069961071, \"iteration\": 651, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021551779354922473, \"iteration\": 652, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00026108836755156517, \"iteration\": 653, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00038285984192043543, \"iteration\": 654, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002697841264307499, \"iteration\": 655, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019629155576694757, \"iteration\": 656, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027734454488381743, \"iteration\": 657, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00029674003599211574, \"iteration\": 658, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00032371998531743884, \"iteration\": 659, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017062437837012112, \"iteration\": 660, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002420141245238483, \"iteration\": 661, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00026853743474930525, \"iteration\": 662, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002196960267610848, \"iteration\": 663, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021237897453829646, \"iteration\": 664, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002797219203785062, \"iteration\": 665, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027933772071264684, \"iteration\": 666, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020626155310310423, \"iteration\": 667, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002666363725438714, \"iteration\": 668, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019437356968410313, \"iteration\": 669, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002709974651224911, \"iteration\": 670, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002083731524180621, \"iteration\": 671, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000304336630506441, \"iteration\": 672, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00047458233893848956, \"iteration\": 673, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002156898262910545, \"iteration\": 674, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016151541785802692, \"iteration\": 675, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00026449852157384157, \"iteration\": 676, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018734883633442223, \"iteration\": 677, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004110047884751111, \"iteration\": 678, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018355138308834285, \"iteration\": 679, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017249726806767285, \"iteration\": 680, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop out\n",
    "\n",
    "print(\"Train model\")\n",
    "models_dir = Path('models/es')\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    "    optimizer_params= {\"lr\": 0.001, \"weight_decay\": 0.01, }\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_data_nn, batch_size=128, shuffle=True)\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "model_nn = NeuralNetwork(\n",
    "    input_size=len(tfidf_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    dropout=0.5,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn.pt').exists() and USE_CACHE:\n",
    "    model_nn = load_model(model_nn, models_dir, 'model_nn')\n",
    "else:\n",
    "    model_nn.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn, models_dir, \"model_nn\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # X_test = torch.stack([dta[0] for dta in test])\n",
    "    X_test = torch.stack([test[0] for test in test_data_nn]).cpu()\n",
    "    y_test = torch.stack([test[1] for test in test_data_nn]).cpu()\n",
    "    y_pred = model_nn.predict(X_test).cpu()\n",
    "\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, y_pred, average='binary'))\n",
    "print(\"AUC\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn, train_config, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other classifiers\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "\"Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC, SVM\n",
    "Effective in high dimensional spaces.\n",
    "\n",
    "Still effective in cases where number of dimensions is greater than the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC with TfIdf did good on balanced English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "(0.7828114590273151, 0.9073359073359073, 0.8404864091559371, None)\n",
      "AUC: 0.7300739893332486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "# LinearSVC, tfidf\n",
    "X_train = tfidf_encoder.transform(train_raw.texts)\n",
    "print(\"Fit model\")\n",
    "model_LinearSVC_tfidf = LinearSVC()\n",
    "model_LinearSVC_tfidf.fit(X_train, train_raw.labels)\n",
    "\n",
    "pred_LinearSVC_tfidf = model_LinearSVC_tfidf.predict(tfidf_encoder.transform(test_raw.texts))\n",
    "\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_LinearSVC_tfidf, average='binary'))\n",
    "print(\"AUC:\", roc_auc_score(y_test, pred_LinearSVC_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier\n",
    "SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "\n",
    "SGD is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7813923227065712, 0.9274131274131274, 0.8481638418079096, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7332538888094443"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model_SGDClassifier_tfidf = SGDClassifier()\n",
    "model_SGDClassifier_tfidf.fit(X_train, train_raw.labels)\n",
    "\n",
    "pred_SGDClassifier_tfidf = model_SGDClassifier_tfidf.predict(tfidf_encoder.transform(test_raw.texts))\n",
    "\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_SGDClassifier_tfidf, average='binary'))\n",
    "\n",
    "roc_auc_score(test_raw.labels, pred_SGDClassifier_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Overall bad performance, not worth pursuing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model_GaussianNB_tfidf = GaussianNB()\n",
    "model_GaussianNB_tfidf.fit(X_train.toarray(), train_raw.labels)\n",
    "\n",
    "pred_GaussianNB_tfidf = model_GaussianNB_tfidf.predict(tfidf_encoder.transform(test_raw.texts).toarray())\n",
    "\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_GaussianNB_tfidf, average='binary'))\n",
    "roc_auc_score(test_raw.labels, pred_GaussianNB_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "- Neural network is still a good option\n",
    "- sklearn's SGD is also good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard count vectors & scale\n",
    "Not good on both LinearSVC and SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "encoding_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('scaler', StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "encoding_pipeline.fit(train_raw.texts)\n",
    "\n",
    "X_train = encoding_pipeline.transform(train_raw.texts)\n",
    "\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_LinearSVC_tfidf = LinearSVC()\n",
    "model_LinearSVC_tfidf.fit(X_train, train_raw.labels)\n",
    "pred_LinearSVC_tfidf = model_LinearSVC_tfidf.predict(encoding_pipeline.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_LinearSVC_tfidf, average='binary'))\n",
    "print(\"AUC:\", roc_auc_score(y_test, pred_LinearSVC_tfidf))\n",
    "\n",
    "model_SGDClassifier_tfidf = SGDClassifier()\n",
    "model_SGDClassifier_tfidf.fit(X_train, train_raw.labels)\n",
    "pred_SGDClassifier_tfidf = model_SGDClassifier_tfidf.predict(encoding_pipeline.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_SGDClassifier_tfidf, average='binary'))\n",
    "roc_auc_score(test_raw.labels, pred_SGDClassifier_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_tfidf = TfidfVectorizer(sublinear_tf=True, analyzer=\"word\", ngram_range=(3,5), max_features=10000)\n",
    "\n",
    "X_train = word_tfidf.fit_transform(train_raw.texts)\n",
    "\n",
    "\n",
    "print(\"LinearSVC\")\n",
    "model_LinearSVC_tfidf = LinearSVC()\n",
    "model_LinearSVC_tfidf.fit(X_train, train_raw.labels)\n",
    "pred_LinearSVC_tfidf = model_LinearSVC_tfidf.predict(word_tfidf.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_LinearSVC_tfidf, average='binary'))\n",
    "print(\"AUC:\", roc_auc_score(y_test, pred_LinearSVC_tfidf))\n",
    "\n",
    "print(\"SGDClassifier\")\n",
    "model_SGDClassifier_tfidf = SGDClassifier()\n",
    "model_SGDClassifier_tfidf.fit(X_train, train_raw.labels)\n",
    "\n",
    "pred_SGDClassifier_tfidf = model_SGDClassifier_tfidf.predict(word_tfidf.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_SGDClassifier_tfidf, average='binary'))\n",
    "roc_auc_score(test_raw.labels, pred_SGDClassifier_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use more tfidf word (50000) features improve 1%, but takes much more time to transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n",
      "(0.7557692307692307, 0.9104247104247104, 0.8259194395796847, None)\n",
      "AUC: 0.6938954827843717\n",
      "SGDClassifier\n",
      "(0.7554278416347382, 0.9135135135135135, 0.8269835721775602, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6940681422162904"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tfidf = TfidfVectorizer(sublinear_tf=True, analyzer=\"word\", ngram_range=(3,5), max_features=50000)\n",
    "\n",
    "X_train = word_tfidf.fit_transform(train_raw.texts)\n",
    "\n",
    "\n",
    "# import scipy\n",
    "# scipy.sparse.save_npz(\"models/tfidf/ngram_word_3to7_50000.npz\", X_train)\n",
    "\n",
    "print(\"LinearSVC\")\n",
    "model_LinearSVC_tfidf = LinearSVC()\n",
    "model_LinearSVC_tfidf.fit(X_train, train_raw.labels)\n",
    "pred_LinearSVC_tfidf = model_LinearSVC_tfidf.predict(word_tfidf.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_LinearSVC_tfidf, average='binary'))\n",
    "print(\"AUC:\", roc_auc_score(test_raw.labels, pred_LinearSVC_tfidf))\n",
    "\n",
    "print(\"SGDClassifier\")\n",
    "model_SGDClassifier_tfidf = SGDClassifier()\n",
    "model_SGDClassifier_tfidf.fit(X_train, train_raw.labels)\n",
    "\n",
    "pred_SGDClassifier_tfidf = model_SGDClassifier_tfidf.predict(word_tfidf.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_SGDClassifier_tfidf, average='binary'))\n",
    "roc_auc_score(test_raw.labels, pred_SGDClassifier_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Char ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n",
      "(0.7997293640054127, 0.9127413127413128, 0.85250631085467, None)\n",
      "AUC: 0.753352823723194\n",
      "SGDClassifier\n",
      "(0.7918313570487484, 0.9281853281853282, 0.8546036260220405, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7473574103203733"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tfidf = TfidfVectorizer(sublinear_tf=True, analyzer=\"char\", ngram_range=(3,7), max_features=50000)\n",
    "\n",
    "X_train = char_tfidf.fit_transform(train_raw.texts)\n",
    "\n",
    "import scipy\n",
    "scipy.sparse.save_npz(\"models/tfidf/ngram_char_3to7_50000.npz\", X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n",
      "Accuracy: 0.7979, Precision: 0.7918, Recall: 0.9282, F1: 0.8546, AUC: 0.7474\n",
      "SGDClassifier\n",
      "Accuracy: 0.7959, Precision: 0.7905, Recall: 0.9266, F1: 0.8532, AUC: 0.7452\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"LinearSVC\")\n",
    "model_LinearSVC_tfidf = LinearSVC()\n",
    "model_LinearSVC_tfidf.fit(X_train, train_raw.labels)\n",
    "pred_LinearSVC_tfidf = model_LinearSVC_tfidf.predict(char_tfidf.transform(test_raw.texts))\n",
    "result_LinearSVC_tfidf = evaluate(test_raw.labels, pred_SGDClassifier_tfidf)\n",
    "\n",
    "print(\"SGDClassifier\")\n",
    "model_SGDClassifier_tfidf = SGDClassifier()\n",
    "model_SGDClassifier_tfidf.fit(X_train, train_raw.labels)\n",
    "\n",
    "pred_SGDClassifier_tfidf = model_SGDClassifier_tfidf.predict(char_tfidf.transform(test_raw.texts))\n",
    "result_SGDClassifier_tfidf = evaluate(test_raw.labels, pred_SGDClassifier_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7979, Precision: 0.7918, Recall: 0.9282, F1: 0.8546, AUC: 0.7474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7979249011857708,\n",
       " 'precision': 0.7918313570487484,\n",
       " 'recall': 0.9281853281853282,\n",
       " 'f1': 0.8546036260220405,\n",
       " 'auc': 0.7473574103203733}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(y_test, y_pred) -> float:\n",
    "    \n",
    "    true_pos = sum([pred == y == 1 for pred, y in zip(y_pred, y_test)])\n",
    "    true_neg = sum([pred == y == 0 for pred, y in zip(y_pred, y_test)])\n",
    "    false_pos = sum([(pred == 1) * (y == 0) for pred, y in zip(y_pred, y_test)])\n",
    "    false_neg = sum([(pred == 0) * (y == 1) for pred, y in zip(y_pred, y_test)])\n",
    "    total = len(y_test)\n",
    "\n",
    "    accuracy = (true_pos + true_neg) / total\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1 = 2 * true_pos / (2 * true_pos + false_pos + false_neg)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    result = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc\n",
    "    }\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 68/68 [00:01<00:00, 65.04batch/s, batch_accuracy=0.813, loss=0.518]\n",
      "Epoch 2: 100%|██████████| 68/68 [00:01<00:00, 66.21batch/s, batch_accuracy=0.832, loss=0.379]\n",
      "Epoch 3: 100%|██████████| 68/68 [00:00<00:00, 69.93batch/s, batch_accuracy=0.925, loss=0.309]\n",
      "Epoch 4: 100%|██████████| 68/68 [00:01<00:00, 66.92batch/s, batch_accuracy=0.972, loss=0.12]  \n",
      "Epoch 5: 100%|██████████| 68/68 [00:00<00:00, 69.10batch/s, batch_accuracy=0.991, loss=0.0586]\n",
      "Epoch 6: 100%|██████████| 68/68 [00:01<00:00, 65.78batch/s, batch_accuracy=0.991, loss=0.0502]\n",
      "Epoch 7: 100%|██████████| 68/68 [00:01<00:00, 66.62batch/s, batch_accuracy=1, loss=0.00338]   \n",
      "Epoch 8: 100%|██████████| 68/68 [00:01<00:00, 66.00batch/s, batch_accuracy=1, loss=0.00258]   \n",
      "Epoch 9: 100%|██████████| 68/68 [00:00<00:00, 68.44batch/s, batch_accuracy=1, loss=0.00113]\n",
      "Epoch 10: 100%|██████████| 68/68 [00:01<00:00, 64.62batch/s, batch_accuracy=1, loss=0.00088] \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m     y_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([test[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m test \u001b[38;5;129;01min\u001b[39;00m test_data_nn])\u001b[38;5;241m.\u001b[39mto(model_nn\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     34\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model_nn\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m\"\u001b[39m, roc_auc_score(y_test, y_pred))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Plot training accuracy and loss side-by-side\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1775\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m \n\u001b[1;32m   1614\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1775\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1547\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1547\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/metrics/_classification.py:100\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m--> 100\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/multiclass.py:316\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/multiclass.py:171\u001b[0m, in \u001b[0;36mis_multilabel\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    169\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, VisibleDeprecationWarning)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_y_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (VisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/validation.py:1007\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1011\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/sklearn/utils/_array_api.py:746\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    744\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    748\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/torch/_tensor.py:1087\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1087\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# Test char tfidf feature on NN\n",
    "train_data_nn = encode_data(train_raw, char_tfidf)\n",
    "test_data_nn = encode_data(test_raw, char_tfidf)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_data_nn, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "USE_CACHE = False\n",
    "models_dir = Path(\"models\")\n",
    "\n",
    "model_nn = NeuralNetwork(\n",
    "    input_size=len(char_tfidf.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn.pt').exists() and USE_CACHE:\n",
    "    model_nn = load_model(model_nn, models_dir, 'model_nn')\n",
    "else:\n",
    "    model_nn.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn, models_dir, \"model_nn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7998585572842999, 0.8733590733590734, 0.8349944629014396, None)\n",
      "AUC 0.7425780277632129\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-c83cfdf1834b46079af368c882513678.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-c83cfdf1834b46079af368c882513678.vega-embed details,\n",
       "  #altair-viz-c83cfdf1834b46079af368c882513678.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-c83cfdf1834b46079af368c882513678\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-c83cfdf1834b46079af368c882513678\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-c83cfdf1834b46079af368c882513678\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-b7845598ad28d7e4bf495816e7e37269\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-b7845598ad28d7e4bf495816e7e37269\": [{\"training_acc\": 0.71875, \"training_loss\": 0.6825670003890991, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6737840175628662, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6695839166641235, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6561164855957031, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6433330178260803, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.6192862391471863, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.655867338180542, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6178705096244812, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6449909210205078, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6737445592880249, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6395412087440491, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.660193920135498, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.6016807556152344, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6360348463058472, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6373251080513, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6201752424240112, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.6035482287406921, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.615105390548706, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5796573162078857, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6187509298324585, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5854553580284119, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6243829727172852, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5651721954345703, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6403824090957642, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6314356327056885, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.533381462097168, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6661070585250854, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5641010999679565, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.600019097328186, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.6064589619636536, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.614656925201416, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6092414259910583, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5564138889312744, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6450324654579163, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6390364170074463, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5530010461807251, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5882064700126648, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5649434328079224, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5782806873321533, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5810430645942688, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.5811024308204651, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5458444952964783, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6331746578216553, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5477661490440369, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.511924147605896, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.5760127305984497, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5377721786499023, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5854505300521851, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5308409929275513, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5210458040237427, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5312174558639526, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.5337535738945007, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5319833755493164, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.5630954504013062, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.54651939868927, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.5279467105865479, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.5550763010978699, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.5734115839004517, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5094981789588928, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5143011212348938, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.5548133850097656, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.5388855338096619, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5321666598320007, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5162820219993591, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4621516466140747, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5470939874649048, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6277451515197754, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.8130841121495327, \"training_loss\": 0.5179875493049622, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.48412036895751953, \"iteration\": 69, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.44992169737815857, \"iteration\": 70, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4585703909397125, \"iteration\": 71, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.40523117780685425, \"iteration\": 72, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.42845791578292847, \"iteration\": 73, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.45170316100120544, \"iteration\": 74, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.4507313668727875, \"iteration\": 75, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.4483228921890259, \"iteration\": 76, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.47634655237197876, \"iteration\": 77, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.40810009837150574, \"iteration\": 78, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.4089793860912323, \"iteration\": 79, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4741583466529846, \"iteration\": 80, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.43949562311172485, \"iteration\": 81, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.41214364767074585, \"iteration\": 82, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4324701726436615, \"iteration\": 83, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.395649790763855, \"iteration\": 84, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.427823007106781, \"iteration\": 85, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.413920521736145, \"iteration\": 86, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3334811329841614, \"iteration\": 87, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.37577468156814575, \"iteration\": 88, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.3097427785396576, \"iteration\": 89, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4028550982475281, \"iteration\": 90, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3519584536552429, \"iteration\": 91, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3567997217178345, \"iteration\": 92, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.33606159687042236, \"iteration\": 93, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3419489562511444, \"iteration\": 94, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.38998138904571533, \"iteration\": 95, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.3795478641986847, \"iteration\": 96, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4081283211708069, \"iteration\": 97, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3071778416633606, \"iteration\": 98, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3847867250442505, \"iteration\": 99, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4068279266357422, \"iteration\": 100, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.34309181571006775, \"iteration\": 101, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.29423579573631287, \"iteration\": 102, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.29078733921051025, \"iteration\": 103, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.352486252784729, \"iteration\": 104, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.2676492929458618, \"iteration\": 105, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3808049261569977, \"iteration\": 106, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.34705525636672974, \"iteration\": 107, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3427506983280182, \"iteration\": 108, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.2947531044483185, \"iteration\": 109, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.334994912147522, \"iteration\": 110, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.29536721110343933, \"iteration\": 111, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.43493765592575073, \"iteration\": 112, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3247374892234802, \"iteration\": 113, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3453381061553955, \"iteration\": 114, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2961539030075073, \"iteration\": 115, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.34661322832107544, \"iteration\": 116, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.36626318097114563, \"iteration\": 117, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.30072832107543945, \"iteration\": 118, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3784504532814026, \"iteration\": 119, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30282309651374817, \"iteration\": 120, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3116566836833954, \"iteration\": 121, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3704516887664795, \"iteration\": 122, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.33587509393692017, \"iteration\": 123, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.30008307099342346, \"iteration\": 124, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.38859954476356506, \"iteration\": 125, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.35036543011665344, \"iteration\": 126, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3292567729949951, \"iteration\": 127, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2823944687843323, \"iteration\": 128, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3055863678455353, \"iteration\": 129, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3429711163043976, \"iteration\": 130, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3080366253852844, \"iteration\": 131, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.31966596841812134, \"iteration\": 132, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.2679813504219055, \"iteration\": 133, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3690352141857147, \"iteration\": 134, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2800810635089874, \"iteration\": 135, \"epoch\": 2}, {\"training_acc\": 0.8317757009345794, \"training_loss\": 0.3794834017753601, \"iteration\": 136, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2980949580669403, \"iteration\": 137, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2857721447944641, \"iteration\": 138, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21813233196735382, \"iteration\": 139, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19334575533866882, \"iteration\": 140, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2380906045436859, \"iteration\": 141, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26176613569259644, \"iteration\": 142, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2516571283340454, \"iteration\": 143, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2204984426498413, \"iteration\": 144, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2221222221851349, \"iteration\": 145, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17269332706928253, \"iteration\": 146, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.16891682147979736, \"iteration\": 147, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2303812950849533, \"iteration\": 148, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2909546196460724, \"iteration\": 149, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16934041678905487, \"iteration\": 150, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25002777576446533, \"iteration\": 151, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1819794774055481, \"iteration\": 152, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18003463745117188, \"iteration\": 153, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.15750303864479065, \"iteration\": 154, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18850916624069214, \"iteration\": 155, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2280709147453308, \"iteration\": 156, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2747925817966461, \"iteration\": 157, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.16476887464523315, \"iteration\": 158, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17394721508026123, \"iteration\": 159, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.23950299620628357, \"iteration\": 160, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2536560297012329, \"iteration\": 161, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22398759424686432, \"iteration\": 162, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.19989526271820068, \"iteration\": 163, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2070530205965042, \"iteration\": 164, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2369137555360794, \"iteration\": 165, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.1912202686071396, \"iteration\": 166, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.18855080008506775, \"iteration\": 167, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2181103527545929, \"iteration\": 168, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2319745421409607, \"iteration\": 169, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22673159837722778, \"iteration\": 170, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2002113163471222, \"iteration\": 171, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.14460909366607666, \"iteration\": 172, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.28729456663131714, \"iteration\": 173, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23079341650009155, \"iteration\": 174, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1692536175251007, \"iteration\": 175, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.26184654235839844, \"iteration\": 176, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.22225333750247955, \"iteration\": 177, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.22251476347446442, \"iteration\": 178, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2295372486114502, \"iteration\": 179, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1852482557296753, \"iteration\": 180, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21107985079288483, \"iteration\": 181, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22994530200958252, \"iteration\": 182, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2254394292831421, \"iteration\": 183, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2315325140953064, \"iteration\": 184, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2080463021993637, \"iteration\": 185, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3208983540534973, \"iteration\": 186, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27422797679901123, \"iteration\": 187, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26299193501472473, \"iteration\": 188, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1954209953546524, \"iteration\": 189, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.24434736371040344, \"iteration\": 190, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.19094741344451904, \"iteration\": 191, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.20128066837787628, \"iteration\": 192, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22163549065589905, \"iteration\": 193, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.20260505378246307, \"iteration\": 194, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19640150666236877, \"iteration\": 195, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21347351372241974, \"iteration\": 196, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.22721263766288757, \"iteration\": 197, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.26203620433807373, \"iteration\": 198, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22347529232501984, \"iteration\": 199, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.17669308185577393, \"iteration\": 200, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2698490023612976, \"iteration\": 201, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22056597471237183, \"iteration\": 202, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18590563535690308, \"iteration\": 203, \"epoch\": 3}, {\"training_acc\": 0.9252336448598131, \"training_loss\": 0.3091365098953247, \"iteration\": 204, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.13737061619758606, \"iteration\": 205, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.18665818870067596, \"iteration\": 206, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.17207780480384827, \"iteration\": 207, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12354627251625061, \"iteration\": 208, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16057194769382477, \"iteration\": 209, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.14646746218204498, \"iteration\": 210, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09040630608797073, \"iteration\": 211, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1664440929889679, \"iteration\": 212, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.13228604197502136, \"iteration\": 213, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09952868521213531, \"iteration\": 214, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13107100129127502, \"iteration\": 215, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1290641725063324, \"iteration\": 216, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14779168367385864, \"iteration\": 217, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13014690577983856, \"iteration\": 218, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.14759330451488495, \"iteration\": 219, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.11969408392906189, \"iteration\": 220, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1599225401878357, \"iteration\": 221, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12403504550457001, \"iteration\": 222, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1285121738910675, \"iteration\": 223, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10350102931261063, \"iteration\": 224, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12464682757854462, \"iteration\": 225, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11321447789669037, \"iteration\": 226, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0997176468372345, \"iteration\": 227, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.146327406167984, \"iteration\": 228, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10516443848609924, \"iteration\": 229, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12695437669754028, \"iteration\": 230, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14666487276554108, \"iteration\": 231, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1080087274312973, \"iteration\": 232, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12060417234897614, \"iteration\": 233, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10820700973272324, \"iteration\": 234, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11549115926027298, \"iteration\": 235, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10286174714565277, \"iteration\": 236, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07943744957447052, \"iteration\": 237, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1029747873544693, \"iteration\": 238, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1369088888168335, \"iteration\": 239, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06605853885412216, \"iteration\": 240, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12435184419155121, \"iteration\": 241, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0688604861497879, \"iteration\": 242, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0942683219909668, \"iteration\": 243, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.14496594667434692, \"iteration\": 244, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09582354128360748, \"iteration\": 245, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07876117527484894, \"iteration\": 246, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0952892154455185, \"iteration\": 247, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.14659689366817474, \"iteration\": 248, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07550615072250366, \"iteration\": 249, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12352576106786728, \"iteration\": 250, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16442403197288513, \"iteration\": 251, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10633315145969391, \"iteration\": 252, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09825682640075684, \"iteration\": 253, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.13994480669498444, \"iteration\": 254, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.12830859422683716, \"iteration\": 255, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.14000126719474792, \"iteration\": 256, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08979269862174988, \"iteration\": 257, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.13484561443328857, \"iteration\": 258, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.10946902632713318, \"iteration\": 259, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.12829065322875977, \"iteration\": 260, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.20115920901298523, \"iteration\": 261, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.069706492125988, \"iteration\": 262, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18253907561302185, \"iteration\": 263, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08974798023700714, \"iteration\": 264, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.12751422822475433, \"iteration\": 265, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1547325849533081, \"iteration\": 266, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.09112802147865295, \"iteration\": 267, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.13204209506511688, \"iteration\": 268, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10646053403615952, \"iteration\": 269, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.15769194066524506, \"iteration\": 270, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17085754871368408, \"iteration\": 271, \"epoch\": 4}, {\"training_acc\": 0.9719626168224299, \"training_loss\": 0.11997798085212708, \"iteration\": 272, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08161291480064392, \"iteration\": 273, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.041376590728759766, \"iteration\": 274, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09337180852890015, \"iteration\": 275, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09559184312820435, \"iteration\": 276, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.057048022747039795, \"iteration\": 277, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.038550786674022675, \"iteration\": 278, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06778709590435028, \"iteration\": 279, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07332170754671097, \"iteration\": 280, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.04375479370355606, \"iteration\": 281, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0591721348464489, \"iteration\": 282, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04734392464160919, \"iteration\": 283, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03774087131023407, \"iteration\": 284, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09737879782915115, \"iteration\": 285, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0526697114109993, \"iteration\": 286, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04379144310951233, \"iteration\": 287, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07990509271621704, \"iteration\": 288, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.05726762115955353, \"iteration\": 289, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.050215091556310654, \"iteration\": 290, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0494084395468235, \"iteration\": 291, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.035620443522930145, \"iteration\": 292, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07934888452291489, \"iteration\": 293, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.043838366866111755, \"iteration\": 294, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.059029627591371536, \"iteration\": 295, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1089584231376648, \"iteration\": 296, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.043955471366643906, \"iteration\": 297, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09679104387760162, \"iteration\": 298, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07367870211601257, \"iteration\": 299, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0861089825630188, \"iteration\": 300, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08165454864501953, \"iteration\": 301, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07709255814552307, \"iteration\": 302, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.045287080109119415, \"iteration\": 303, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06236235424876213, \"iteration\": 304, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.034144479781389236, \"iteration\": 305, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.10412301123142242, \"iteration\": 306, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05394182354211807, \"iteration\": 307, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0402861014008522, \"iteration\": 308, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0334145687520504, \"iteration\": 309, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0414881706237793, \"iteration\": 310, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08171708136796951, \"iteration\": 311, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.0737881064414978, \"iteration\": 312, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0677327960729599, \"iteration\": 313, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04653369262814522, \"iteration\": 314, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.052443478256464005, \"iteration\": 315, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.06216517463326454, \"iteration\": 316, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.05046793818473816, \"iteration\": 317, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03448561951518059, \"iteration\": 318, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08263643085956573, \"iteration\": 319, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.09363266825675964, \"iteration\": 320, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.037284739315509796, \"iteration\": 321, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05245890095829964, \"iteration\": 322, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05026223883032799, \"iteration\": 323, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.04035983607172966, \"iteration\": 324, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.05058389529585838, \"iteration\": 325, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.046929240226745605, \"iteration\": 326, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05629514902830124, \"iteration\": 327, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.04497797042131424, \"iteration\": 328, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04442740976810455, \"iteration\": 329, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.035363197326660156, \"iteration\": 330, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07937505841255188, \"iteration\": 331, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.06339458376169205, \"iteration\": 332, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11713580787181854, \"iteration\": 333, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.053210534155368805, \"iteration\": 334, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.0789923146367073, \"iteration\": 335, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06433717906475067, \"iteration\": 336, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.05633523687720299, \"iteration\": 337, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.044441934674978256, \"iteration\": 338, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.028934510424733162, \"iteration\": 339, \"epoch\": 5}, {\"training_acc\": 0.9906542056074766, \"training_loss\": 0.058564476668834686, \"iteration\": 340, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026243671774864197, \"iteration\": 341, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.016686780378222466, \"iteration\": 342, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.018904011696577072, \"iteration\": 343, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.051518965512514114, \"iteration\": 344, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.020698249340057373, \"iteration\": 345, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.016757283359766006, \"iteration\": 346, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05436473339796066, \"iteration\": 347, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012113256379961967, \"iteration\": 348, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.018019169569015503, \"iteration\": 349, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02140388824045658, \"iteration\": 350, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.022901058197021484, \"iteration\": 351, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01609296165406704, \"iteration\": 352, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01583348587155342, \"iteration\": 353, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013073790818452835, \"iteration\": 354, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.026821007952094078, \"iteration\": 355, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.045324891805648804, \"iteration\": 356, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03620064631104469, \"iteration\": 357, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014370573684573174, \"iteration\": 358, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012678351253271103, \"iteration\": 359, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.017279738560318947, \"iteration\": 360, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05075018107891083, \"iteration\": 361, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01848398894071579, \"iteration\": 362, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.020975148305296898, \"iteration\": 363, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.015749122947454453, \"iteration\": 364, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03686492517590523, \"iteration\": 365, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05490056052803993, \"iteration\": 366, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01736820489168167, \"iteration\": 367, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012333530001342297, \"iteration\": 368, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014983377419412136, \"iteration\": 369, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01580125093460083, \"iteration\": 370, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.043216608464717865, \"iteration\": 371, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01389833353459835, \"iteration\": 372, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011265723034739494, \"iteration\": 373, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.017304658889770508, \"iteration\": 374, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02457909658551216, \"iteration\": 375, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04588495194911957, \"iteration\": 376, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.018057413399219513, \"iteration\": 377, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011726215481758118, \"iteration\": 378, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011720404028892517, \"iteration\": 379, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01151997223496437, \"iteration\": 380, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02712498977780342, \"iteration\": 381, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.047616999596357346, \"iteration\": 382, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011372026987373829, \"iteration\": 383, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0120288897305727, \"iteration\": 384, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.020632266998291016, \"iteration\": 385, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010095048695802689, \"iteration\": 386, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.015950357541441917, \"iteration\": 387, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011377949267625809, \"iteration\": 388, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.032640162855386734, \"iteration\": 389, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010676011443138123, \"iteration\": 390, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01816537417471409, \"iteration\": 391, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.040033020079135895, \"iteration\": 392, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012892275117337704, \"iteration\": 393, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026039903983473778, \"iteration\": 394, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.020254161208868027, \"iteration\": 395, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03232935070991516, \"iteration\": 396, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009589300490915775, \"iteration\": 397, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007695061154663563, \"iteration\": 398, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01729145646095276, \"iteration\": 399, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01633552834391594, \"iteration\": 400, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013562070205807686, \"iteration\": 401, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0205831378698349, \"iteration\": 402, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.044138215482234955, \"iteration\": 403, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04085228964686394, \"iteration\": 404, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02674061432480812, \"iteration\": 405, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011692054569721222, \"iteration\": 406, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008628562092781067, \"iteration\": 407, \"epoch\": 6}, {\"training_acc\": 0.9906542056074766, \"training_loss\": 0.05019791051745415, \"iteration\": 408, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008683482185006142, \"iteration\": 409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006611723452806473, \"iteration\": 410, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02448505163192749, \"iteration\": 411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009251940995454788, \"iteration\": 412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005829899571835995, \"iteration\": 413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006519693415611982, \"iteration\": 414, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.019944243133068085, \"iteration\": 415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010916688479483128, \"iteration\": 416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006731582805514336, \"iteration\": 417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004589344374835491, \"iteration\": 418, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005108000710606575, \"iteration\": 419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006636846344918013, \"iteration\": 420, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.016432641074061394, \"iteration\": 421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005655772052705288, \"iteration\": 422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005306262988597155, \"iteration\": 423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005338096059858799, \"iteration\": 424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004199251066893339, \"iteration\": 425, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.027777448296546936, \"iteration\": 426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004481889307498932, \"iteration\": 427, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007578497752547264, \"iteration\": 428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007505158893764019, \"iteration\": 429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006453939247876406, \"iteration\": 430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005306371487677097, \"iteration\": 431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007106928154826164, \"iteration\": 432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0052894181571900845, \"iteration\": 433, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.015065180137753487, \"iteration\": 434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005483274348080158, \"iteration\": 435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005478912964463234, \"iteration\": 436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00635053776204586, \"iteration\": 437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005897757597267628, \"iteration\": 438, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03369113430380821, \"iteration\": 439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003627386409789324, \"iteration\": 440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006939474493265152, \"iteration\": 441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003766357433050871, \"iteration\": 442, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0051813586615026, \"iteration\": 443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00441256957128644, \"iteration\": 444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006017114967107773, \"iteration\": 445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00953065138310194, \"iteration\": 446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005467519164085388, \"iteration\": 447, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.014232436195015907, \"iteration\": 448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005619863513857126, \"iteration\": 449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004840456880629063, \"iteration\": 450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003786422312259674, \"iteration\": 451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005641359835863113, \"iteration\": 452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006525931879878044, \"iteration\": 453, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005711570847779512, \"iteration\": 454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005549844820052385, \"iteration\": 455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005721707362681627, \"iteration\": 456, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005383868236094713, \"iteration\": 457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005785136483609676, \"iteration\": 458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004977158270776272, \"iteration\": 459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004061367362737656, \"iteration\": 460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0067990003153681755, \"iteration\": 461, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.03143397718667984, \"iteration\": 462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004628718830645084, \"iteration\": 463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0062754107639193535, \"iteration\": 464, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.012770585715770721, \"iteration\": 465, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006049254443496466, \"iteration\": 466, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004423418082296848, \"iteration\": 467, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0052024186588823795, \"iteration\": 468, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02503209561109543, \"iteration\": 469, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005733957514166832, \"iteration\": 470, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004012742545455694, \"iteration\": 471, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006791817955672741, \"iteration\": 472, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006471750792115927, \"iteration\": 473, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026675578206777573, \"iteration\": 474, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004826700780540705, \"iteration\": 475, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0033831181935966015, \"iteration\": 476, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004064488690346479, \"iteration\": 477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004051811061799526, \"iteration\": 478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0029131623450666666, \"iteration\": 479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003255977761000395, \"iteration\": 480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005691379774361849, \"iteration\": 481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0030660382471978664, \"iteration\": 482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0026619727723300457, \"iteration\": 483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002832751488313079, \"iteration\": 484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00324905663728714, \"iteration\": 485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0032580813858658075, \"iteration\": 486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0032006343826651573, \"iteration\": 487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002720529679208994, \"iteration\": 488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00803382322192192, \"iteration\": 489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0035905479453504086, \"iteration\": 490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002885229652747512, \"iteration\": 491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0030045511666685343, \"iteration\": 492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006710757501423359, \"iteration\": 493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002542155561968684, \"iteration\": 494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0029873205348849297, \"iteration\": 495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0037256302312016487, \"iteration\": 496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003316578222438693, \"iteration\": 497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002804039977490902, \"iteration\": 498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003083796938881278, \"iteration\": 499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00281796557828784, \"iteration\": 500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0031718311365693808, \"iteration\": 501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0026683565229177475, \"iteration\": 502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0025727590546011925, \"iteration\": 503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006405271589756012, \"iteration\": 504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003201611340045929, \"iteration\": 505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002659175544977188, \"iteration\": 506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004121981095522642, \"iteration\": 507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0022605773992836475, \"iteration\": 508, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.013788693584501743, \"iteration\": 509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0028060206677764654, \"iteration\": 510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0025062342174351215, \"iteration\": 511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0031011193059384823, \"iteration\": 512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002625676104798913, \"iteration\": 513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0031367738265544176, \"iteration\": 514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0018173002172261477, \"iteration\": 515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002336733043193817, \"iteration\": 516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016849037492647767, \"iteration\": 517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0020590757485479116, \"iteration\": 518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002209747675806284, \"iteration\": 519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0027359528467059135, \"iteration\": 520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016779296565800905, \"iteration\": 521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00402130838483572, \"iteration\": 522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002313370583578944, \"iteration\": 523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0025290162302553654, \"iteration\": 524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003037871792912483, \"iteration\": 525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0030426206067204475, \"iteration\": 526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002153083449229598, \"iteration\": 527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0018471860093995929, \"iteration\": 528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0019458020105957985, \"iteration\": 529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0014846125850453973, \"iteration\": 530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0023751109838485718, \"iteration\": 531, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.014085182920098305, \"iteration\": 532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0056703174486756325, \"iteration\": 533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001782204257324338, \"iteration\": 534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0019258912652730942, \"iteration\": 535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002605907618999481, \"iteration\": 536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001730529242195189, \"iteration\": 537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0024447955656796694, \"iteration\": 538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007328181527554989, \"iteration\": 539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017576664686203003, \"iteration\": 540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002218293258920312, \"iteration\": 541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002626886824145913, \"iteration\": 542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0023554384242743254, \"iteration\": 543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0025778361596167088, \"iteration\": 544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016539423959329724, \"iteration\": 545, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015519940061494708, \"iteration\": 546, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001664418145082891, \"iteration\": 547, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0019697872921824455, \"iteration\": 548, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017679054290056229, \"iteration\": 549, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0037402561865746975, \"iteration\": 550, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014983557630330324, \"iteration\": 551, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001612497610040009, \"iteration\": 552, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015044594183564186, \"iteration\": 553, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001953477505594492, \"iteration\": 554, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016285229939967394, \"iteration\": 555, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001440342515707016, \"iteration\": 556, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012041495647281408, \"iteration\": 557, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001338559901341796, \"iteration\": 558, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014692782424390316, \"iteration\": 559, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012979130260646343, \"iteration\": 560, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0022840213496237993, \"iteration\": 561, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023955211509019136, \"iteration\": 562, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016351430676877499, \"iteration\": 563, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013328628847375512, \"iteration\": 564, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014314362779259682, \"iteration\": 565, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001299686380662024, \"iteration\": 566, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014332503778859973, \"iteration\": 567, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013360914308577776, \"iteration\": 568, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012028163764625788, \"iteration\": 569, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015810694312676787, \"iteration\": 570, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001155678415670991, \"iteration\": 571, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001044978853315115, \"iteration\": 572, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012725151609629393, \"iteration\": 573, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013043973594903946, \"iteration\": 574, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012312091421335936, \"iteration\": 575, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002450689673423767, \"iteration\": 576, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010639680549502373, \"iteration\": 577, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002661045640707016, \"iteration\": 578, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011076750233769417, \"iteration\": 579, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00212749931961298, \"iteration\": 580, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016009254613891244, \"iteration\": 581, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012649715645238757, \"iteration\": 582, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00475981505587697, \"iteration\": 583, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029676351696252823, \"iteration\": 584, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010434340219944715, \"iteration\": 585, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0019334193784743547, \"iteration\": 586, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018284120596945286, \"iteration\": 587, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00209967908449471, \"iteration\": 588, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001479443395510316, \"iteration\": 589, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012939156731590629, \"iteration\": 590, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001027036109007895, \"iteration\": 591, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020653940737247467, \"iteration\": 592, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011863709660246968, \"iteration\": 593, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001047913683578372, \"iteration\": 594, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011823551030829549, \"iteration\": 595, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001368122873827815, \"iteration\": 596, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003333189059048891, \"iteration\": 597, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015154930297285318, \"iteration\": 598, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012216453906148672, \"iteration\": 599, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014528346946462989, \"iteration\": 600, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016285781748592854, \"iteration\": 601, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003284473903477192, \"iteration\": 602, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001260503544472158, \"iteration\": 603, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016895208973437548, \"iteration\": 604, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001393596176058054, \"iteration\": 605, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014550467021763325, \"iteration\": 606, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012672164011746645, \"iteration\": 607, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013814086560159922, \"iteration\": 608, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015345016727223992, \"iteration\": 609, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014224222395569086, \"iteration\": 610, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016672746278345585, \"iteration\": 611, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011280656326562166, \"iteration\": 612, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011269246460869908, \"iteration\": 613, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010963388485834002, \"iteration\": 614, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014016019413247705, \"iteration\": 615, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010647515300661325, \"iteration\": 616, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010526480618864298, \"iteration\": 617, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008843680843710899, \"iteration\": 618, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010274127125740051, \"iteration\": 619, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001501099206507206, \"iteration\": 620, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010086288675665855, \"iteration\": 621, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0012067209463566542, \"iteration\": 622, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0012608428951352835, \"iteration\": 623, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011642095632851124, \"iteration\": 624, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010163555853068829, \"iteration\": 625, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0020531797781586647, \"iteration\": 626, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015743812546133995, \"iteration\": 627, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007319628493860364, \"iteration\": 628, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009937176946550608, \"iteration\": 629, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008081833366304636, \"iteration\": 630, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013898896286264062, \"iteration\": 631, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009558458114042878, \"iteration\": 632, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0012256462359800935, \"iteration\": 633, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009890971705317497, \"iteration\": 634, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009897002018988132, \"iteration\": 635, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010020553600043058, \"iteration\": 636, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009556891163811088, \"iteration\": 637, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0012608962133526802, \"iteration\": 638, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014941690023988485, \"iteration\": 639, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010910764103755355, \"iteration\": 640, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009714586776681244, \"iteration\": 641, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010378739098086953, \"iteration\": 642, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010562355164438486, \"iteration\": 643, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011284261709079146, \"iteration\": 644, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001007881248369813, \"iteration\": 645, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009759500389918685, \"iteration\": 646, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009811744093894958, \"iteration\": 647, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000900093000382185, \"iteration\": 648, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010772031964734197, \"iteration\": 649, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006577718304470181, \"iteration\": 650, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008648946532048285, \"iteration\": 651, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007958546048030257, \"iteration\": 652, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011774399317800999, \"iteration\": 653, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009925751946866512, \"iteration\": 654, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009823930449783802, \"iteration\": 655, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013992195017635822, \"iteration\": 656, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007821205072104931, \"iteration\": 657, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007947537233121693, \"iteration\": 658, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007330633234232664, \"iteration\": 659, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015170525293797255, \"iteration\": 660, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008018960943445563, \"iteration\": 661, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008014958584681153, \"iteration\": 662, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008389953291043639, \"iteration\": 663, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009602897334843874, \"iteration\": 664, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001089542987756431, \"iteration\": 665, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007871602429077029, \"iteration\": 666, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004861367342527956, \"iteration\": 667, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006044954061508179, \"iteration\": 668, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007729027420282364, \"iteration\": 669, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007733349921181798, \"iteration\": 670, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00211994512937963, \"iteration\": 671, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006821876158937812, \"iteration\": 672, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009150473051704466, \"iteration\": 673, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011610580841079354, \"iteration\": 674, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008677972364239395, \"iteration\": 675, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007684200536459684, \"iteration\": 676, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011024014092981815, \"iteration\": 677, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008340909262187779, \"iteration\": 678, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001447040936909616, \"iteration\": 679, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008797231712378561, \"iteration\": 680, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    # X_test = torch.stack([dta[0] for dta in test])\n",
    "    X_test = torch.stack([test[0] for test in test_data_nn]).cpu()\n",
    "    y_test = torch.stack([test[1] for test in test_data_nn]).cpu()\n",
    "    y_pred = model_nn.predict(X_test).cpu()\n",
    "\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, y_pred, average='binary'))\n",
    "print(\"AUC\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn, train_config, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-identification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from models import NeuralNetwork, TrainConfig, evaluate_nn_model\n",
    "from utils import load_data, split_data, encode_data\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(\"Device: cuda\")\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    print(\"Device: cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview of the flow:**\n",
    "1. Load the raw data in to a RawParliamentData object containing (text ID, speaker ID, text, label)\n",
    "2. Split the raw data into train, dev, test datasets. Data is split so that speakers in each set does not appear in another set.\n",
    "3. Prepare a TfidfVectorizer. Fit the vectorizer on the train set, and use it to transform all train, dev, test sets. Use the `create_dataset()` function on the RawParliamentData objects, and supply the fitted encoder so that the same trained encoder is used on all sets.\n",
    "4. Run the `train_neural_network()` function.\n",
    "\n",
    "**To test different types of train-dev-test sets:**\n",
    "If you want to use some countries as the **train set**, and some other countries as the **dev & test set**, you will need to load the train, dev, test countries separately. For example: \n",
    "\n",
    "```python\n",
    "train_raw = load_data(folder_path=\"data/power/\", file_list=['power-gb-train.tsv',],text_head='text_en')\n",
    "dev_raw = load_data(folder_path=\"data/power/\", file_list=['power-ua-train.tsv',],text_head='text_en')\n",
    "test_raw = load_data(folder_path=\"data/power/\", file_list=['power-cz-train.tsv',],text_head='text_en')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load power-gb-train.tsv...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%%\n",
    "file_list = [\n",
    "    'power-gb-train.tsv',\n",
    "    # 'power-ua-train.tsv',\n",
    "    # 'power-fr-train.tsv',\n",
    "    # 'power-nl-train.tsv',\n",
    "]\n",
    "\n",
    "full_data = load_data(folder_path=\"data/power/\", file_list=file_list,text_head='text_en')\n",
    "train_dev_raw, test_raw = split_data(full_data, test_size=0.2, random_state=0)\n",
    "train_raw, dev_raw = split_data(train_dev_raw, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data encoder...\n",
      "Prepare data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prepare data encoder...\")\n",
    "train_encoder = TfidfVectorizer(sublinear_tf=True, analyzer=\"char\", ngram_range=(1,3))\n",
    "train_encoder.fit(train_raw.texts)\n",
    "\n",
    "print(\"Prepare data...\")\n",
    "train_dataset = encode_data(train_raw, train_encoder)\n",
    "dev_dataset = encode_data(dev_raw, train_encoder)\n",
    "test_dataset = encode_data(test_raw, train_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model.\n",
    "If you use Google Colab or your machine has a CUDA-supported graphic card, you can try setting `device='cuda'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 168/168 [00:03<00:00, 53.81batch/s]\n"
     ]
    }
   ],
   "source": [
    "train_config = TrainConfig(\n",
    "    num_epochs      = 1,\n",
    "    early_stop      = True,\n",
    "    violation_limit = 5,\n",
    ")\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    input_size=len(train_encoder.vocabulary_),\n",
    "    num_classes=2,\n",
    "    hidden_size=128,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "model.fit(\n",
    "    train_dataloader   = train_dataloader,\n",
    "    dev_dataloader     = dev_dataloader,\n",
    "    train_config       = train_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model's final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7496002190046804 0.7082020798259653 0.708926310652036\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = evaluate_nn_model(model, test_dataset)\n",
    "print(precision, recall, f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-identification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from models import train_neural_network, evaluate_model\n",
    "from utils import DataProcessor, CustomDataset\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the data creation flow:\n",
    "1. Load the raw data into a list of tuples (text ID, speaker ID, text, label)\n",
    "2. Split the raw data into train, dev, test datasets, if you only need to randomly split data\n",
    "3. Prepare a TfidfVectorizer. Fit the vectorizer on the train set, and use it to transform all train, dev, test sets.\n",
    "4. Prepare the CustomDataset objects to be fed to the `train_` function.\n",
    "\n",
    "What's important is that at the end, the train, dev, test sets must be CustomDataset objects. So if you want to use some countries as the **train set**, and some other countries as the **dev & test set**, you will need to load the train, dev, test countries separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load power-gb-train.tsv...\n",
      "Load power-ua-train.tsv...\n",
      "Prepare data encoder...\n",
      "Prepare data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%%\n",
    "file_list = [\n",
    "    'power-gb-train.tsv',\n",
    "    'power-ua-train.tsv'\n",
    "]\n",
    "\n",
    "processor = DataProcessor()\n",
    "\n",
    "raw_data = processor.load_data(\n",
    "    folder_path=\"data/power/\",\n",
    "    file_list=file_list,\n",
    "    text_head='text_en'\n",
    ")\n",
    "\n",
    "train_dev_raw, test_raw = processor.split_data(raw_data, test_size=0.2)\n",
    "train_raw, dev_raw = processor.split_data(train_dev_raw, test_size=0.2)\n",
    "\n",
    "#%%\n",
    "print(\"Prepare data encoder...\")\n",
    "train_texts = [tup[2] for tup in train_raw]\n",
    "train_encoder = TfidfVectorizer(sublinear_tf=True, analyzer=\"char\", ngram_range=(1,3))\n",
    "train_encoder.fit(train_texts)\n",
    "\n",
    "print(\"Prepare data...\")\n",
    "train_dataset = CustomDataset(train_raw, train_encoder)\n",
    "dev_dataset = CustomDataset(dev_raw, train_encoder)\n",
    "test_dataset = CustomDataset(test_raw, train_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model.\n",
    "If you use Google Colab or your machine has a CUDA-supported graphic card, you can try setting `device='cuda'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 443/443 [00:03<00:00, 118.17batch/s]\n",
      "Epoch 2: 100%|██████████| 443/443 [00:03<00:00, 116.17batch/s]\n",
      "Epoch 3: 100%|██████████| 443/443 [00:03<00:00, 119.85batch/s]\n",
      "Epoch 4: 100%|██████████| 443/443 [00:03<00:00, 121.25batch/s]\n",
      "Epoch 5: 100%|██████████| 443/443 [00:03<00:00, 122.35batch/s]\n",
      "Epoch 6: 100%|██████████| 443/443 [00:03<00:00, 112.63batch/s]\n",
      "Epoch 7: 100%|██████████| 443/443 [00:03<00:00, 116.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss did not improve for 5 epochs. Stopping early.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Train model...\")\n",
    "model = train_neural_network(\n",
    "    train_data=train_dataset,\n",
    "    dev_data=dev_dataset,\n",
    "    num_classes=2,\n",
    "    hidden_size=64,\n",
    "    num_epochs=20,\n",
    "    early_stop_patience=5,\n",
    "    device='cpu'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model's final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7163155387764184 0.7146647603415488 0.7152942856591957\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = evaluate_model(model, test_dataset)\n",
    "print(precision, recall, f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-identification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

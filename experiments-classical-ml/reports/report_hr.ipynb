{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Total: 4,091.48 MB\n",
      "Reserved: 0.00 MB\n",
      "Allocated: 0.00 MB\n",
      "Free in reserved: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from models import NeuralNetwork, RNNClassifier, TrainConfig, evaluate, save_model, load_model, plot_results\n",
    "from utils import load_data, split_data, encode_data, check_cuda_memory, PositionalEncoder\n",
    "from pathlib import Path\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device: cuda\")\n",
    "    check_cuda_memory()\n",
    "else:\n",
    "    print(\"Device: cpu\")\n",
    "\n",
    "models_dir = Path('models/gb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 10741, % positive class: 39.73%\n"
     ]
    }
   ],
   "source": [
    "data = load_data(folder_path=\"data/train/power/\", file_list=['power-hr-train.tsv'],text_head='text')\n",
    "train_raw, test_raw = split_data(data, test_size=0.2, random_state=0)\n",
    "\n",
    "print(f\"Data size: {len(data)}, % positive class: {sum(data.labels) / len(data) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare words_encoder...\n",
      "Prepare chars_encoder...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, max_features=50000, ngram_range=(3, 5),\n",
       "                sublinear_tf=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, max_features=50000, ngram_range=(3, 5),\n",
       "                sublinear_tf=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(analyzer='char', max_features=50000, ngram_range=(3, 5),\n",
       "                sublinear_tf=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"Prepare words_encoder...\")\n",
    "words_encoder = TfidfVectorizer(max_features=50000)\n",
    "words_encoder.fit(train_raw.texts)\n",
    "\n",
    "print(\"Prepare chars_encoder...\")\n",
    "chars_encoder = TfidfVectorizer(max_features=50000, analyzer=\"char\", ngram_range=(3,5), use_idf=True, sublinear_tf=True)\n",
    "chars_encoder.fit(train_raw.texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data...\n",
      "Train model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 55.83batch/s, batch_accuracy=0.688, loss=0.569]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 66.90batch/s, batch_accuracy=0.909, loss=0.288]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 66.78batch/s, batch_accuracy=1, loss=0.025]     \n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:00<00:00, 68.56batch/s, batch_accuracy=1, loss=0.00358]   \n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:00<00:00, 69.56batch/s, batch_accuracy=1, loss=0.00138]   \n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:00<00:00, 68.93batch/s, batch_accuracy=1, loss=0.00063]    \n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 65.45batch/s, batch_accuracy=1, loss=0.000254]   \n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 65.83batch/s, batch_accuracy=1, loss=0.000113]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:00<00:00, 71.80batch/s, batch_accuracy=1, loss=6.76e-5] \n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:00<00:00, 72.62batch/s, batch_accuracy=1, loss=5.25e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6667, Precision: 0.5624, Recall: 0.5411, F1: 0.5515, AUC: 0.6914\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-c8826ccffc36420281fea90f3c11cb80.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-c8826ccffc36420281fea90f3c11cb80.vega-embed details,\n",
       "  #altair-viz-c8826ccffc36420281fea90f3c11cb80.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-c8826ccffc36420281fea90f3c11cb80\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-c8826ccffc36420281fea90f3c11cb80\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-c8826ccffc36420281fea90f3c11cb80\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-92d78b1270d96b40ad16fd6f05b52c31\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-92d78b1270d96b40ad16fd6f05b52c31\": [{\"training_acc\": 0.6640625, \"training_loss\": 0.6921868920326233, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6904283165931702, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6896173357963562, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6877145767211914, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6862053871154785, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6845877766609192, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6805912256240845, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6788844466209412, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6865072250366211, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.674633264541626, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.676996648311615, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6846951842308044, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6744865775108337, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.674066960811615, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6834822297096252, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6810600757598877, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6553148627281189, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6900529861450195, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6665540933609009, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6629143953323364, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6619207859039307, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6610695123672485, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.668956995010376, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6811428070068359, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6755048036575317, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6971359252929688, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6613073348999023, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6818597912788391, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6478581428527832, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.665975034236908, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6801027059555054, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 0.6991639137268066, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6726241111755371, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.661620020866394, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6846835613250732, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6663196086883545, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6675007343292236, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6594246625900269, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6532771587371826, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6480677723884583, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6603779196739197, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.647579550743103, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6575279235839844, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6378961801528931, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6511660814285278, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6285139322280884, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6361122131347656, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6267244219779968, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6480177640914917, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6458739638328552, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6492522358894348, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6464442610740662, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.648984968662262, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6170614957809448, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6291360855102539, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6273942589759827, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6069859266281128, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6152411103248596, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5879385471343994, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6236818432807922, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.597326934337616, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5762428045272827, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6096270084381104, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5667080879211426, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.583336353302002, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6194868683815002, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6281849145889282, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.6883116883116883, \"training_loss\": 0.5691314339637756, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.8984375, \"training_loss\": 0.47815611958503723, \"iteration\": 69, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4895132780075073, \"iteration\": 70, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.5107256770133972, \"iteration\": 71, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.48341822624206543, \"iteration\": 72, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.45520296692848206, \"iteration\": 73, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4909948706626892, \"iteration\": 74, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.42604950070381165, \"iteration\": 75, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.4369173049926758, \"iteration\": 76, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.4144069254398346, \"iteration\": 77, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4364722967147827, \"iteration\": 78, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.3773622512817383, \"iteration\": 79, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.3811662197113037, \"iteration\": 80, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.3976700007915497, \"iteration\": 81, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.41048455238342285, \"iteration\": 82, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.4127757251262665, \"iteration\": 83, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4420245587825775, \"iteration\": 84, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3935946226119995, \"iteration\": 85, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.38950055837631226, \"iteration\": 86, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3885064721107483, \"iteration\": 87, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.3579370975494385, \"iteration\": 88, \"epoch\": 2}, {\"training_acc\": 0.953125, \"training_loss\": 0.30557781457901, \"iteration\": 89, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3393806517124176, \"iteration\": 90, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3217722177505493, \"iteration\": 91, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3002660870552063, \"iteration\": 92, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3101217746734619, \"iteration\": 93, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.3306941092014313, \"iteration\": 94, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3169293999671936, \"iteration\": 95, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3351873755455017, \"iteration\": 96, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.30983075499534607, \"iteration\": 97, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3270496129989624, \"iteration\": 98, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2665596902370453, \"iteration\": 99, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2946147322654724, \"iteration\": 100, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2770998477935791, \"iteration\": 101, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.266907662153244, \"iteration\": 102, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.28706175088882446, \"iteration\": 103, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.253578782081604, \"iteration\": 104, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2840718626976013, \"iteration\": 105, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3174501061439514, \"iteration\": 106, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3214253783226013, \"iteration\": 107, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4151899516582489, \"iteration\": 108, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.33466827869415283, \"iteration\": 109, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.31947100162506104, \"iteration\": 110, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.29519620537757874, \"iteration\": 111, \"epoch\": 2}, {\"training_acc\": 0.96875, \"training_loss\": 0.2402557134628296, \"iteration\": 112, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.293537974357605, \"iteration\": 113, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.27814561128616333, \"iteration\": 114, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.4925641417503357, \"iteration\": 115, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.29460716247558594, \"iteration\": 116, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2288096696138382, \"iteration\": 117, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2649478018283844, \"iteration\": 118, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3431583046913147, \"iteration\": 119, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30710622668266296, \"iteration\": 120, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.32200032472610474, \"iteration\": 121, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2395603507757187, \"iteration\": 122, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.34844493865966797, \"iteration\": 123, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2888380289077759, \"iteration\": 124, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2685006856918335, \"iteration\": 125, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.31223809719085693, \"iteration\": 126, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.29437124729156494, \"iteration\": 127, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.38314735889434814, \"iteration\": 128, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3333439528942108, \"iteration\": 129, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.26549896597862244, \"iteration\": 130, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2844289541244507, \"iteration\": 131, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2922589182853699, \"iteration\": 132, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2555525600910187, \"iteration\": 133, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.30483341217041016, \"iteration\": 134, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.28580665588378906, \"iteration\": 135, \"epoch\": 2}, {\"training_acc\": 0.9090909090909091, \"training_loss\": 0.288019597530365, \"iteration\": 136, \"epoch\": 2}, {\"training_acc\": 1.0, \"training_loss\": 0.07505874335765839, \"iteration\": 137, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.07230041921138763, \"iteration\": 138, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.07830128073692322, \"iteration\": 139, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.07522410154342651, \"iteration\": 140, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.05761518329381943, \"iteration\": 141, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08196791261434555, \"iteration\": 142, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.11278413981199265, \"iteration\": 143, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08958646655082703, \"iteration\": 144, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.0778813362121582, \"iteration\": 145, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07324935495853424, \"iteration\": 146, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.05572523921728134, \"iteration\": 147, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05883575603365898, \"iteration\": 148, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05650999769568443, \"iteration\": 149, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07582482695579529, \"iteration\": 150, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06672269105911255, \"iteration\": 151, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0847356989979744, \"iteration\": 152, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.04113660007715225, \"iteration\": 153, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05682475492358208, \"iteration\": 154, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04949836805462837, \"iteration\": 155, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.06416846066713333, \"iteration\": 156, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08149196207523346, \"iteration\": 157, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.060147952288389206, \"iteration\": 158, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05399000644683838, \"iteration\": 159, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.048639170825481415, \"iteration\": 160, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09385973960161209, \"iteration\": 161, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08723233640193939, \"iteration\": 162, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0461750403046608, \"iteration\": 163, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.047396186739206314, \"iteration\": 164, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.06898525357246399, \"iteration\": 165, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04399268329143524, \"iteration\": 166, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.10125674307346344, \"iteration\": 167, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.05109046772122383, \"iteration\": 168, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.055408187210559845, \"iteration\": 169, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.03040372021496296, \"iteration\": 170, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.035653866827487946, \"iteration\": 171, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.060210708528757095, \"iteration\": 172, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06530468165874481, \"iteration\": 173, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.09507167339324951, \"iteration\": 174, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06197572126984596, \"iteration\": 175, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.046571165323257446, \"iteration\": 176, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.03486119210720062, \"iteration\": 177, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.04629330337047577, \"iteration\": 178, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.050241995602846146, \"iteration\": 179, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.05161414295434952, \"iteration\": 180, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06734930723905563, \"iteration\": 181, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.037166427820920944, \"iteration\": 182, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.03437561169266701, \"iteration\": 183, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.06773938238620758, \"iteration\": 184, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.07525762915611267, \"iteration\": 185, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.09710629284381866, \"iteration\": 186, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.048725154250860214, \"iteration\": 187, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.02762928605079651, \"iteration\": 188, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.05350187420845032, \"iteration\": 189, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08283534646034241, \"iteration\": 190, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.03282085061073303, \"iteration\": 191, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.09060683846473694, \"iteration\": 192, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.04643324017524719, \"iteration\": 193, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04274242743849754, \"iteration\": 194, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05954552814364433, \"iteration\": 195, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04485339671373367, \"iteration\": 196, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05429720878601074, \"iteration\": 197, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06385747343301773, \"iteration\": 198, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.042628780007362366, \"iteration\": 199, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06603659689426422, \"iteration\": 200, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.03534798324108124, \"iteration\": 201, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.03683488816022873, \"iteration\": 202, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.027709858492016792, \"iteration\": 203, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.024952709674835205, \"iteration\": 204, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.009106956422328949, \"iteration\": 205, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008984701707959175, \"iteration\": 206, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.011976685374975204, \"iteration\": 207, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009330647066235542, \"iteration\": 208, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.017839781939983368, \"iteration\": 209, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00993164349347353, \"iteration\": 210, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.011200999841094017, \"iteration\": 211, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.011913467198610306, \"iteration\": 212, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008627640083432198, \"iteration\": 213, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.011663548648357391, \"iteration\": 214, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007300934288650751, \"iteration\": 215, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.011407904326915741, \"iteration\": 216, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00916809868067503, \"iteration\": 217, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006292739883065224, \"iteration\": 218, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008201796561479568, \"iteration\": 219, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03872095048427582, \"iteration\": 220, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007492083124816418, \"iteration\": 221, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.01032291166484356, \"iteration\": 222, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010132290422916412, \"iteration\": 223, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010570913553237915, \"iteration\": 224, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007483175490051508, \"iteration\": 225, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005774926394224167, \"iteration\": 226, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0058089569211006165, \"iteration\": 227, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007161227520555258, \"iteration\": 228, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008211992681026459, \"iteration\": 229, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00586449820548296, \"iteration\": 230, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006885083392262459, \"iteration\": 231, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006945203989744186, \"iteration\": 232, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006409849040210247, \"iteration\": 233, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006285958923399448, \"iteration\": 234, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005238241516053677, \"iteration\": 235, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00623694621026516, \"iteration\": 236, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007460962515324354, \"iteration\": 237, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004640858620405197, \"iteration\": 238, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03598269447684288, \"iteration\": 239, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005098644644021988, \"iteration\": 240, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005799487233161926, \"iteration\": 241, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.024653643369674683, \"iteration\": 242, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005455946549773216, \"iteration\": 243, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01875041052699089, \"iteration\": 244, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009029524400830269, \"iteration\": 245, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005152221769094467, \"iteration\": 246, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.007259039208292961, \"iteration\": 247, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.003671759506687522, \"iteration\": 248, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07752668857574463, \"iteration\": 249, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006733971182256937, \"iteration\": 250, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008670839481055737, \"iteration\": 251, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0036246394738554955, \"iteration\": 252, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004005672410130501, \"iteration\": 253, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004303193651139736, \"iteration\": 254, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006599819287657738, \"iteration\": 255, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008242245763540268, \"iteration\": 256, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005943595431745052, \"iteration\": 257, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.006093963049352169, \"iteration\": 258, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.008612102828919888, \"iteration\": 259, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.00325689441524446, \"iteration\": 260, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004784394055604935, \"iteration\": 261, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0055056046694517136, \"iteration\": 262, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005352843552827835, \"iteration\": 263, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004528649151325226, \"iteration\": 264, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004636452533304691, \"iteration\": 265, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004860999993979931, \"iteration\": 266, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.010358547791838646, \"iteration\": 267, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005735822021961212, \"iteration\": 268, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.005826016888022423, \"iteration\": 269, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.009187718853354454, \"iteration\": 270, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.004089802503585815, \"iteration\": 271, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0035753280390053988, \"iteration\": 272, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.003016761504113674, \"iteration\": 273, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00349359679967165, \"iteration\": 274, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002289277734234929, \"iteration\": 275, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03327512368559837, \"iteration\": 276, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0032008416019380093, \"iteration\": 277, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002947034779936075, \"iteration\": 278, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.003423544578254223, \"iteration\": 279, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002998196054250002, \"iteration\": 280, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0022827705834060907, \"iteration\": 281, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0042004091665148735, \"iteration\": 282, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0023516465444117785, \"iteration\": 283, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.003164278343319893, \"iteration\": 284, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0022944877855479717, \"iteration\": 285, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0026043334510177374, \"iteration\": 286, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002748355967923999, \"iteration\": 287, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00248664035461843, \"iteration\": 288, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002419492229819298, \"iteration\": 289, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002258900087326765, \"iteration\": 290, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0022872923873364925, \"iteration\": 291, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0024142188485711813, \"iteration\": 292, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002028131391853094, \"iteration\": 293, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001886832294985652, \"iteration\": 294, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0018849364714697003, \"iteration\": 295, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00235194806009531, \"iteration\": 296, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0016366846393793821, \"iteration\": 297, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019998960196971893, \"iteration\": 298, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0021318718791007996, \"iteration\": 299, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019241401460021734, \"iteration\": 300, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0018145089270547032, \"iteration\": 301, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001536405412480235, \"iteration\": 302, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0019480314804241061, \"iteration\": 303, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0014825151301920414, \"iteration\": 304, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001993761397898197, \"iteration\": 305, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00212088949047029, \"iteration\": 306, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0014762398786842823, \"iteration\": 307, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0018096354324370623, \"iteration\": 308, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017594522796571255, \"iteration\": 309, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.002175448462367058, \"iteration\": 310, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0020128393080085516, \"iteration\": 311, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013577528297901154, \"iteration\": 312, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0015531606040894985, \"iteration\": 313, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017589544877409935, \"iteration\": 314, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0012065847404301167, \"iteration\": 315, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001226147636771202, \"iteration\": 316, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013861511833965778, \"iteration\": 317, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0015496613923460245, \"iteration\": 318, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013822237960994244, \"iteration\": 319, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001125306822359562, \"iteration\": 320, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001695394515991211, \"iteration\": 321, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013231801567599177, \"iteration\": 322, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0014957699459046125, \"iteration\": 323, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0018471323419362307, \"iteration\": 324, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0015131460968405008, \"iteration\": 325, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02768673561513424, \"iteration\": 326, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0010130638256669044, \"iteration\": 327, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0014739097096025944, \"iteration\": 328, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0012565450742840767, \"iteration\": 329, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0011129716876894236, \"iteration\": 330, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.014577447436749935, \"iteration\": 331, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001132528530433774, \"iteration\": 332, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0012340361718088388, \"iteration\": 333, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0011229621013626456, \"iteration\": 334, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013977019116282463, \"iteration\": 335, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02607075683772564, \"iteration\": 336, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0011087254388257861, \"iteration\": 337, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0010846328223124146, \"iteration\": 338, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0011696306755766273, \"iteration\": 339, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0013848409289494157, \"iteration\": 340, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.001124642789363861, \"iteration\": 341, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009741652756929398, \"iteration\": 342, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009441751171834767, \"iteration\": 343, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007916473550722003, \"iteration\": 344, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00093253911472857, \"iteration\": 345, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008585307514294982, \"iteration\": 346, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010055607417598367, \"iteration\": 347, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008917775703594089, \"iteration\": 348, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009397211251780391, \"iteration\": 349, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009457952110096812, \"iteration\": 350, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007779165171086788, \"iteration\": 351, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006320964312180877, \"iteration\": 352, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0010547151323407888, \"iteration\": 353, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009010949870571494, \"iteration\": 354, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007760155713185668, \"iteration\": 355, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00076943205203861, \"iteration\": 356, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006380167324095964, \"iteration\": 357, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0009100404568016529, \"iteration\": 358, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006467199418693781, \"iteration\": 359, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0008998567936941981, \"iteration\": 360, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007002806523814797, \"iteration\": 361, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007504589739255607, \"iteration\": 362, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007362218457274139, \"iteration\": 363, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007381566101685166, \"iteration\": 364, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006979021709412336, \"iteration\": 365, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005944262375123799, \"iteration\": 366, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006528018275275826, \"iteration\": 367, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006757950177416205, \"iteration\": 368, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007452581194229424, \"iteration\": 369, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000728180049918592, \"iteration\": 370, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006570798577740788, \"iteration\": 371, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007700267597101629, \"iteration\": 372, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007791302632540464, \"iteration\": 373, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007255024975165725, \"iteration\": 374, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006048977375030518, \"iteration\": 375, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006611792487092316, \"iteration\": 376, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005804159445688128, \"iteration\": 377, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005517977988347411, \"iteration\": 378, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0004896294558420777, \"iteration\": 379, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007043799851089716, \"iteration\": 380, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006273690378293395, \"iteration\": 381, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005222863401286304, \"iteration\": 382, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005412091268226504, \"iteration\": 383, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006996659794822335, \"iteration\": 384, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005656484863720834, \"iteration\": 385, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005172574892640114, \"iteration\": 386, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006255373591557145, \"iteration\": 387, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000622220104560256, \"iteration\": 388, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007003521313890815, \"iteration\": 389, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000782215385697782, \"iteration\": 390, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0004799561866093427, \"iteration\": 391, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005758396000601351, \"iteration\": 392, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005954810185357928, \"iteration\": 393, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0007049869163893163, \"iteration\": 394, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00046159702469594777, \"iteration\": 395, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.009875187650322914, \"iteration\": 396, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0004805264179594815, \"iteration\": 397, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006754841888323426, \"iteration\": 398, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0166497603058815, \"iteration\": 399, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.000392717745853588, \"iteration\": 400, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006044607143849134, \"iteration\": 401, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0004963698447681963, \"iteration\": 402, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00043800866114906967, \"iteration\": 403, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0005543846637010574, \"iteration\": 404, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0004357824509497732, \"iteration\": 405, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.018763426691293716, \"iteration\": 406, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0004626532318070531, \"iteration\": 407, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0006296532228589058, \"iteration\": 408, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0035712679382413626, \"iteration\": 409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003524553612805903, \"iteration\": 410, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004408759414218366, \"iteration\": 411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004374513518996537, \"iteration\": 412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000454675464425236, \"iteration\": 413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003768217866308987, \"iteration\": 414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00046297968947328627, \"iteration\": 415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004344504850450903, \"iteration\": 416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000538778374902904, \"iteration\": 417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00042342045344412327, \"iteration\": 418, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004656720848288387, \"iteration\": 419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003294547204859555, \"iteration\": 420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00039479503175243735, \"iteration\": 421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00036791915772482753, \"iteration\": 422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00030623713973909616, \"iteration\": 423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003084533382207155, \"iteration\": 424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00040736631490290165, \"iteration\": 425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005352935986593366, \"iteration\": 426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00044183063437230885, \"iteration\": 427, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00030427714227698743, \"iteration\": 428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0004097626078873873, \"iteration\": 429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003576400631573051, \"iteration\": 430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00035569636384025216, \"iteration\": 431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00038274400867521763, \"iteration\": 432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002835462219081819, \"iteration\": 433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00042718404438346624, \"iteration\": 434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00027566769858822227, \"iteration\": 435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003469967341516167, \"iteration\": 436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000254021433647722, \"iteration\": 437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00028143744566477835, \"iteration\": 438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00025543669471517205, \"iteration\": 439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003670229052659124, \"iteration\": 440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003128464159090072, \"iteration\": 441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00027125998167321086, \"iteration\": 442, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001010431908071041, \"iteration\": 443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002891607291530818, \"iteration\": 444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002612813259474933, \"iteration\": 445, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.006987912580370903, \"iteration\": 446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00037366009200923145, \"iteration\": 447, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002460154064465314, \"iteration\": 448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002038245293078944, \"iteration\": 449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00027823448181152344, \"iteration\": 450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00027261156355962157, \"iteration\": 451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003161694621667266, \"iteration\": 452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002445424906909466, \"iteration\": 453, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002767971600405872, \"iteration\": 454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002394332259427756, \"iteration\": 455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000286576192593202, \"iteration\": 456, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00023603496083524078, \"iteration\": 457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00020732672419399023, \"iteration\": 458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00024822281557135284, \"iteration\": 459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0003073064726777375, \"iteration\": 460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00029257830465212464, \"iteration\": 461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00027584380586631596, \"iteration\": 462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002627320936881006, \"iteration\": 463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002719018957577646, \"iteration\": 464, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00023703154874965549, \"iteration\": 465, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00023425021208822727, \"iteration\": 466, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00021289497090037912, \"iteration\": 467, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00026563950814306736, \"iteration\": 468, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002464827266521752, \"iteration\": 469, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00020964615396223962, \"iteration\": 470, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00035707291681319475, \"iteration\": 471, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00025373039534315467, \"iteration\": 472, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002202219911850989, \"iteration\": 473, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00034205534029752016, \"iteration\": 474, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0002501755952835083, \"iteration\": 475, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00025420013116672635, \"iteration\": 476, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0001949399447767064, \"iteration\": 477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00018877779075410217, \"iteration\": 478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016343135212082416, \"iteration\": 479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00017697607108857483, \"iteration\": 480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00019874780264217407, \"iteration\": 481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015536296996288002, \"iteration\": 482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00020073911582585424, \"iteration\": 483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015577266458421946, \"iteration\": 484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015505895134992898, \"iteration\": 485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00014734019350726157, \"iteration\": 486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015658933261875063, \"iteration\": 487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0042748223058879375, \"iteration\": 488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00017956647207029164, \"iteration\": 489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016314002277795225, \"iteration\": 490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001608713500900194, \"iteration\": 491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001621980918571353, \"iteration\": 492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001960679655894637, \"iteration\": 493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015375578368548304, \"iteration\": 494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016715571109671146, \"iteration\": 495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00014727267262060195, \"iteration\": 496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013872311683371663, \"iteration\": 497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016152842727024108, \"iteration\": 498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00019461866759229451, \"iteration\": 499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00018124440975952893, \"iteration\": 500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00023576707462780178, \"iteration\": 501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00014819533680565655, \"iteration\": 502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016332129598595202, \"iteration\": 503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00012330830213613808, \"iteration\": 504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00020380818750709295, \"iteration\": 505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001430986449122429, \"iteration\": 506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00012797655654139817, \"iteration\": 507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011674420238705352, \"iteration\": 508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009663615492172539, \"iteration\": 509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00014239326992537826, \"iteration\": 510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00021651388669852167, \"iteration\": 511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00010365361958974972, \"iteration\": 512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00014910224126651883, \"iteration\": 513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00012191222049295902, \"iteration\": 514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00022783355962019414, \"iteration\": 515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016324265743605793, \"iteration\": 516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015409651678055525, \"iteration\": 517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00012633507139980793, \"iteration\": 518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00015966856153681874, \"iteration\": 519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00010492745059309527, \"iteration\": 520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011679346789605916, \"iteration\": 521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00016189667803701013, \"iteration\": 522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004864828661084175, \"iteration\": 523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001393501297570765, \"iteration\": 524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00012801625416614115, \"iteration\": 525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00010838632442755625, \"iteration\": 526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00010880593617912382, \"iteration\": 527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001127235300373286, \"iteration\": 528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 9.70210530795157e-05, \"iteration\": 529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011946314771194011, \"iteration\": 530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011773737787734717, \"iteration\": 531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00013713537191506475, \"iteration\": 532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011103942961199209, \"iteration\": 533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00010953178571071476, \"iteration\": 534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00010136492346646264, \"iteration\": 535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00010228464816464111, \"iteration\": 536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0001610519684618339, \"iteration\": 537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011841770901810378, \"iteration\": 538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00012614397564902902, \"iteration\": 539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00010520323849050328, \"iteration\": 540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 7.916449249023572e-05, \"iteration\": 541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 8.959848491940647e-05, \"iteration\": 542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011829688446596265, \"iteration\": 543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00011331303539918736, \"iteration\": 544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 9.700885129859671e-05, \"iteration\": 545, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001118990039685741, \"iteration\": 546, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.883328700903803e-05, \"iteration\": 547, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001292610540986061, \"iteration\": 548, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.81117269070819e-05, \"iteration\": 549, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011191458906978369, \"iteration\": 550, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010046201350633055, \"iteration\": 551, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.755430462770164e-05, \"iteration\": 552, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.967289224732667e-05, \"iteration\": 553, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.86844500200823e-05, \"iteration\": 554, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001423689245712012, \"iteration\": 555, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.535773329436779e-05, \"iteration\": 556, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011245943460380659, \"iteration\": 557, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.370556315639988e-05, \"iteration\": 558, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.545751188648865e-05, \"iteration\": 559, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.247957157436758e-05, \"iteration\": 560, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.395407737931237e-05, \"iteration\": 561, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.152747225016356e-05, \"iteration\": 562, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.038047235459089e-05, \"iteration\": 563, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.905288657639176e-05, \"iteration\": 564, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.966643508756533e-05, \"iteration\": 565, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011254117998760194, \"iteration\": 566, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00299687497317791, \"iteration\": 567, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.974580173846334e-05, \"iteration\": 568, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.856639422243461e-05, \"iteration\": 569, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010360460873926058, \"iteration\": 570, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.881996862124652e-05, \"iteration\": 571, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.903203368186951e-05, \"iteration\": 572, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.975063635967672e-05, \"iteration\": 573, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.497938349843025e-05, \"iteration\": 574, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.206156053347513e-05, \"iteration\": 575, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.625598275102675e-05, \"iteration\": 576, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.51787772262469e-05, \"iteration\": 577, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.113418880384415e-05, \"iteration\": 578, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.409280467778444e-05, \"iteration\": 579, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.841739716241136e-05, \"iteration\": 580, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.298320269910619e-05, \"iteration\": 581, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000639076461084187, \"iteration\": 582, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.914799789432436e-05, \"iteration\": 583, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.284800085471943e-05, \"iteration\": 584, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.279646186158061e-05, \"iteration\": 585, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.008439570199698e-05, \"iteration\": 586, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.044210360618308e-05, \"iteration\": 587, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.758627518545836e-05, \"iteration\": 588, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.623050234746188e-05, \"iteration\": 589, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.4753039876231924e-05, \"iteration\": 590, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.67140300013125e-05, \"iteration\": 591, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 9.094154665945098e-05, \"iteration\": 592, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.669263919116929e-05, \"iteration\": 593, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00027354940539225936, \"iteration\": 594, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.444907143712044e-05, \"iteration\": 595, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.663847645744681e-05, \"iteration\": 596, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.074560562614352e-05, \"iteration\": 597, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.919656880199909e-05, \"iteration\": 598, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.151807221816853e-05, \"iteration\": 599, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.136901017976925e-05, \"iteration\": 600, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.22393485577777e-05, \"iteration\": 601, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.4160576837603e-05, \"iteration\": 602, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.197542436188087e-05, \"iteration\": 603, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.730035627493635e-05, \"iteration\": 604, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 7.469022239092737e-05, \"iteration\": 605, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.935259468969889e-05, \"iteration\": 606, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.392141222022474e-05, \"iteration\": 607, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 8.811696898192167e-05, \"iteration\": 608, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 5.555710959015414e-05, \"iteration\": 609, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 4.476237518247217e-05, \"iteration\": 610, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010653859499143437, \"iteration\": 611, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 6.759161624358967e-05, \"iteration\": 612, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0024369743186980486, \"iteration\": 613, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.547644443344325e-05, \"iteration\": 614, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.459510768763721e-05, \"iteration\": 615, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.518984835362062e-05, \"iteration\": 616, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.283058610279113e-05, \"iteration\": 617, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.5382504797307774e-05, \"iteration\": 618, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.284726987360045e-05, \"iteration\": 619, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.6839926653774455e-05, \"iteration\": 620, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.9920731069287285e-05, \"iteration\": 621, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.2280418711015955e-05, \"iteration\": 622, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.101038550492376e-05, \"iteration\": 623, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.3639298382913694e-05, \"iteration\": 624, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.676446355413646e-05, \"iteration\": 625, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00044132760376669466, \"iteration\": 626, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.0288286476861686e-05, \"iteration\": 627, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.7062923840712756e-05, \"iteration\": 628, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.061416934244335e-05, \"iteration\": 629, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.388161662267521e-05, \"iteration\": 630, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.2864303142996505e-05, \"iteration\": 631, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.590643948176876e-05, \"iteration\": 632, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.925880421069451e-05, \"iteration\": 633, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.664335913024843e-05, \"iteration\": 634, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.952223778469488e-05, \"iteration\": 635, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.065429261070676e-05, \"iteration\": 636, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.227536894381046e-05, \"iteration\": 637, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.4622553105000407e-05, \"iteration\": 638, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.645096462103538e-05, \"iteration\": 639, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.1378468924667686e-05, \"iteration\": 640, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.432451820932329e-05, \"iteration\": 641, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.717124440707266e-05, \"iteration\": 642, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.412697985069826e-05, \"iteration\": 643, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.922847776673734e-05, \"iteration\": 644, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.4187345085665584e-05, \"iteration\": 645, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.194917321205139e-05, \"iteration\": 646, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.874081448884681e-05, \"iteration\": 647, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.4173168134875596e-05, \"iteration\": 648, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.4881580834044144e-05, \"iteration\": 649, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.3624822865240276e-05, \"iteration\": 650, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.690046964446083e-05, \"iteration\": 651, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.2480765589280054e-05, \"iteration\": 652, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.565731796901673e-05, \"iteration\": 653, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.7824822104303166e-05, \"iteration\": 654, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.3339619878679514e-05, \"iteration\": 655, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.204554327065125e-05, \"iteration\": 656, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.249048652127385e-05, \"iteration\": 657, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 2.738063813012559e-05, \"iteration\": 658, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020796321041416377, \"iteration\": 659, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.092456558486447e-05, \"iteration\": 660, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.674669591826387e-05, \"iteration\": 661, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.267644883133471e-05, \"iteration\": 662, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.763989661820233e-05, \"iteration\": 663, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.166780076455325e-05, \"iteration\": 664, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.629862814908847e-05, \"iteration\": 665, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.4727781641995534e-05, \"iteration\": 666, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.620741674443707e-05, \"iteration\": 667, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.917945261695422e-05, \"iteration\": 668, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.8163856263272464e-05, \"iteration\": 669, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.543785689747892e-05, \"iteration\": 670, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.216430872678757e-05, \"iteration\": 671, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.490149437217042e-05, \"iteration\": 672, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.08142659580335e-05, \"iteration\": 673, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.854801980196498e-05, \"iteration\": 674, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.232248102198355e-05, \"iteration\": 675, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.7915513530606404e-05, \"iteration\": 676, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.4692467781715095e-05, \"iteration\": 677, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 3.849142376566306e-05, \"iteration\": 678, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.7422937970841303e-05, \"iteration\": 679, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.25292671227362e-05, \"iteration\": 680, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Prepare data...\")\n",
    "train_nn_words = encode_data(train_raw, words_encoder)\n",
    "test_nn_words = encode_data(test_raw, words_encoder)\n",
    "\n",
    "print(\"Train model\")\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    "    \n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_nn_words, batch_size=128, shuffle=True)\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "model_nn_words = NeuralNetwork(\n",
    "    input_size=len(words_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn_words.pt').exists() and USE_CACHE:\n",
    "    model_nn_words = load_model(model_nn_words, models_dir, 'model_nn_words')\n",
    "else:\n",
    "    model_nn_words.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn_words, models_dir, \"model_nn_words\")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    X_test_nn = torch.stack([test[0] for test in test_nn_words]).cpu()\n",
    "    y_test_nn = torch.stack([test[1] for test in test_nn_words]).cpu()\n",
    "    y_pred_nn_words = model_nn_words.predict(X_test_nn)\n",
    "    logits_nn_words = model_nn_words.forward(X_test_nn)\n",
    "\n",
    "result_nn_words = evaluate(y_test_nn.cpu(), y_pred_nn_words.cpu(), logits_nn_words.cpu())\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn_words, train_config, dataloader)\n",
    "\n",
    "model_nn_words.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data...\n",
      "Train model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 66.10batch/s, batch_accuracy=0.857, loss=0.478]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:00<00:00, 68.57batch/s, batch_accuracy=0.896, loss=0.358]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:00<00:00, 69.11batch/s, batch_accuracy=0.922, loss=0.192]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 67.85batch/s, batch_accuracy=1, loss=0.0679]    \n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:00<00:00, 69.23batch/s, batch_accuracy=1, loss=0.0188]    \n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 65.31batch/s, batch_accuracy=1, loss=0.0186]    \n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 62.56batch/s, batch_accuracy=1, loss=0.00127]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 62.21batch/s, batch_accuracy=1, loss=0.000612]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 64.55batch/s, batch_accuracy=1, loss=0.000344]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:01<00:00, 67.39batch/s, batch_accuracy=1, loss=0.000151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6834, Precision: 0.5840, Recall: 0.5714, F1: 0.5776, AUC: 0.7158\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-9f06346eef554574ba2183e26d367dba.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-9f06346eef554574ba2183e26d367dba.vega-embed details,\n",
       "  #altair-viz-9f06346eef554574ba2183e26d367dba.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-9f06346eef554574ba2183e26d367dba\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-9f06346eef554574ba2183e26d367dba\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-9f06346eef554574ba2183e26d367dba\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-45d5ae15ca58278ecbc047e5facbf92d\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-45d5ae15ca58278ecbc047e5facbf92d\": [{\"training_acc\": 0.7890625, \"training_loss\": 0.6943238973617554, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6922269463539124, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6874004006385803, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6870368719100952, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6897390484809875, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6787556409835815, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6730830669403076, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.453125, \"training_loss\": 0.7113938927650452, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6540965437889099, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.661231517791748, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6664267778396606, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6599115133285522, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6421733498573303, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6953744888305664, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6747571229934692, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6511396169662476, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6867654323577881, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6581189632415771, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6919516324996948, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6469686627388, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6886152625083923, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6431832909584045, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6823142766952515, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6556431651115417, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6598001718521118, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6447775363922119, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6755450963973999, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6870290040969849, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6843851804733276, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6672580242156982, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6472560167312622, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6544660329818726, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6439434289932251, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6672242879867554, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6311062574386597, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6529756784439087, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6610427498817444, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.633456289768219, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6281895637512207, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6027368307113647, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6416835188865662, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6588674187660217, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6125340461730957, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6150903105735779, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6410427093505859, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5825361013412476, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.6011524200439453, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6594582200050354, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6364113688468933, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6030089259147644, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6310772895812988, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6187913417816162, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6447431445121765, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.6105495095252991, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5791365504264832, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6189056038856506, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6287252902984619, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6008328199386597, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.585610032081604, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5914895534515381, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5879231095314026, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5136805176734924, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5850358009338379, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.563770592212677, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5250784158706665, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5907076001167297, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5339410901069641, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.8571428571428571, \"training_loss\": 0.4776892364025116, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5846620798110962, \"iteration\": 69, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.39765283465385437, \"iteration\": 70, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.41252779960632324, \"iteration\": 71, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.474730521440506, \"iteration\": 72, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.46466588973999023, \"iteration\": 73, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.39688584208488464, \"iteration\": 74, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4603886008262634, \"iteration\": 75, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4170553684234619, \"iteration\": 76, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.4580218493938446, \"iteration\": 77, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3953808546066284, \"iteration\": 78, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3632059097290039, \"iteration\": 79, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4178556799888611, \"iteration\": 80, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.37792468070983887, \"iteration\": 81, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4552634358406067, \"iteration\": 82, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.40719902515411377, \"iteration\": 83, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.43883058428764343, \"iteration\": 84, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3881256580352783, \"iteration\": 85, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.5284438133239746, \"iteration\": 86, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4553164839744568, \"iteration\": 87, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.38662874698638916, \"iteration\": 88, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4022213816642761, \"iteration\": 89, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3442588448524475, \"iteration\": 90, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.33839869499206543, \"iteration\": 91, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.45129159092903137, \"iteration\": 92, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.40912020206451416, \"iteration\": 93, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.38376984000205994, \"iteration\": 94, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3861468434333801, \"iteration\": 95, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4523783028125763, \"iteration\": 96, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5115809440612793, \"iteration\": 97, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4109426736831665, \"iteration\": 98, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4661334753036499, \"iteration\": 99, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3912529945373535, \"iteration\": 100, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.35624849796295166, \"iteration\": 101, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4257392883300781, \"iteration\": 102, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.39724671840667725, \"iteration\": 103, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.37961089611053467, \"iteration\": 104, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3333249092102051, \"iteration\": 105, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.34218430519104004, \"iteration\": 106, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3791232109069824, \"iteration\": 107, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.3730984628200531, \"iteration\": 108, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.39507412910461426, \"iteration\": 109, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.41351401805877686, \"iteration\": 110, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.42975422739982605, \"iteration\": 111, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.47822481393814087, \"iteration\": 112, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.3694879412651062, \"iteration\": 113, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4618094563484192, \"iteration\": 114, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.37788254022598267, \"iteration\": 115, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3545549809932709, \"iteration\": 116, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.42842715978622437, \"iteration\": 117, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.4644440710544586, \"iteration\": 118, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4176669120788574, \"iteration\": 119, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4590575695037842, \"iteration\": 120, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.41887903213500977, \"iteration\": 121, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.34265828132629395, \"iteration\": 122, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.41908082365989685, \"iteration\": 123, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3450959324836731, \"iteration\": 124, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.37269943952560425, \"iteration\": 125, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.45172053575515747, \"iteration\": 126, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4387282133102417, \"iteration\": 127, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.40796512365341187, \"iteration\": 128, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4324316680431366, \"iteration\": 129, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.37032872438430786, \"iteration\": 130, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3626633882522583, \"iteration\": 131, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5104884505271912, \"iteration\": 132, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3777281939983368, \"iteration\": 133, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.39668959379196167, \"iteration\": 134, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3940768241882324, \"iteration\": 135, \"epoch\": 2}, {\"training_acc\": 0.8961038961038961, \"training_loss\": 0.3582620620727539, \"iteration\": 136, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.27396160364151, \"iteration\": 137, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.24241358041763306, \"iteration\": 138, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2727450132369995, \"iteration\": 139, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.25761550664901733, \"iteration\": 140, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.2014540433883667, \"iteration\": 141, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.251308411359787, \"iteration\": 142, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.27511173486709595, \"iteration\": 143, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21982595324516296, \"iteration\": 144, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.28615233302116394, \"iteration\": 145, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.25065845251083374, \"iteration\": 146, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.23822692036628723, \"iteration\": 147, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.26322102546691895, \"iteration\": 148, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.18433909118175507, \"iteration\": 149, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2230740487575531, \"iteration\": 150, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2285442352294922, \"iteration\": 151, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20970842242240906, \"iteration\": 152, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.20541009306907654, \"iteration\": 153, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20535923540592194, \"iteration\": 154, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.17928096652030945, \"iteration\": 155, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18457669019699097, \"iteration\": 156, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.22458046674728394, \"iteration\": 157, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2551235258579254, \"iteration\": 158, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.15965399146080017, \"iteration\": 159, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.21866706013679504, \"iteration\": 160, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.26668810844421387, \"iteration\": 161, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20577789843082428, \"iteration\": 162, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15288487076759338, \"iteration\": 163, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19282090663909912, \"iteration\": 164, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.18297159671783447, \"iteration\": 165, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.26757925748825073, \"iteration\": 166, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.18854691088199615, \"iteration\": 167, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17593657970428467, \"iteration\": 168, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20117530226707458, \"iteration\": 169, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2523382306098938, \"iteration\": 170, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.24444051086902618, \"iteration\": 171, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22477659583091736, \"iteration\": 172, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.219670832157135, \"iteration\": 173, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19313977658748627, \"iteration\": 174, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21077919006347656, \"iteration\": 175, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1836726814508438, \"iteration\": 176, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3355320990085602, \"iteration\": 177, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22064054012298584, \"iteration\": 178, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.18050698935985565, \"iteration\": 179, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2638434171676636, \"iteration\": 180, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2348261922597885, \"iteration\": 181, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.25183314085006714, \"iteration\": 182, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25424671173095703, \"iteration\": 183, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17044374346733093, \"iteration\": 184, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18820513784885406, \"iteration\": 185, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2628295421600342, \"iteration\": 186, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16437727212905884, \"iteration\": 187, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2161802500486374, \"iteration\": 188, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2079884111881256, \"iteration\": 189, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16839738190174103, \"iteration\": 190, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14778020977973938, \"iteration\": 191, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.28486067056655884, \"iteration\": 192, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.25239259004592896, \"iteration\": 193, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.15651969611644745, \"iteration\": 194, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.27363109588623047, \"iteration\": 195, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23191618919372559, \"iteration\": 196, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21202023327350616, \"iteration\": 197, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2905158996582031, \"iteration\": 198, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21106302738189697, \"iteration\": 199, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.19131022691726685, \"iteration\": 200, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16426736116409302, \"iteration\": 201, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.20674341917037964, \"iteration\": 202, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.246609628200531, \"iteration\": 203, \"epoch\": 3}, {\"training_acc\": 0.922077922077922, \"training_loss\": 0.19167904555797577, \"iteration\": 204, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.08808942884206772, \"iteration\": 205, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10895587503910065, \"iteration\": 206, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08497336506843567, \"iteration\": 207, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08234051614999771, \"iteration\": 208, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08969822525978088, \"iteration\": 209, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11628720164299011, \"iteration\": 210, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1268826425075531, \"iteration\": 211, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.05712243914604187, \"iteration\": 212, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0736503154039383, \"iteration\": 213, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10336621105670929, \"iteration\": 214, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09788519889116287, \"iteration\": 215, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08279545605182648, \"iteration\": 216, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.06051422655582428, \"iteration\": 217, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08158931136131287, \"iteration\": 218, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09886343032121658, \"iteration\": 219, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07324841618537903, \"iteration\": 220, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.07141603529453278, \"iteration\": 221, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.07736647129058838, \"iteration\": 222, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.06328853964805603, \"iteration\": 223, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08378887176513672, \"iteration\": 224, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12695632874965668, \"iteration\": 225, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05783533304929733, \"iteration\": 226, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.07343125343322754, \"iteration\": 227, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.109489306807518, \"iteration\": 228, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10845761746168137, \"iteration\": 229, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09726080298423767, \"iteration\": 230, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10319344699382782, \"iteration\": 231, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11616887897253036, \"iteration\": 232, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.06736452132463455, \"iteration\": 233, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05813567340373993, \"iteration\": 234, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09581442177295685, \"iteration\": 235, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0715954601764679, \"iteration\": 236, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.061916597187519073, \"iteration\": 237, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.05829954147338867, \"iteration\": 238, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09750626236200333, \"iteration\": 239, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07564609497785568, \"iteration\": 240, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.057732127606868744, \"iteration\": 241, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06161178648471832, \"iteration\": 242, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08574787527322769, \"iteration\": 243, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.13003650307655334, \"iteration\": 244, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07743142545223236, \"iteration\": 245, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09701608121395111, \"iteration\": 246, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08710572868585587, \"iteration\": 247, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09026580303907394, \"iteration\": 248, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.056576941162347794, \"iteration\": 249, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07806631922721863, \"iteration\": 250, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.052027251571416855, \"iteration\": 251, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.07308842986822128, \"iteration\": 252, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08468636870384216, \"iteration\": 253, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1088438332080841, \"iteration\": 254, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1438288390636444, \"iteration\": 255, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.06810739636421204, \"iteration\": 256, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06034992262721062, \"iteration\": 257, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1466250717639923, \"iteration\": 258, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05451260507106781, \"iteration\": 259, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07998290657997131, \"iteration\": 260, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.06392298638820648, \"iteration\": 261, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.061507873237133026, \"iteration\": 262, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08430101722478867, \"iteration\": 263, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08337105810642242, \"iteration\": 264, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09790647774934769, \"iteration\": 265, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1326501965522766, \"iteration\": 266, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08300191164016724, \"iteration\": 267, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10009756684303284, \"iteration\": 268, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08200440555810928, \"iteration\": 269, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.042324937880039215, \"iteration\": 270, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08560368418693542, \"iteration\": 271, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.06792352348566055, \"iteration\": 272, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.022609131410717964, \"iteration\": 273, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02136143483221531, \"iteration\": 274, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.019672084599733353, \"iteration\": 275, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03299091011285782, \"iteration\": 276, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023711949586868286, \"iteration\": 277, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01981741562485695, \"iteration\": 278, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018053490668535233, \"iteration\": 279, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04115457832813263, \"iteration\": 280, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.014980238862335682, \"iteration\": 281, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020163586363196373, \"iteration\": 282, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.023978346958756447, \"iteration\": 283, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03921888768672943, \"iteration\": 284, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022907650098204613, \"iteration\": 285, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.016736969351768494, \"iteration\": 286, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01759663037955761, \"iteration\": 287, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.019785473123192787, \"iteration\": 288, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01678609289228916, \"iteration\": 289, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017494868487119675, \"iteration\": 290, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03856648877263069, \"iteration\": 291, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018776560202240944, \"iteration\": 292, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0344783291220665, \"iteration\": 293, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020954735577106476, \"iteration\": 294, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02264556474983692, \"iteration\": 295, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03167646378278732, \"iteration\": 296, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.030299842357635498, \"iteration\": 297, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01319135446101427, \"iteration\": 298, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017184829339385033, \"iteration\": 299, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.024271804839372635, \"iteration\": 300, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.043412815779447556, \"iteration\": 301, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017245259135961533, \"iteration\": 302, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.021209750324487686, \"iteration\": 303, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01172223873436451, \"iteration\": 304, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.036528922617435455, \"iteration\": 305, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018862830474972725, \"iteration\": 306, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.016393035650253296, \"iteration\": 307, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020959986373782158, \"iteration\": 308, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.034446220844984055, \"iteration\": 309, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023561611771583557, \"iteration\": 310, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017209142446517944, \"iteration\": 311, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017435073852539062, \"iteration\": 312, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017839603126049042, \"iteration\": 313, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.021476423367857933, \"iteration\": 314, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01451782789081335, \"iteration\": 315, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01733044721186161, \"iteration\": 316, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05170578509569168, \"iteration\": 317, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017660848796367645, \"iteration\": 318, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0332585871219635, \"iteration\": 319, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03002951107919216, \"iteration\": 320, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.014588113874197006, \"iteration\": 321, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01817489042878151, \"iteration\": 322, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018421530723571777, \"iteration\": 323, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.014262160286307335, \"iteration\": 324, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01970118097960949, \"iteration\": 325, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01963711343705654, \"iteration\": 326, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01883687451481819, \"iteration\": 327, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.013341391459107399, \"iteration\": 328, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.046035826206207275, \"iteration\": 329, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020032308995723724, \"iteration\": 330, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.012418166734278202, \"iteration\": 331, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01899006962776184, \"iteration\": 332, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03949914872646332, \"iteration\": 333, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.015948358923196793, \"iteration\": 334, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.012332393787801266, \"iteration\": 335, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.014372721314430237, \"iteration\": 336, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.015025492757558823, \"iteration\": 337, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03605807200074196, \"iteration\": 338, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.010566428303718567, \"iteration\": 339, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01875862292945385, \"iteration\": 340, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00816186424344778, \"iteration\": 341, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007102183531969786, \"iteration\": 342, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00811430811882019, \"iteration\": 343, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005086374469101429, \"iteration\": 344, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009172944352030754, \"iteration\": 345, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006042080000042915, \"iteration\": 346, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006108487024903297, \"iteration\": 347, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006301145534962416, \"iteration\": 348, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008077232167124748, \"iteration\": 349, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006358115468174219, \"iteration\": 350, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0069728270173072815, \"iteration\": 351, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005299394950270653, \"iteration\": 352, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007083806209266186, \"iteration\": 353, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007113847881555557, \"iteration\": 354, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006304210051894188, \"iteration\": 355, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007080742623656988, \"iteration\": 356, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00794430635869503, \"iteration\": 357, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005501157138496637, \"iteration\": 358, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006417433265596628, \"iteration\": 359, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005178813822567463, \"iteration\": 360, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007530702278017998, \"iteration\": 361, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005218581762164831, \"iteration\": 362, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007703215349465609, \"iteration\": 363, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00737430714070797, \"iteration\": 364, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005320610478520393, \"iteration\": 365, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006528710946440697, \"iteration\": 366, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0058033643290400505, \"iteration\": 367, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006632506847381592, \"iteration\": 368, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0038226412143558264, \"iteration\": 369, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0048743560910224915, \"iteration\": 370, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003491145558655262, \"iteration\": 371, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005306667648255825, \"iteration\": 372, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00906581524759531, \"iteration\": 373, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005214067175984383, \"iteration\": 374, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005017575807869434, \"iteration\": 375, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005459295120090246, \"iteration\": 376, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005387069191783667, \"iteration\": 377, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005164818838238716, \"iteration\": 378, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0037763984873890877, \"iteration\": 379, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0056681158021092415, \"iteration\": 380, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003567202016711235, \"iteration\": 381, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00582907535135746, \"iteration\": 382, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003952935338020325, \"iteration\": 383, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00577385351061821, \"iteration\": 384, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004199338145554066, \"iteration\": 385, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005025374703109264, \"iteration\": 386, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004670154303312302, \"iteration\": 387, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005053451284766197, \"iteration\": 388, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005678738933056593, \"iteration\": 389, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0036646632943302393, \"iteration\": 390, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004422033205628395, \"iteration\": 391, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005450043827295303, \"iteration\": 392, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.015898603945970535, \"iteration\": 393, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004306165501475334, \"iteration\": 394, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005472722463309765, \"iteration\": 395, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005145850591361523, \"iteration\": 396, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0037558572366833687, \"iteration\": 397, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004484524019062519, \"iteration\": 398, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004543274641036987, \"iteration\": 399, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004250579513609409, \"iteration\": 400, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005163317080587149, \"iteration\": 401, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005201163701713085, \"iteration\": 402, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004588070325553417, \"iteration\": 403, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.023459430783987045, \"iteration\": 404, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004819747991859913, \"iteration\": 405, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004150077234953642, \"iteration\": 406, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003938121255487204, \"iteration\": 407, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.018606653437018394, \"iteration\": 408, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0022149207070469856, \"iteration\": 409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003276711329817772, \"iteration\": 410, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0030998769216239452, \"iteration\": 411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0033237687312066555, \"iteration\": 412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003911825828254223, \"iteration\": 413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003119097091257572, \"iteration\": 414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004888641647994518, \"iteration\": 415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0026302237529307604, \"iteration\": 416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002158013405278325, \"iteration\": 417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0024988106451928616, \"iteration\": 418, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0024828799068927765, \"iteration\": 419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003033710177987814, \"iteration\": 420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0028804840985685587, \"iteration\": 421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0025607245042920113, \"iteration\": 422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0027940396685153246, \"iteration\": 423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003237109864130616, \"iteration\": 424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002395660150796175, \"iteration\": 425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0023822798393666744, \"iteration\": 426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0022421430330723524, \"iteration\": 427, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002383294515311718, \"iteration\": 428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0027174674905836582, \"iteration\": 429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0027383798733353615, \"iteration\": 430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002703377977013588, \"iteration\": 431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0021850443445146084, \"iteration\": 432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020573732908815145, \"iteration\": 433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001965570729225874, \"iteration\": 434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0022357399575412273, \"iteration\": 435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0024011938367038965, \"iteration\": 436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001992996782064438, \"iteration\": 437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017121137352660298, \"iteration\": 438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0021061208099126816, \"iteration\": 439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002211325103417039, \"iteration\": 440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016798445722088218, \"iteration\": 441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002290177857503295, \"iteration\": 442, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020793015137314796, \"iteration\": 443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018440275453031063, \"iteration\": 444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002084130886942148, \"iteration\": 445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002013800200074911, \"iteration\": 446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020683526527136564, \"iteration\": 447, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017941236728802323, \"iteration\": 448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00207397248595953, \"iteration\": 449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018903695745393634, \"iteration\": 450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014170556096360087, \"iteration\": 451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017138238763436675, \"iteration\": 452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013815837446600199, \"iteration\": 453, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001609954982995987, \"iteration\": 454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002073713345453143, \"iteration\": 455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0019154698820784688, \"iteration\": 456, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0024456677492707968, \"iteration\": 457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020311730913817883, \"iteration\": 458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001563004101626575, \"iteration\": 459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013146500568836927, \"iteration\": 460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0019046562956646085, \"iteration\": 461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001447839429602027, \"iteration\": 462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018833992071449757, \"iteration\": 463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0019054675940424204, \"iteration\": 464, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001752503216266632, \"iteration\": 465, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001559312455356121, \"iteration\": 466, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020640799775719643, \"iteration\": 467, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002067748922854662, \"iteration\": 468, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015191836282610893, \"iteration\": 469, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001531523885205388, \"iteration\": 470, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014099935069680214, \"iteration\": 471, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015123751945793629, \"iteration\": 472, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014972980134189129, \"iteration\": 473, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001513893948867917, \"iteration\": 474, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014098654501140118, \"iteration\": 475, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012696110643446445, \"iteration\": 476, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013818780425935984, \"iteration\": 477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016523050144314766, \"iteration\": 478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010226144222542644, \"iteration\": 479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0014324943767860532, \"iteration\": 480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010096662444993854, \"iteration\": 481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010096807964146137, \"iteration\": 482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009505184716545045, \"iteration\": 483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012294348562136292, \"iteration\": 484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010448399698361754, \"iteration\": 485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008725130464881659, \"iteration\": 486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009530176757834852, \"iteration\": 487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008726761443540454, \"iteration\": 488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009704442927613854, \"iteration\": 489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012376385275274515, \"iteration\": 490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009861139114946127, \"iteration\": 491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012225693790242076, \"iteration\": 492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008718837052583694, \"iteration\": 493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008721521589905024, \"iteration\": 494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0011044676648452878, \"iteration\": 495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009737302316352725, \"iteration\": 496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008069398463703692, \"iteration\": 497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009739208617247641, \"iteration\": 498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000857815146446228, \"iteration\": 499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010210114996880293, \"iteration\": 500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008371982257813215, \"iteration\": 501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009870780631899834, \"iteration\": 502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008581183501519263, \"iteration\": 503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010169806191697717, \"iteration\": 504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008933551143854856, \"iteration\": 505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008458631346002221, \"iteration\": 506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009296989301219583, \"iteration\": 507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007136940839700401, \"iteration\": 508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008012364851310849, \"iteration\": 509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000931476941332221, \"iteration\": 510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0011957454262301326, \"iteration\": 511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009526708163321018, \"iteration\": 512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008676012512296438, \"iteration\": 513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008283581701107323, \"iteration\": 514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008198345312848687, \"iteration\": 515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008366728434339166, \"iteration\": 516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008824113174341619, \"iteration\": 517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007940086652524769, \"iteration\": 518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009276632918044925, \"iteration\": 519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008358928607776761, \"iteration\": 520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007055799942463636, \"iteration\": 521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00080664805136621, \"iteration\": 522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006852836813777685, \"iteration\": 523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007505773683078587, \"iteration\": 524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000666468869894743, \"iteration\": 525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008174602990038693, \"iteration\": 526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006826408789493144, \"iteration\": 527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008007282740436494, \"iteration\": 528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007811528630554676, \"iteration\": 529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006869768258184195, \"iteration\": 530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008333700243383646, \"iteration\": 531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007124269031919539, \"iteration\": 532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008320058695971966, \"iteration\": 533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006522804615087807, \"iteration\": 534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008106732275336981, \"iteration\": 535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007151684258133173, \"iteration\": 536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005050176405347884, \"iteration\": 537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006190069252625108, \"iteration\": 538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007535868207924068, \"iteration\": 539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000728811020962894, \"iteration\": 540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007430211408063769, \"iteration\": 541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000741036725230515, \"iteration\": 542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006103893392719328, \"iteration\": 543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006118268938735127, \"iteration\": 544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006344326538965106, \"iteration\": 545, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004606797592714429, \"iteration\": 546, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005632323445752263, \"iteration\": 547, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005028402665629983, \"iteration\": 548, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005829597939737141, \"iteration\": 549, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005336939357221127, \"iteration\": 550, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006016779225319624, \"iteration\": 551, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004605501890182495, \"iteration\": 552, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005529922200366855, \"iteration\": 553, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00039441988337785006, \"iteration\": 554, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005794417229481041, \"iteration\": 555, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004140409582760185, \"iteration\": 556, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000490799080580473, \"iteration\": 557, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005157273262739182, \"iteration\": 558, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005163425812497735, \"iteration\": 559, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006439516437239945, \"iteration\": 560, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005383298266679049, \"iteration\": 561, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005385308759287, \"iteration\": 562, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005425626877695322, \"iteration\": 563, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00041170616168528795, \"iteration\": 564, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004040210915263742, \"iteration\": 565, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005435948842205107, \"iteration\": 566, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005334686720743775, \"iteration\": 567, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006089371745474637, \"iteration\": 568, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006098122685216367, \"iteration\": 569, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000508861499838531, \"iteration\": 570, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004360144375823438, \"iteration\": 571, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00044261079165153205, \"iteration\": 572, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00045342871453613043, \"iteration\": 573, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005447834264487028, \"iteration\": 574, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00040932773845270276, \"iteration\": 575, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004364779742900282, \"iteration\": 576, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004819483729079366, \"iteration\": 577, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003327165904920548, \"iteration\": 578, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004655670782085508, \"iteration\": 579, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00038134161150082946, \"iteration\": 580, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003921144816558808, \"iteration\": 581, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004225859302096069, \"iteration\": 582, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00046497691073454916, \"iteration\": 583, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003873530949931592, \"iteration\": 584, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00041493761818856, \"iteration\": 585, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000443110300693661, \"iteration\": 586, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004004554357379675, \"iteration\": 587, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004368934314697981, \"iteration\": 588, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00042677135206758976, \"iteration\": 589, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00043129114783369005, \"iteration\": 590, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00039068932528607547, \"iteration\": 591, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003205872781109065, \"iteration\": 592, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000501342467032373, \"iteration\": 593, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000569207826629281, \"iteration\": 594, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003838805132545531, \"iteration\": 595, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00036267671384848654, \"iteration\": 596, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003334220964461565, \"iteration\": 597, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00035727181239053607, \"iteration\": 598, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00035552552435547113, \"iteration\": 599, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003291653993073851, \"iteration\": 600, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00039567521889694035, \"iteration\": 601, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003833700902760029, \"iteration\": 602, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00035159950493834913, \"iteration\": 603, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003346383455209434, \"iteration\": 604, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003379852860234678, \"iteration\": 605, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003786322777159512, \"iteration\": 606, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004165760474279523, \"iteration\": 607, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003004629397764802, \"iteration\": 608, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003634247987065464, \"iteration\": 609, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00032272288808599114, \"iteration\": 610, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003706752322614193, \"iteration\": 611, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00034401382436044514, \"iteration\": 612, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003410888311918825, \"iteration\": 613, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00030907694599591196, \"iteration\": 614, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00029212539084255695, \"iteration\": 615, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00038438118644990027, \"iteration\": 616, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027106545167043805, \"iteration\": 617, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003625461831688881, \"iteration\": 618, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000311841577058658, \"iteration\": 619, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00028443936025723815, \"iteration\": 620, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00029999742400832474, \"iteration\": 621, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00034074397990480065, \"iteration\": 622, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002848270523827523, \"iteration\": 623, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027110957307741046, \"iteration\": 624, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00033915002131834626, \"iteration\": 625, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022977472690399736, \"iteration\": 626, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002815601765178144, \"iteration\": 627, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00031550100538879633, \"iteration\": 628, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003215680771972984, \"iteration\": 629, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020132253121118993, \"iteration\": 630, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021936379198450595, \"iteration\": 631, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021332589676603675, \"iteration\": 632, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000249928270932287, \"iteration\": 633, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002711935667321086, \"iteration\": 634, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023338136088568717, \"iteration\": 635, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002514538646209985, \"iteration\": 636, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023872125893831253, \"iteration\": 637, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024047466285992414, \"iteration\": 638, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022645361605100334, \"iteration\": 639, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021660822676494718, \"iteration\": 640, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001961446541827172, \"iteration\": 641, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021323052351363003, \"iteration\": 642, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021689280401915312, \"iteration\": 643, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021619202743750066, \"iteration\": 644, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000197405373910442, \"iteration\": 645, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002516537206247449, \"iteration\": 646, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002608679933473468, \"iteration\": 647, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024773526820354164, \"iteration\": 648, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023550435435026884, \"iteration\": 649, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022347926278598607, \"iteration\": 650, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002302567008882761, \"iteration\": 651, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016707213944755495, \"iteration\": 652, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020115223014727235, \"iteration\": 653, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002758879563771188, \"iteration\": 654, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000201183240278624, \"iteration\": 655, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023516977671533823, \"iteration\": 656, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022968051780480891, \"iteration\": 657, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001889533014036715, \"iteration\": 658, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021010494674555957, \"iteration\": 659, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016296221292577684, \"iteration\": 660, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021981364989187568, \"iteration\": 661, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001936791668413207, \"iteration\": 662, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021545850904658437, \"iteration\": 663, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019824641640298069, \"iteration\": 664, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020441695232875645, \"iteration\": 665, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017909565940499306, \"iteration\": 666, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017881215899251401, \"iteration\": 667, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024921903968788683, \"iteration\": 668, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001649127807468176, \"iteration\": 669, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017561815911903977, \"iteration\": 670, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000169489998370409, \"iteration\": 671, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018219766207039356, \"iteration\": 672, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002000901149585843, \"iteration\": 673, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019377419084776193, \"iteration\": 674, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015773960330989212, \"iteration\": 675, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001796998258214444, \"iteration\": 676, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014618897694163024, \"iteration\": 677, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001730969233904034, \"iteration\": 678, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018704913964029402, \"iteration\": 679, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001514078612672165, \"iteration\": 680, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Prepare data...\")\n",
    "test_nn_chars = encode_data(test_raw, chars_encoder)\n",
    "\n",
    "print(\"Train model\")\n",
    "models_dir = Path('models/gb')\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "model_nn_chars = NeuralNetwork(\n",
    "    input_size=len(chars_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn_chars.pt').exists() and USE_CACHE:\n",
    "    model_nn_chars = load_model(model_nn_chars, models_dir, 'model_nn_chars')\n",
    "else:\n",
    "    train_nn_chars = encode_data(train_raw, chars_encoder)\n",
    "    dataloader = DataLoader(train_nn_chars, batch_size=128, shuffle=True)\n",
    "\n",
    "    model_nn_chars.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn_chars, models_dir, \"model_nn_chars\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = torch.stack([test[0] for test in test_nn_chars]).to(model_nn_chars.device)\n",
    "    y_test = torch.stack([test[1] for test in test_nn_chars]).to(model_nn_chars.device)\n",
    "    y_pred = model_nn_chars.predict(X_test)\n",
    "    logits = model_nn_chars.forward(X_test)\n",
    "\n",
    "result_nn_chars = evaluate(y_test.cpu(), y_pred.cpu(), logits.cpu())\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn_chars, train_config, dataloader)\n",
    "\n",
    "model_nn_words.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:33<00:00,  2.06batch/s, batch_accuracy=0.571, loss=143]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.09batch/s, batch_accuracy=0.597, loss=132]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.08batch/s, batch_accuracy=0.649, loss=110]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.10batch/s, batch_accuracy=0.545, loss=139]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.10batch/s, batch_accuracy=0.571, loss=120]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:31<00:00,  2.13batch/s, batch_accuracy=0.506, loss=146]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.12batch/s, batch_accuracy=0.701, loss=103]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:31<00:00,  2.13batch/s, batch_accuracy=0.623, loss=102]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.11batch/s, batch_accuracy=1, loss=129]    \n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.12batch/s, batch_accuracy=1, loss=111]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6250, Precision: 0.5072, Recall: 0.3565, F1: 0.4187, AUC: 0.6305\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prepare data encoder...\")\n",
    "rnn_words_encoder = PositionalEncoder()\n",
    "rnn_words_encoder.fit(train_raw.texts)\n",
    "\n",
    "train_dataloader = DataLoader(train_raw, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_raw, batch_size=128, shuffle=False)\n",
    "\n",
    "test_inputs = rnn_words_encoder.transform(test_raw.texts)\n",
    "\n",
    "# Prepare baseline config\n",
    "train_config = TrainConfig(\n",
    "    optimizer_params = {'lr': 0.01},\n",
    "    num_epochs       = 10,\n",
    "    early_stop       = False,\n",
    "    violation_limit  = 5\n",
    ")\n",
    "\n",
    "# Train baseline model\n",
    "model_lstm_words = RNNClassifier(\n",
    "    rnn_network         = nn.LSTM,\n",
    "    word_embedding_dim  = 32,\n",
    "    hidden_dim          = 64,\n",
    "    bidirectional       = False,\n",
    "    dropout             = 0,\n",
    "    encoder             = rnn_words_encoder,\n",
    "    device              = 'cuda'\n",
    ")\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "\n",
    "if (models_dir / 'model_lstm_words.pt').exists() and USE_CACHE:\n",
    "    model_lstm_words = load_model(model_lstm_words, 'model_lstm_words')\n",
    "else:\n",
    "    model_lstm_words.fit(train_dataloader, train_config, no_progress_bar=False)\n",
    "    save_model(model_lstm_words, models_dir, \"model_lstm_words\")\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    model_lstm_words.device = \"cpu\"\n",
    "    model_lstm_words.cpu()\n",
    "\n",
    "    pred_LSTM_words = []\n",
    "    logits_LSTM_words = []\n",
    "\n",
    "    for _, _, raw_inputs, raw_targets in test_dataloader:\n",
    "        batch_encoder = PositionalEncoder(vocabulary=rnn_words_encoder.vocabulary)\n",
    "        test_inputs = batch_encoder.fit_transform(raw_inputs).cpu()\n",
    "        test_targets = torch.as_tensor(raw_targets, dtype=torch.float).cpu()  # nn.CrossEntropyLoss() require target to be float\n",
    "\n",
    "        pred_LSTM_words.append(model_lstm_words.predict(test_inputs))\n",
    "        logits_LSTM_words.append(model_lstm_words.forward(test_inputs))\n",
    "\n",
    "pred_LSTM_words = torch.concat(pred_LSTM_words).numpy()\n",
    "logits_LSTM_words = torch.concat(logits_LSTM_words).numpy()\n",
    "\n",
    "\n",
    "model_lstm_words_result = evaluate(test_raw.labels, pred_LSTM_words, logits_LSTM_words)\n",
    "# print(model_lstm_words_result)\n",
    "\n",
    "np.save(models_dir / 'model_lstm_words_results.npy', model_lstm_words_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.07batch/s, batch_accuracy=0.377, loss=126]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.07batch/s, batch_accuracy=0.532, loss=173]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.09batch/s, batch_accuracy=0.416, loss=138]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.09batch/s, batch_accuracy=0.455, loss=131]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:31<00:00,  2.13batch/s, batch_accuracy=0.325, loss=87.4]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:32<00:00,  2.12batch/s, batch_accuracy=0.455, loss=132]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:31<00:00,  2.14batch/s, batch_accuracy=0.494, loss=131]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:31<00:00,  2.15batch/s, batch_accuracy=0.338, loss=77.1]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:31<00:00,  2.14batch/s, batch_accuracy=0.468, loss=121]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:31<00:00,  2.14batch/s, batch_accuracy=0.558, loss=104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4411, Precision: 0.3923, Recall: 0.8660, F1: 0.5400, AUC: 0.5439\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prepare data encoder...\")\n",
    "rnn_chars_encoder = PositionalEncoder(tokenizer=chars_encoder.build_analyzer())\n",
    "rnn_chars_encoder.fit(train_raw.texts)\n",
    "\n",
    "train_dataloader = DataLoader(train_raw, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_raw, batch_size=128, shuffle=False)\n",
    "\n",
    "test_inputs = rnn_chars_encoder.transform(test_raw.texts)\n",
    "\n",
    "# Prepare baseline config\n",
    "train_config = TrainConfig(\n",
    "    optimizer_params = {'lr': 0.01},\n",
    "    num_epochs       = 10,\n",
    "    early_stop       = False,\n",
    "    violation_limit  = 5\n",
    ")\n",
    "\n",
    "# Train baseline model\n",
    "model_lstm_chars = RNNClassifier(\n",
    "    rnn_network         = nn.LSTM,\n",
    "    word_embedding_dim  = 32,\n",
    "    hidden_dim          = 64,\n",
    "    bidirectional       = False,\n",
    "    dropout             = 0,\n",
    "    encoder             = rnn_chars_encoder,\n",
    "    device              = 'cuda'\n",
    ")\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "if (models_dir / 'model_lstm_chars.pt').exists() and USE_CACHE:\n",
    "    model_lstm_chars = load_model(model_lstm_chars, 'model_lstm_chars')\n",
    "else:\n",
    "    model_lstm_chars.fit(train_dataloader, train_config, no_progress_bar=False)\n",
    "    save_model(model_lstm_chars, models_dir, \"model_lstm_chars\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_lstm_chars.device = \"cpu\"\n",
    "    model_lstm_chars.cpu()\n",
    "\n",
    "    pred_LSTM_chars = []\n",
    "    logits_LSTM_chars = []\n",
    "\n",
    "    for _, _, raw_inputs, raw_targets in test_dataloader:\n",
    "        batch_encoder = PositionalEncoder(vocabulary=rnn_chars_encoder.vocabulary)\n",
    "        test_inputs = batch_encoder.fit_transform(raw_inputs).cpu()\n",
    "        test_targets = torch.as_tensor(raw_targets, dtype=torch.float).cpu()  # nn.CrossEntropyLoss() require target to be float\n",
    "\n",
    "\n",
    "        pred_LSTM_chars.append(model_lstm_chars.predict(test_inputs))\n",
    "        logits_LSTM_chars.append(model_lstm_chars.forward(test_inputs))\n",
    "\n",
    "pred_LSTM_chars = torch.concat(pred_LSTM_chars).numpy()\n",
    "logits_LSTM_chars = torch.concat(logits_LSTM_chars).numpy()\n",
    "\n",
    "model_lstm_chars_result = evaluate(test_raw.labels, pred_LSTM_chars, logits_LSTM_chars)\n",
    "# print(model_lstm_chars_result)\n",
    "\n",
    "np.save(models_dir / 'model_lstm_chars_results.npy', model_lstm_chars_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other classifiers from sklearn\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "\"Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train & test set\n",
    "X_train_skl_words = words_encoder.transform(train_raw.texts)\n",
    "X_test_skl_words = words_encoder.transform(test_raw.texts)\n",
    "\n",
    "X_train_skl_chars = chars_encoder.transform(train_raw.texts)\n",
    "X_test_skl_chars = chars_encoder.transform(test_raw.texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC\n",
    "Effective in high dimensional spaces.\n",
    "\n",
    "Still effective in cases where number of dimensions is greater than the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC with TfIdf did good on balanced English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.6930, Precision: 0.6190, Recall: 0.4930, F1: 0.5489, AUC: 0.7136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# LinearSVC, word\n",
    "\n",
    "\n",
    "print(\"Fit model\")\n",
    "base_svc = LinearSVC()\n",
    "model_LinearSVC_words = CalibratedClassifierCV(estimator=base_svc, cv=5)\n",
    "model_LinearSVC_words.fit(X_train_skl_words, train_raw.labels)\n",
    "\n",
    "pred_LinearSVC_words = model_LinearSVC_words.predict(X_test_skl_words)\n",
    "logits_linearSVC_words = model_LinearSVC_words.predict_proba(X_test_skl_words)\n",
    "\n",
    "result_linearSVC_words =  evaluate(test_raw.labels, pred_LinearSVC_words, logits_linearSVC_words[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7136, Precision: 0.6524, Recall: 0.5221, F1: 0.5801, AUC: 0.7407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "print(\"Fit model\")\n",
    "base_svc = LinearSVC()\n",
    "model_LinearSVC_chars = CalibratedClassifierCV(estimator=base_svc, cv=5)\n",
    "model_LinearSVC_chars.fit(X_train_skl_chars, train_raw.labels)\n",
    "\n",
    "pred_LinearSVC_chars = model_LinearSVC_chars.predict(X_test_skl_chars)\n",
    "logits_linearSVC_chars = model_LinearSVC_chars.predict_proba(X_test_skl_chars)\n",
    "\n",
    "result_linearSVC_chars =  evaluate(test_raw.labels, pred_LinearSVC_chars, logits_linearSVC_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7016, Precision: 0.6511, Recall: 0.4576, F1: 0.5375, AUC: 0.7307\n"
     ]
    }
   ],
   "source": [
    "# Word features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_logreg_words = LogisticRegression()\n",
    "model_logreg_words.fit(X_train_skl_words, train_raw.labels)\n",
    "\n",
    "pred_logreg_words = model_logreg_words.predict(X_test_skl_words)\n",
    "logits_logreg_words = model_logreg_words.predict_proba(X_test_skl_words)\n",
    "\n",
    "result_linearlogreg_words =  evaluate(test_raw.labels, pred_logreg_words, logits_logreg_words[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7126, Precision: 0.6616, Recall: 0.4943, F1: 0.5658, AUC: 0.7570\n"
     ]
    }
   ],
   "source": [
    "# char features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_logreg_chars = LogisticRegression()\n",
    "model_logreg_chars.fit(X_train_skl_chars, train_raw.labels)\n",
    "\n",
    "pred_logreg_chars = model_logreg_chars.predict(X_test_skl_chars)\n",
    "logits_logreg_chars = model_logreg_chars.predict_proba(X_test_skl_chars)\n",
    "\n",
    "result_linearlogreg_chars =  evaluate(test_raw.labels, pred_logreg_chars, logits_logreg_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier\n",
    "SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "\n",
    "SGD is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7031, Precision: 0.6734, Recall: 0.4197, F1: 0.5171, AUC: 0.7286\n"
     ]
    }
   ],
   "source": [
    "# word features\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_sgd_words = SGDClassifier(loss='log_loss')\n",
    "model_sgd_words.fit(X_train_skl_words, train_raw.labels)\n",
    "\n",
    "pred_sgd_words = model_sgd_words.predict(X_test_skl_words)\n",
    "logits_sgd_words = model_sgd_words.predict_proba(X_test_skl_words)\n",
    "\n",
    "result_linearsgd_words =  evaluate(test_raw.labels, pred_sgd_words, logits_sgd_words[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7150, Precision: 0.6576, Recall: 0.5171, F1: 0.5789, AUC: 0.7564\n"
     ]
    }
   ],
   "source": [
    "# chars features\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_sgd_chars = SGDClassifier(loss='log_loss')\n",
    "model_sgd_chars.fit(X_train_skl_chars, train_raw.labels)\n",
    "\n",
    "pred_sgd_chars = model_sgd_chars.predict(X_test_skl_chars)\n",
    "logits_sgd_chars = model_sgd_chars.predict_proba(X_test_skl_chars)\n",
    "\n",
    "result_linearsgd_chars =  evaluate(test_raw.labels, pred_sgd_chars, logits_sgd_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Overall bad performance, not worth pursuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.5972, Precision: 0.4681, Recall: 0.4640, F1: 0.4660, AUC: 0.5723\n"
     ]
    }
   ],
   "source": [
    "# words features\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_gnb_words = GaussianNB()\n",
    "model_gnb_words.fit(X_train_skl_words.toarray(), train_raw.labels)\n",
    "\n",
    "pred_gnb_words = model_gnb_words.predict(X_test_skl_words.toarray())\n",
    "logits_gnb_words = model_gnb_words.predict_proba(X_test_skl_words.toarray())\n",
    "\n",
    "result_lineargnb_words =  evaluate(test_raw.labels, pred_gnb_words, logits_gnb_words[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.6810, Precision: 0.5767, Recall: 0.5942, F1: 0.5853, AUC: 0.6972\n"
     ]
    }
   ],
   "source": [
    "# chars features\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_gnb_chars = GaussianNB()\n",
    "model_gnb_chars.fit(X_train_skl_chars.toarray(), train_raw.labels)\n",
    "\n",
    "pred_gnb_chars = model_gnb_chars.predict(X_test_skl_chars.toarray())\n",
    "logits_gnb_chars = model_gnb_chars.predict_proba(X_test_skl_chars.toarray())\n",
    "\n",
    "result_lineargnb_chars =  evaluate(test_raw.labels, pred_gnb_chars, logits_gnb_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data for xgboost\n",
    "train_dmat_words = xgb.DMatrix(X_train_skl_words, pd.array(train_raw.labels).astype('category'))\n",
    "test_dmat_words = xgb.DMatrix(X_test_skl_words, pd.array(test_raw.labels).astype('category'))\n",
    "\n",
    "train_dmat_chars = xgb.DMatrix(X_train_skl_chars, pd.array(train_raw.labels).astype('category'))\n",
    "test_dmat_chars = xgb.DMatrix(X_test_skl_chars, pd.array(test_raw.labels).astype('category'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.62021\n",
      "[100]\ttrain-logloss:0.19892\n",
      "[200]\ttrain-logloss:0.10027\n",
      "[300]\ttrain-logloss:0.05127\n",
      "[400]\ttrain-logloss:0.02922\n",
      "[500]\ttrain-logloss:0.01756\n",
      "[600]\ttrain-logloss:0.01152\n",
      "[700]\ttrain-logloss:0.00825\n",
      "[800]\ttrain-logloss:0.00615\n",
      "[900]\ttrain-logloss:0.00485\n",
      "[1000]\ttrain-logloss:0.00380\n",
      "[1100]\ttrain-logloss:0.00309\n",
      "[1200]\ttrain-logloss:0.00252\n",
      "[1300]\ttrain-logloss:0.00213\n",
      "[1400]\ttrain-logloss:0.00183\n",
      "[1500]\ttrain-logloss:0.00160\n",
      "[1600]\ttrain-logloss:0.00143\n",
      "[1700]\ttrain-logloss:0.00129\n",
      "[1800]\ttrain-logloss:0.00119\n",
      "[1900]\ttrain-logloss:0.00110\n",
      "[1999]\ttrain-logloss:0.00104\n",
      "Accuracy: 0.7031, Precision: 0.6290, Recall: 0.5272, F1: 0.5736, AUC: 0.7364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"objective\": \"binary:logistic\",  # there is also binary:hinge but hinge does not output probability\n",
    "    \"tree_method\": \"hist\",  # default to hist\n",
    "    \"device\": \"cuda\",\n",
    "\n",
    "    # Params for tree booster\n",
    "    \"eta\": 0.3,\n",
    "    \"gamma\": 0.0,  # Min loss achieved to split the tree\n",
    "    \"max_depth\": 6,\n",
    "    \"reg_alpha\": 0,\n",
    "    \"reg_lambda\": 1,\n",
    "\n",
    "}\n",
    "evals_words = [(train_dmat_words, \"train\")]\n",
    "iterations = 2000\n",
    "\n",
    "model_xgb_words = xgb.train(\n",
    "    params = params,\n",
    "    dtrain = train_dmat_words,\n",
    "    num_boost_round = iterations,\n",
    "    evals = evals_words,\n",
    "    verbose_eval = 100\n",
    ")\n",
    "\n",
    "pred_xgb_words_probs = model_xgb_words.predict(test_dmat_words)\n",
    "result_xgb_words = evaluate(test_raw.labels, pred_xgb_words_probs > 0.5, pred_xgb_words_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[20:00:12] /workspace/src/tree/updater_gpu_hist.cu:781: Exception in gpu_hist: [20:00:12] /workspace/src/c_api/../data/../common/device_helpers.cuh:431: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 1327693824\n- Requested memory: 2536918976\n\nStack trace:\n  [bt] (0) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x77f79a) [0x7f0c63ba179a]\n  [bt] (1) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x783994) [0x7f0c63ba5994]\n  [bt] (2) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb3737b) [0x7f0c63f5937b]\n  [bt] (3) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb3d674) [0x7f0c63f5f674]\n  [bt] (4) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb425c2) [0x7f0c63f645c2]\n  [bt] (5) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb436ab) [0x7f0c63f656ab]\n  [bt] (6) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb47453) [0x7f0c63f69453]\n  [bt] (7) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb48301) [0x7f0c63f6a301]\n  [bt] (8) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7f0c63882c79]\n\n\n\nStack trace:\n  [bt] (0) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7f0c63f49f2a]\n  [bt] (1) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb485c9) [0x7f0c63f6a5c9]\n  [bt] (2) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7f0c63882c79]\n  [bt] (3) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7f0c6388376c]\n  [bt] (4) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7f0c638e74f7]\n  [bt] (5) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7f0c63583ef0]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7f0f4406ce2e]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7f0f44069493]\n  [bt] (8) /home/hapham/.pyenv/versions/3.11.5/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0x13cd6) [0x7f0f446bccd6]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m evals_chars \u001b[38;5;241m=\u001b[39m [(train_dmat_chars, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     30\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[0;32m---> 32\u001b[0m model_xgb_chars \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dmat_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mevals_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m pred_xgb_chars_probs \u001b[38;5;241m=\u001b[39m model_xgb_chars\u001b[38;5;241m.\u001b[39mpredict(test_dmat_chars)\n\u001b[1;32m     41\u001b[0m result_xgb_chars \u001b[38;5;241m=\u001b[39m evaluate(test_raw\u001b[38;5;241m.\u001b[39mlabels, pred_xgb_chars_probs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, pred_xgb_chars_probs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2050\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2056\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/core.py:282\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [20:00:12] /workspace/src/tree/updater_gpu_hist.cu:781: Exception in gpu_hist: [20:00:12] /workspace/src/c_api/../data/../common/device_helpers.cuh:431: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 1327693824\n- Requested memory: 2536918976\n\nStack trace:\n  [bt] (0) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x77f79a) [0x7f0c63ba179a]\n  [bt] (1) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x783994) [0x7f0c63ba5994]\n  [bt] (2) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb3737b) [0x7f0c63f5937b]\n  [bt] (3) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb3d674) [0x7f0c63f5f674]\n  [bt] (4) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb425c2) [0x7f0c63f645c2]\n  [bt] (5) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb436ab) [0x7f0c63f656ab]\n  [bt] (6) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb47453) [0x7f0c63f69453]\n  [bt] (7) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb48301) [0x7f0c63f6a301]\n  [bt] (8) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7f0c63882c79]\n\n\n\nStack trace:\n  [bt] (0) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb27f2a) [0x7f0c63f49f2a]\n  [bt] (1) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0xb485c9) [0x7f0c63f6a5c9]\n  [bt] (2) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x460c79) [0x7f0c63882c79]\n  [bt] (3) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x46176c) [0x7f0c6388376c]\n  [bt] (4) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(+0x4c54f7) [0x7f0c638e74f7]\n  [bt] (5) /home/hapham/.pyenv/versions/3.11.5/envs/power/lib/python3.11/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x70) [0x7f0c63583ef0]\n  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7f0f4406ce2e]\n  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7f0f44069493]\n  [bt] (8) /home/hapham/.pyenv/versions/3.11.5/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0x13cd6) [0x7f0f446bccd6]\n\n"
     ]
    }
   ],
   "source": [
    "# Can use only half of the original max features\n",
    "xgb_chars_encoder = TfidfVectorizer(max_features=20000, analyzer=\"char\", ngram_range=(3,5), use_idf=True, sublinear_tf=True)\n",
    "xgb_chars_encoder.fit(train_raw.texts)\n",
    "\n",
    "# Prepare train & test set\n",
    "X_train_xgb_chars = xgb_chars_encoder.transform(train_raw.texts)\n",
    "X_test_xgb_chars = xgb_chars_encoder.transform(test_raw.texts)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "train_dmat_chars = xgb.DMatrix(X_train_xgb_chars, pd.array(train_raw.labels).astype('category'))\n",
    "test_dmat_chars = xgb.DMatrix(X_test_xgb_chars, pd.array(test_raw.labels).astype('category'))\n",
    "\n",
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"device\": \"cpu\",\n",
    "    \"objective\": \"binary:logistic\",  # there is also binary:hinge but hinge does not output probability\n",
    "    \"tree_method\": \"hist\",  # default to hist\n",
    "    \"device\": \"cuda\",\n",
    "\n",
    "    # Params for tree booster\n",
    "    \"eta\": 0.3,\n",
    "    \"gamma\": 0.0,  # Min loss achieved to split the tree\n",
    "    \"max_depth\": 6,\n",
    "    \"reg_alpha\": 0,\n",
    "    \"reg_lambda\": 1,\n",
    "\n",
    "}\n",
    "evals_chars = [(train_dmat_chars, \"train\")]\n",
    "iterations = 2000\n",
    "\n",
    "model_xgb_chars = xgb.train(\n",
    "    params = params,\n",
    "    dtrain = train_dmat_chars,\n",
    "    num_boost_round = iterations,\n",
    "    evals = evals_chars,\n",
    "    verbose_eval = 100\n",
    ")\n",
    "\n",
    "pred_xgb_chars_probs = model_xgb_chars.predict(test_dmat_chars)\n",
    "result_xgb_chars = evaluate(test_raw.labels, pred_xgb_chars_probs > 0.5, pred_xgb_chars_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-identification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

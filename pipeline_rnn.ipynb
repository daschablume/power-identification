{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h4tdvqsOEPZ",
        "outputId": "c712a488-b2f1-4721-d55b-4c4e4b3a05fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "NVIDIA GeForce RTX 3050 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "from utils import PositionalEncoder, load_data, split_data\n",
        "from models import TrainConfig, RNNClassifier, evaluate_rnn_model, save_model, load_model, plot_results\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(\"Device: cuda\")\n",
        "        print(torch.cuda.get_device_name(i))\n",
        "else:\n",
        "    print(\"Device: cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjuDVvbUOSt0",
        "outputId": "cd648194-87ec-4fbf-c73c-7f0744aaa5b0"
      },
      "outputs": [],
      "source": [
        "# Use this if running on Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# base_dir = \"/content/drive/MyDrive/data/power\"\n",
        "\n",
        "# Use this if running locally\n",
        "base_dir = \"data/train/power\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAoCz_MqOEPc",
        "outputId": "1ef01292-4f44-41e6-8e36-59f3b672860b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load power-gb-train.tsv...\n",
            "Load power-ua-train.tsv...\n"
          ]
        }
      ],
      "source": [
        "# Load packages\n",
        "file_list = [\n",
        "    'power-gb-train.tsv',\n",
        "    'power-ua-train.tsv',\n",
        "    # 'power-fr-train.tsv',\n",
        "    # 'power-nl-train.tsv',\n",
        "]\n",
        "\n",
        "full_data = load_data(folder_path=base_dir, file_list=file_list,text_head='text_en')\n",
        "train_dev_raw, test_raw = split_data(full_data, test_size=0.2, random_state=0)\n",
        "train_raw, dev_raw = split_data(train_dev_raw, test_size=0.2, random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "T16XsA0YOEPc",
        "outputId": "885a7921-da2a-457f-d0aa-a7abb1f53415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prepare data encoder...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PositionalEncoder(tokenizer=&lt;function PositionalEncoder.build_tokenizer.&lt;locals&gt;.simple_tokenizer at 0x7f199b576340&gt;,\n",
              "                  vocabulary={&#x27;&#x27;: 1, &#x27;!&#x27;: 63796, &#x27;&quot;&#x27;: 68750, &#x27;#&#x27;: 35922,\n",
              "                              &#x27;$&#x27;: 33148, &#x27;%&#x27;: 10679, &#x27;&amp;&#x27;: 21783, &quot;&#x27;&quot;: 72787,\n",
              "                              &#x27;(&#x27;: 19166, &#x27;)&#x27;: 56035, &#x27;*&#x27;: 7770, &#x27;+&#x27;: 52968,\n",
              "                              &#x27;,&#x27;: 14681, &#x27;-&#x27;: 50292, &#x27;.&#x27;: 11993, &#x27;/&#x27;: 6372,\n",
              "                              &#x27;0&#x27;: 15562, &#x27;00&#x27;: 62551, &#x27;000&#x27;: 3284,\n",
              "                              &#x27;0000&#x27;: 50639, &#x27;000th&#x27;: 32236, &#x27;001&#x27;: 995,\n",
              "                              &#x27;002&#x27;: 8219, &#x27;0023&#x27;: 705, &#x27;0025&#x27;: 9448,\n",
              "                              &#x27;0026&#x27;: 42244, &#x27;0029&#x27;: 61776, &#x27;004&#x27;: 2828,\n",
              "                              &#x27;0040&#x27;: 34655, &#x27;005&#x27;: 66925, ...})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;PositionalEncoder<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>PositionalEncoder(tokenizer=&lt;function PositionalEncoder.build_tokenizer.&lt;locals&gt;.simple_tokenizer at 0x7f199b576340&gt;,\n",
              "                  vocabulary={&#x27;&#x27;: 1, &#x27;!&#x27;: 63796, &#x27;&quot;&#x27;: 68750, &#x27;#&#x27;: 35922,\n",
              "                              &#x27;$&#x27;: 33148, &#x27;%&#x27;: 10679, &#x27;&amp;&#x27;: 21783, &quot;&#x27;&quot;: 72787,\n",
              "                              &#x27;(&#x27;: 19166, &#x27;)&#x27;: 56035, &#x27;*&#x27;: 7770, &#x27;+&#x27;: 52968,\n",
              "                              &#x27;,&#x27;: 14681, &#x27;-&#x27;: 50292, &#x27;.&#x27;: 11993, &#x27;/&#x27;: 6372,\n",
              "                              &#x27;0&#x27;: 15562, &#x27;00&#x27;: 62551, &#x27;000&#x27;: 3284,\n",
              "                              &#x27;0000&#x27;: 50639, &#x27;000th&#x27;: 32236, &#x27;001&#x27;: 995,\n",
              "                              &#x27;002&#x27;: 8219, &#x27;0023&#x27;: 705, &#x27;0025&#x27;: 9448,\n",
              "                              &#x27;0026&#x27;: 42244, &#x27;0029&#x27;: 61776, &#x27;004&#x27;: 2828,\n",
              "                              &#x27;0040&#x27;: 34655, &#x27;005&#x27;: 66925, ...})</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "PositionalEncoder(tokenizer=<function PositionalEncoder.build_tokenizer.<locals>.simple_tokenizer at 0x7f199b576340>,\n",
              "                  vocabulary={'': 1, '!': 63796, '\"': 68750, '#': 35922,\n",
              "                              '$': 33148, '%': 10679, '&': 21783, \"'\": 72787,\n",
              "                              '(': 19166, ')': 56035, '*': 7770, '+': 52968,\n",
              "                              ',': 14681, '-': 50292, '.': 11993, '/': 6372,\n",
              "                              '0': 15562, '00': 62551, '000': 3284,\n",
              "                              '0000': 50639, '000th': 32236, '001': 995,\n",
              "                              '002': 8219, '0023': 705, '0025': 9448,\n",
              "                              '0026': 42244, '0029': 61776, '004': 2828,\n",
              "                              '0040': 34655, '005': 66925, ...})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "print(\"Prepare data encoder...\")\n",
        "train_encoder = PositionalEncoder()\n",
        "train_encoder.fit(train_raw.texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmKV1p_6OEPc",
        "outputId": "b3e42164-8c44-49c4-9324-8dfa7f8e78c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1:   0%|          | 0/222 [00:00<?, ?batch/s]/media/hapham/Work/study/power-identification/utils.py:94: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  tokens_sparse = torch.sparse_csr_tensor(crow, col, token_val, size=mat_size, dtype=torch.long)\n",
            "Epoch 1: 100%|██████████| 222/222 [01:36<00:00,  2.31batch/s, batch_accuracy=0.49, loss=78.1]\n",
            "Epoch 2: 100%|██████████| 222/222 [01:36<00:00,  2.30batch/s, batch_accuracy=0.429, loss=78.9]\n",
            "Epoch 3: 100%|██████████| 222/222 [01:37<00:00,  2.27batch/s, batch_accuracy=0.51, loss=84.5]\n",
            "Epoch 4: 100%|██████████| 222/222 [01:41<00:00,  2.19batch/s, batch_accuracy=0.531, loss=73.7]\n",
            "Epoch 5: 100%|██████████| 222/222 [01:40<00:00,  2.21batch/s, batch_accuracy=0.878, loss=45.6]\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(train_raw, batch_size=128, shuffle=True)\n",
        "test_dataloader = DataLoader(test_raw, batch_size=128, shuffle=True)\n",
        "\n",
        "# Prepare baseline config\n",
        "train_config = TrainConfig(\n",
        "    optimizer_params = {'lr': 0.01},\n",
        "    num_epochs       = 10,\n",
        "    early_stop       = False,\n",
        "    violation_limit  = 5\n",
        ")\n",
        "\n",
        "# Train baseline model\n",
        "model_lstm = RNNClassifier(\n",
        "    rnn_network         = nn.LSTM,\n",
        "    word_embedding_dim  = 32,\n",
        "    hidden_dim          = 64,\n",
        "    bidirectional       = False,\n",
        "    dropout             = 0,\n",
        "    encoder             = train_encoder,\n",
        "    device              = 'cuda'\n",
        ")\n",
        "\n",
        "if Path('models/model_lstm.pt').exists():\n",
        "    model_lstm = load_model(model_lstm, 'model_lstm')\n",
        "else:\n",
        "    model_lstm.fit(train_dataloader, train_config, no_progress_bar=False)\n",
        "    save_model(model_lstm, \"model_lstm\")\n",
        "\n",
        "\n",
        "model_lstm_result = evaluate_rnn_model(model_lstm, test_dataloader, train_encoder)\n",
        "print(model_lstm_result)\n",
        "\n",
        "np.save('models/model_lstm_results.npy', model_lstm_result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-14e97481047f44cbb74b525a6a0e52b4.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-14e97481047f44cbb74b525a6a0e52b4.vega-embed details,\n",
              "  #altair-viz-14e97481047f44cbb74b525a6a0e52b4.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-14e97481047f44cbb74b525a6a0e52b4\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-14e97481047f44cbb74b525a6a0e52b4\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-14e97481047f44cbb74b525a6a0e52b4\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-b4ec9c33a573efc7c2d3ac58bcc8fa3b\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-b4ec9c33a573efc7c2d3ac58bcc8fa3b\": [{\"training_acc\": 0.6015625, \"training_loss\": 305.94866943359375, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 318.67108154296875, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 309.2485656738281, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 315.4113464355469, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 320.6465759277344, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 322.0889587402344, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 316.6962585449219, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 279.3127746582031, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 309.30645751953125, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 338.3255615234375, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 289.4328918457031, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 350.6089782714844, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 302.0039367675781, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 279.00396728515625, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 307.1020812988281, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 273.0736999511719, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 323.7996520996094, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 318.8865966796875, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 278.9598083496094, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 276.11517333984375, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 307.97564697265625, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 361.84808349609375, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 303.457763671875, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 295.4994201660156, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 259.7607727050781, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.4609375, \"training_loss\": 350.1203308105469, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 310.1988525390625, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 315.1435546875, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 284.5033874511719, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 309.56439208984375, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.4453125, \"training_loss\": 272.2148132324219, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.4453125, \"training_loss\": 270.4635009765625, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 324.5732421875, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 333.5359802246094, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.4375, \"training_loss\": 270.76568603515625, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.453125, \"training_loss\": 281.3143615722656, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 333.37017822265625, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 314.5299377441406, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 323.583251953125, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 358.2449951171875, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.484375, \"training_loss\": 302.73748779296875, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.421875, \"training_loss\": 260.5755310058594, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 331.7332763671875, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 330.9339599609375, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.4609375, \"training_loss\": 286.24713134765625, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.46875, \"training_loss\": 289.940673828125, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.484375, \"training_loss\": 300.001708984375, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.453125, \"training_loss\": 280.0323791503906, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 332.7511291503906, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 289.4053955078125, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 307.4225158691406, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 317.63787841796875, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 332.4586486816406, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 331.7408447265625, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.453125, \"training_loss\": 277.2408752441406, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.484375, \"training_loss\": 296.741455078125, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 323.8889465332031, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 332.77410888671875, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.4609375, \"training_loss\": 283.05023193359375, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.4765625, \"training_loss\": 295.70501708984375, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.484375, \"training_loss\": 298.161376953125, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 327.67315673828125, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 303.9167175292969, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 333.75250244140625, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.4609375, \"training_loss\": 249.54075622558594, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.4609375, \"training_loss\": 271.3160400390625, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 312.42413330078125, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 299.6336975097656, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 318.45452880859375, \"iteration\": 69, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 298.43121337890625, \"iteration\": 70, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 308.3742980957031, \"iteration\": 71, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 295.659912109375, \"iteration\": 72, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 317.7120361328125, \"iteration\": 73, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 283.5407409667969, \"iteration\": 74, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 329.11083984375, \"iteration\": 75, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 326.38348388671875, \"iteration\": 76, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 309.19671630859375, \"iteration\": 77, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 335.6331787109375, \"iteration\": 78, \"epoch\": 1}, {\"training_acc\": 0.46875, \"training_loss\": 287.98651123046875, \"iteration\": 79, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 381.50982666015625, \"iteration\": 80, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 313.0435791015625, \"iteration\": 81, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 308.33245849609375, \"iteration\": 82, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 342.44091796875, \"iteration\": 83, \"epoch\": 1}, {\"training_acc\": 0.46875, \"training_loss\": 279.88848876953125, \"iteration\": 84, \"epoch\": 1}, {\"training_acc\": 0.4765625, \"training_loss\": 267.63775634765625, \"iteration\": 85, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 278.2637023925781, \"iteration\": 86, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 328.4379577636719, \"iteration\": 87, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 309.77838134765625, \"iteration\": 88, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 294.90570068359375, \"iteration\": 89, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 300.14813232421875, \"iteration\": 90, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 330.5550537109375, \"iteration\": 91, \"epoch\": 1}, {\"training_acc\": 0.484375, \"training_loss\": 258.5137939453125, \"iteration\": 92, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 290.40234375, \"iteration\": 93, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 276.53656005859375, \"iteration\": 94, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 348.2439270019531, \"iteration\": 95, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 338.23175048828125, \"iteration\": 96, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 347.22369384765625, \"iteration\": 97, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 304.8079528808594, \"iteration\": 98, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 320.1978759765625, \"iteration\": 99, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 315.48736572265625, \"iteration\": 100, \"epoch\": 1}, {\"training_acc\": 0.4609375, \"training_loss\": 242.98922729492188, \"iteration\": 101, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 317.65533447265625, \"iteration\": 102, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 309.7818603515625, \"iteration\": 103, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 301.291259765625, \"iteration\": 104, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 289.20428466796875, \"iteration\": 105, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 331.34527587890625, \"iteration\": 106, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 292.86016845703125, \"iteration\": 107, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 298.7554626464844, \"iteration\": 108, \"epoch\": 1}, {\"training_acc\": 0.46875, \"training_loss\": 274.94232177734375, \"iteration\": 109, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 295.6295166015625, \"iteration\": 110, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 342.3573913574219, \"iteration\": 111, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 285.07379150390625, \"iteration\": 112, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 317.3174743652344, \"iteration\": 113, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 289.55450439453125, \"iteration\": 114, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 308.05511474609375, \"iteration\": 115, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 318.34820556640625, \"iteration\": 116, \"epoch\": 1}, {\"training_acc\": 0.46875, \"training_loss\": 287.32977294921875, \"iteration\": 117, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 298.80462646484375, \"iteration\": 118, \"epoch\": 1}, {\"training_acc\": 0.46875, \"training_loss\": 236.61915588378906, \"iteration\": 119, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 363.1248779296875, \"iteration\": 120, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 335.6266174316406, \"iteration\": 121, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 287.34857177734375, \"iteration\": 122, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 315.81695556640625, \"iteration\": 123, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 304.7268981933594, \"iteration\": 124, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 304.4047546386719, \"iteration\": 125, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 384.04058837890625, \"iteration\": 126, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 331.3652648925781, \"iteration\": 127, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 315.23822021484375, \"iteration\": 128, \"epoch\": 1}, {\"training_acc\": 0.453125, \"training_loss\": 278.85333251953125, \"iteration\": 129, \"epoch\": 1}, {\"training_acc\": 0.4765625, \"training_loss\": 286.1341247558594, \"iteration\": 130, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 311.8842468261719, \"iteration\": 131, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 301.11614990234375, \"iteration\": 132, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 296.7994689941406, \"iteration\": 133, \"epoch\": 1}, {\"training_acc\": 0.4765625, \"training_loss\": 267.7694091796875, \"iteration\": 134, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 298.0709228515625, \"iteration\": 135, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 345.72308349609375, \"iteration\": 136, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 309.01953125, \"iteration\": 137, \"epoch\": 1}, {\"training_acc\": 0.484375, \"training_loss\": 265.8675537109375, \"iteration\": 138, \"epoch\": 1}, {\"training_acc\": 0.4453125, \"training_loss\": 250.10574340820312, \"iteration\": 139, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 324.62664794921875, \"iteration\": 140, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 302.1293029785156, \"iteration\": 141, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 305.42205810546875, \"iteration\": 142, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 346.4547119140625, \"iteration\": 143, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 294.49560546875, \"iteration\": 144, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 238.08795166015625, \"iteration\": 145, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 295.72308349609375, \"iteration\": 146, \"epoch\": 1}, {\"training_acc\": 0.46875, \"training_loss\": 272.1329345703125, \"iteration\": 147, \"epoch\": 1}, {\"training_acc\": 0.484375, \"training_loss\": 281.37933349609375, \"iteration\": 148, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 278.8583679199219, \"iteration\": 149, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 323.0850830078125, \"iteration\": 150, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 262.16448974609375, \"iteration\": 151, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 309.2944641113281, \"iteration\": 152, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 304.219970703125, \"iteration\": 153, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 295.3042907714844, \"iteration\": 154, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 307.4246826171875, \"iteration\": 155, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 249.5101776123047, \"iteration\": 156, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 328.6224365234375, \"iteration\": 157, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 306.84033203125, \"iteration\": 158, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 327.00250244140625, \"iteration\": 159, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 269.0352783203125, \"iteration\": 160, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 262.4551696777344, \"iteration\": 161, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 267.69329833984375, \"iteration\": 162, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 340.0504455566406, \"iteration\": 163, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 294.4560546875, \"iteration\": 164, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 284.5649108886719, \"iteration\": 165, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 301.92791748046875, \"iteration\": 166, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 338.6226806640625, \"iteration\": 167, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 309.83477783203125, \"iteration\": 168, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 325.1235656738281, \"iteration\": 169, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 284.88739013671875, \"iteration\": 170, \"epoch\": 1}, {\"training_acc\": 0.4765625, \"training_loss\": 266.50201416015625, \"iteration\": 171, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 281.5130615234375, \"iteration\": 172, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 299.50048828125, \"iteration\": 173, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 308.438720703125, \"iteration\": 174, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 300.586669921875, \"iteration\": 175, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 322.89666748046875, \"iteration\": 176, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 322.6922607421875, \"iteration\": 177, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 247.2451171875, \"iteration\": 178, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 267.7235107421875, \"iteration\": 179, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 321.0532531738281, \"iteration\": 180, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 292.4459228515625, \"iteration\": 181, \"epoch\": 1}, {\"training_acc\": 0.359375, \"training_loss\": 216.07325744628906, \"iteration\": 182, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 294.026123046875, \"iteration\": 183, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 319.01690673828125, \"iteration\": 184, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 342.5643615722656, \"iteration\": 185, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 291.07952880859375, \"iteration\": 186, \"epoch\": 1}, {\"training_acc\": 0.46875, \"training_loss\": 280.9346618652344, \"iteration\": 187, \"epoch\": 1}, {\"training_acc\": 0.453125, \"training_loss\": 273.6526794433594, \"iteration\": 188, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 320.9628601074219, \"iteration\": 189, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 330.98040771484375, \"iteration\": 190, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 284.6419677734375, \"iteration\": 191, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 286.6838073730469, \"iteration\": 192, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 284.51171875, \"iteration\": 193, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 254.57177734375, \"iteration\": 194, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 295.16973876953125, \"iteration\": 195, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 333.34832763671875, \"iteration\": 196, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 301.02978515625, \"iteration\": 197, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 317.4241943359375, \"iteration\": 198, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 257.488525390625, \"iteration\": 199, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 319.4007873535156, \"iteration\": 200, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 304.5137939453125, \"iteration\": 201, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 277.29278564453125, \"iteration\": 202, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 271.43804931640625, \"iteration\": 203, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 282.7502136230469, \"iteration\": 204, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 318.8778076171875, \"iteration\": 205, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 292.4029541015625, \"iteration\": 206, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 304.73431396484375, \"iteration\": 207, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 317.02130126953125, \"iteration\": 208, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 285.3980407714844, \"iteration\": 209, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 297.63507080078125, \"iteration\": 210, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 258.62799072265625, \"iteration\": 211, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 249.70875549316406, \"iteration\": 212, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 254.13546752929688, \"iteration\": 213, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 287.926513671875, \"iteration\": 214, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 261.5621643066406, \"iteration\": 215, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 264.4448547363281, \"iteration\": 216, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 321.630126953125, \"iteration\": 217, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 283.4776306152344, \"iteration\": 218, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 236.48577880859375, \"iteration\": 219, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 300.12054443359375, \"iteration\": 220, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 256.86956787109375, \"iteration\": 221, \"epoch\": 1}, {\"training_acc\": 0.4897959183673469, \"training_loss\": 78.08651733398438, \"iteration\": 222, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 293.06158447265625, \"iteration\": 223, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 343.8641662597656, \"iteration\": 224, \"epoch\": 2}, {\"training_acc\": 0.65625, \"training_loss\": 250.44630432128906, \"iteration\": 225, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 305.41290283203125, \"iteration\": 226, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 361.63037109375, \"iteration\": 227, \"epoch\": 2}, {\"training_acc\": 0.6640625, \"training_loss\": 299.1485290527344, \"iteration\": 228, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 304.5040283203125, \"iteration\": 229, \"epoch\": 2}, {\"training_acc\": 0.59375, \"training_loss\": 305.7482604980469, \"iteration\": 230, \"epoch\": 2}, {\"training_acc\": 0.4921875, \"training_loss\": 265.8146667480469, \"iteration\": 231, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 313.7298278808594, \"iteration\": 232, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 278.90814208984375, \"iteration\": 233, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 238.72943115234375, \"iteration\": 234, \"epoch\": 2}, {\"training_acc\": 0.6796875, \"training_loss\": 245.83140563964844, \"iteration\": 235, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 275.9002685546875, \"iteration\": 236, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 335.52227783203125, \"iteration\": 237, \"epoch\": 2}, {\"training_acc\": 0.65625, \"training_loss\": 279.6871337890625, \"iteration\": 238, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 316.66363525390625, \"iteration\": 239, \"epoch\": 2}, {\"training_acc\": 0.65625, \"training_loss\": 284.3531188964844, \"iteration\": 240, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 304.9836730957031, \"iteration\": 241, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 290.6868896484375, \"iteration\": 242, \"epoch\": 2}, {\"training_acc\": 0.5625, \"training_loss\": 273.02490234375, \"iteration\": 243, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 276.771728515625, \"iteration\": 244, \"epoch\": 2}, {\"training_acc\": 0.6484375, \"training_loss\": 315.0672607421875, \"iteration\": 245, \"epoch\": 2}, {\"training_acc\": 0.5859375, \"training_loss\": 286.2545471191406, \"iteration\": 246, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 340.5035705566406, \"iteration\": 247, \"epoch\": 2}, {\"training_acc\": 0.6796875, \"training_loss\": 288.6922607421875, \"iteration\": 248, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 292.009521484375, \"iteration\": 249, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 280.4605407714844, \"iteration\": 250, \"epoch\": 2}, {\"training_acc\": 0.6796875, \"training_loss\": 275.9796142578125, \"iteration\": 251, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 303.7039794921875, \"iteration\": 252, \"epoch\": 2}, {\"training_acc\": 0.6484375, \"training_loss\": 280.12042236328125, \"iteration\": 253, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 305.64349365234375, \"iteration\": 254, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 310.4853515625, \"iteration\": 255, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 331.0186462402344, \"iteration\": 256, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 307.4326477050781, \"iteration\": 257, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 328.302978515625, \"iteration\": 258, \"epoch\": 2}, {\"training_acc\": 0.6015625, \"training_loss\": 262.92572021484375, \"iteration\": 259, \"epoch\": 2}, {\"training_acc\": 0.6484375, \"training_loss\": 248.84483337402344, \"iteration\": 260, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 255.40032958984375, \"iteration\": 261, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 339.2900390625, \"iteration\": 262, \"epoch\": 2}, {\"training_acc\": 0.6171875, \"training_loss\": 277.7497863769531, \"iteration\": 263, \"epoch\": 2}, {\"training_acc\": 0.6640625, \"training_loss\": 342.2254333496094, \"iteration\": 264, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 293.19647216796875, \"iteration\": 265, \"epoch\": 2}, {\"training_acc\": 0.546875, \"training_loss\": 283.1212158203125, \"iteration\": 266, \"epoch\": 2}, {\"training_acc\": 0.4921875, \"training_loss\": 280.216552734375, \"iteration\": 267, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 260.8079833984375, \"iteration\": 268, \"epoch\": 2}, {\"training_acc\": 0.5703125, \"training_loss\": 301.40850830078125, \"iteration\": 269, \"epoch\": 2}, {\"training_acc\": 0.59375, \"training_loss\": 271.7640075683594, \"iteration\": 270, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 327.0245361328125, \"iteration\": 271, \"epoch\": 2}, {\"training_acc\": 0.46875, \"training_loss\": 264.4187927246094, \"iteration\": 272, \"epoch\": 2}, {\"training_acc\": 0.6171875, \"training_loss\": 260.548095703125, \"iteration\": 273, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 291.758056640625, \"iteration\": 274, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 297.966552734375, \"iteration\": 275, \"epoch\": 2}, {\"training_acc\": 0.65625, \"training_loss\": 324.92218017578125, \"iteration\": 276, \"epoch\": 2}, {\"training_acc\": 0.6484375, \"training_loss\": 290.47442626953125, \"iteration\": 277, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 248.33056640625, \"iteration\": 278, \"epoch\": 2}, {\"training_acc\": 0.5625, \"training_loss\": 275.9870910644531, \"iteration\": 279, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 302.1739501953125, \"iteration\": 280, \"epoch\": 2}, {\"training_acc\": 0.484375, \"training_loss\": 255.47557067871094, \"iteration\": 281, \"epoch\": 2}, {\"training_acc\": 0.5625, \"training_loss\": 299.7315673828125, \"iteration\": 282, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 268.32086181640625, \"iteration\": 283, \"epoch\": 2}, {\"training_acc\": 0.5703125, \"training_loss\": 271.784423828125, \"iteration\": 284, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 292.93218994140625, \"iteration\": 285, \"epoch\": 2}, {\"training_acc\": 0.484375, \"training_loss\": 226.3531494140625, \"iteration\": 286, \"epoch\": 2}, {\"training_acc\": 0.5859375, \"training_loss\": 261.0318603515625, \"iteration\": 287, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 250.08880615234375, \"iteration\": 288, \"epoch\": 2}, {\"training_acc\": 0.5625, \"training_loss\": 264.1664733886719, \"iteration\": 289, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 259.99169921875, \"iteration\": 290, \"epoch\": 2}, {\"training_acc\": 0.4765625, \"training_loss\": 269.24566650390625, \"iteration\": 291, \"epoch\": 2}, {\"training_acc\": 0.46875, \"training_loss\": 271.2720642089844, \"iteration\": 292, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 302.41107177734375, \"iteration\": 293, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 348.98529052734375, \"iteration\": 294, \"epoch\": 2}, {\"training_acc\": 0.546875, \"training_loss\": 326.81585693359375, \"iteration\": 295, \"epoch\": 2}, {\"training_acc\": 0.484375, \"training_loss\": 282.482421875, \"iteration\": 296, \"epoch\": 2}, {\"training_acc\": 0.4921875, \"training_loss\": 275.8446044921875, \"iteration\": 297, \"epoch\": 2}, {\"training_acc\": 0.515625, \"training_loss\": 295.48406982421875, \"iteration\": 298, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 298.66033935546875, \"iteration\": 299, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 286.1493835449219, \"iteration\": 300, \"epoch\": 2}, {\"training_acc\": 0.4765625, \"training_loss\": 264.973876953125, \"iteration\": 301, \"epoch\": 2}, {\"training_acc\": 0.5625, \"training_loss\": 319.6068420410156, \"iteration\": 302, \"epoch\": 2}, {\"training_acc\": 0.5859375, \"training_loss\": 343.97113037109375, \"iteration\": 303, \"epoch\": 2}, {\"training_acc\": 0.4921875, \"training_loss\": 230.669921875, \"iteration\": 304, \"epoch\": 2}, {\"training_acc\": 0.6015625, \"training_loss\": 331.94854736328125, \"iteration\": 305, \"epoch\": 2}, {\"training_acc\": 0.53125, \"training_loss\": 278.3498840332031, \"iteration\": 306, \"epoch\": 2}, {\"training_acc\": 0.5859375, \"training_loss\": 332.8687744140625, \"iteration\": 307, \"epoch\": 2}, {\"training_acc\": 0.515625, \"training_loss\": 298.7565612792969, \"iteration\": 308, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 300.6828308105469, \"iteration\": 309, \"epoch\": 2}, {\"training_acc\": 0.4765625, \"training_loss\": 264.74139404296875, \"iteration\": 310, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 325.235107421875, \"iteration\": 311, \"epoch\": 2}, {\"training_acc\": 0.4765625, \"training_loss\": 258.2421569824219, \"iteration\": 312, \"epoch\": 2}, {\"training_acc\": 0.515625, \"training_loss\": 303.4893798828125, \"iteration\": 313, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 310.12628173828125, \"iteration\": 314, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 302.5067138671875, \"iteration\": 315, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 278.74713134765625, \"iteration\": 316, \"epoch\": 2}, {\"training_acc\": 0.515625, \"training_loss\": 285.3066711425781, \"iteration\": 317, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 288.9834289550781, \"iteration\": 318, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 285.8592529296875, \"iteration\": 319, \"epoch\": 2}, {\"training_acc\": 0.4609375, \"training_loss\": 271.81524658203125, \"iteration\": 320, \"epoch\": 2}, {\"training_acc\": 0.453125, \"training_loss\": 248.4609832763672, \"iteration\": 321, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 271.64508056640625, \"iteration\": 322, \"epoch\": 2}, {\"training_acc\": 0.5390625, \"training_loss\": 319.72332763671875, \"iteration\": 323, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 280.47479248046875, \"iteration\": 324, \"epoch\": 2}, {\"training_acc\": 0.4921875, \"training_loss\": 222.48463439941406, \"iteration\": 325, \"epoch\": 2}, {\"training_acc\": 0.6015625, \"training_loss\": 331.5944519042969, \"iteration\": 326, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 353.0190734863281, \"iteration\": 327, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 325.9569396972656, \"iteration\": 328, \"epoch\": 2}, {\"training_acc\": 0.484375, \"training_loss\": 269.2955322265625, \"iteration\": 329, \"epoch\": 2}, {\"training_acc\": 0.59375, \"training_loss\": 324.87066650390625, \"iteration\": 330, \"epoch\": 2}, {\"training_acc\": 0.5859375, \"training_loss\": 315.642333984375, \"iteration\": 331, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 285.8035888671875, \"iteration\": 332, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 291.88812255859375, \"iteration\": 333, \"epoch\": 2}, {\"training_acc\": 0.5390625, \"training_loss\": 318.2231140136719, \"iteration\": 334, \"epoch\": 2}, {\"training_acc\": 0.515625, \"training_loss\": 262.10943603515625, \"iteration\": 335, \"epoch\": 2}, {\"training_acc\": 0.6015625, \"training_loss\": 333.28692626953125, \"iteration\": 336, \"epoch\": 2}, {\"training_acc\": 0.515625, \"training_loss\": 259.7364196777344, \"iteration\": 337, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 273.78607177734375, \"iteration\": 338, \"epoch\": 2}, {\"training_acc\": 0.609375, \"training_loss\": 300.94195556640625, \"iteration\": 339, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 305.10565185546875, \"iteration\": 340, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 269.66400146484375, \"iteration\": 341, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 282.34893798828125, \"iteration\": 342, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 323.6020202636719, \"iteration\": 343, \"epoch\": 2}, {\"training_acc\": 0.6171875, \"training_loss\": 291.0950927734375, \"iteration\": 344, \"epoch\": 2}, {\"training_acc\": 0.5859375, \"training_loss\": 299.3098449707031, \"iteration\": 345, \"epoch\": 2}, {\"training_acc\": 0.546875, \"training_loss\": 307.25030517578125, \"iteration\": 346, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 313.49725341796875, \"iteration\": 347, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 325.74615478515625, \"iteration\": 348, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 293.6314697265625, \"iteration\": 349, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 300.3934020996094, \"iteration\": 350, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 300.21258544921875, \"iteration\": 351, \"epoch\": 2}, {\"training_acc\": 0.484375, \"training_loss\": 279.20672607421875, \"iteration\": 352, \"epoch\": 2}, {\"training_acc\": 0.53125, \"training_loss\": 298.5342102050781, \"iteration\": 353, \"epoch\": 2}, {\"training_acc\": 0.59375, \"training_loss\": 340.2911376953125, \"iteration\": 354, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 290.5401306152344, \"iteration\": 355, \"epoch\": 2}, {\"training_acc\": 0.4453125, \"training_loss\": 245.4418182373047, \"iteration\": 356, \"epoch\": 2}, {\"training_acc\": 0.5859375, \"training_loss\": 285.5205078125, \"iteration\": 357, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 320.34185791015625, \"iteration\": 358, \"epoch\": 2}, {\"training_acc\": 0.6171875, \"training_loss\": 323.090576171875, \"iteration\": 359, \"epoch\": 2}, {\"training_acc\": 0.6484375, \"training_loss\": 338.08636474609375, \"iteration\": 360, \"epoch\": 2}, {\"training_acc\": 0.5390625, \"training_loss\": 289.9974365234375, \"iteration\": 361, \"epoch\": 2}, {\"training_acc\": 0.53125, \"training_loss\": 271.6594543457031, \"iteration\": 362, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 281.3191223144531, \"iteration\": 363, \"epoch\": 2}, {\"training_acc\": 0.5390625, \"training_loss\": 278.31622314453125, \"iteration\": 364, \"epoch\": 2}, {\"training_acc\": 0.5625, \"training_loss\": 300.01446533203125, \"iteration\": 365, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 283.4263916015625, \"iteration\": 366, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 279.84271240234375, \"iteration\": 367, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 285.82427978515625, \"iteration\": 368, \"epoch\": 2}, {\"training_acc\": 0.609375, \"training_loss\": 319.75494384765625, \"iteration\": 369, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 278.6907043457031, \"iteration\": 370, \"epoch\": 2}, {\"training_acc\": 0.5703125, \"training_loss\": 319.08892822265625, \"iteration\": 371, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 346.1395568847656, \"iteration\": 372, \"epoch\": 2}, {\"training_acc\": 0.5703125, \"training_loss\": 308.3388671875, \"iteration\": 373, \"epoch\": 2}, {\"training_acc\": 0.546875, \"training_loss\": 270.7463073730469, \"iteration\": 374, \"epoch\": 2}, {\"training_acc\": 0.59375, \"training_loss\": 296.9076232910156, \"iteration\": 375, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 262.783203125, \"iteration\": 376, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 290.74749755859375, \"iteration\": 377, \"epoch\": 2}, {\"training_acc\": 0.4765625, \"training_loss\": 241.07733154296875, \"iteration\": 378, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 268.969970703125, \"iteration\": 379, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 282.5076904296875, \"iteration\": 380, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 292.3763427734375, \"iteration\": 381, \"epoch\": 2}, {\"training_acc\": 0.515625, \"training_loss\": 297.1889343261719, \"iteration\": 382, \"epoch\": 2}, {\"training_acc\": 0.453125, \"training_loss\": 250.80850219726562, \"iteration\": 383, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 280.70379638671875, \"iteration\": 384, \"epoch\": 2}, {\"training_acc\": 0.484375, \"training_loss\": 261.2610778808594, \"iteration\": 385, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 293.94610595703125, \"iteration\": 386, \"epoch\": 2}, {\"training_acc\": 0.53125, \"training_loss\": 308.71527099609375, \"iteration\": 387, \"epoch\": 2}, {\"training_acc\": 0.6015625, \"training_loss\": 319.53643798828125, \"iteration\": 388, \"epoch\": 2}, {\"training_acc\": 0.5390625, \"training_loss\": 242.50872802734375, \"iteration\": 389, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 330.03936767578125, \"iteration\": 390, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 291.6580810546875, \"iteration\": 391, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 267.5219421386719, \"iteration\": 392, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 308.51715087890625, \"iteration\": 393, \"epoch\": 2}, {\"training_acc\": 0.4921875, \"training_loss\": 250.9560546875, \"iteration\": 394, \"epoch\": 2}, {\"training_acc\": 0.5625, \"training_loss\": 292.55078125, \"iteration\": 395, \"epoch\": 2}, {\"training_acc\": 0.53125, \"training_loss\": 261.710205078125, \"iteration\": 396, \"epoch\": 2}, {\"training_acc\": 0.546875, \"training_loss\": 284.894775390625, \"iteration\": 397, \"epoch\": 2}, {\"training_acc\": 0.4921875, \"training_loss\": 271.697998046875, \"iteration\": 398, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 288.7366638183594, \"iteration\": 399, \"epoch\": 2}, {\"training_acc\": 0.484375, \"training_loss\": 268.7579345703125, \"iteration\": 400, \"epoch\": 2}, {\"training_acc\": 0.4921875, \"training_loss\": 269.54400634765625, \"iteration\": 401, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 290.0928955078125, \"iteration\": 402, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 303.4669189453125, \"iteration\": 403, \"epoch\": 2}, {\"training_acc\": 0.4921875, \"training_loss\": 275.83197021484375, \"iteration\": 404, \"epoch\": 2}, {\"training_acc\": 0.5390625, \"training_loss\": 275.476806640625, \"iteration\": 405, \"epoch\": 2}, {\"training_acc\": 0.609375, \"training_loss\": 330.06787109375, \"iteration\": 406, \"epoch\": 2}, {\"training_acc\": 0.6171875, \"training_loss\": 321.916748046875, \"iteration\": 407, \"epoch\": 2}, {\"training_acc\": 0.546875, \"training_loss\": 250.2935028076172, \"iteration\": 408, \"epoch\": 2}, {\"training_acc\": 0.5625, \"training_loss\": 292.9396667480469, \"iteration\": 409, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 306.4690856933594, \"iteration\": 410, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 273.158935546875, \"iteration\": 411, \"epoch\": 2}, {\"training_acc\": 0.4765625, \"training_loss\": 273.087646484375, \"iteration\": 412, \"epoch\": 2}, {\"training_acc\": 0.4765625, \"training_loss\": 272.69158935546875, \"iteration\": 413, \"epoch\": 2}, {\"training_acc\": 0.4453125, \"training_loss\": 253.579833984375, \"iteration\": 414, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 330.16357421875, \"iteration\": 415, \"epoch\": 2}, {\"training_acc\": 0.5703125, \"training_loss\": 324.77227783203125, \"iteration\": 416, \"epoch\": 2}, {\"training_acc\": 0.4453125, \"training_loss\": 260.1972351074219, \"iteration\": 417, \"epoch\": 2}, {\"training_acc\": 0.53125, \"training_loss\": 307.7255859375, \"iteration\": 418, \"epoch\": 2}, {\"training_acc\": 0.53125, \"training_loss\": 309.14129638671875, \"iteration\": 419, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 287.2271423339844, \"iteration\": 420, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 301.11175537109375, \"iteration\": 421, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 296.8362731933594, \"iteration\": 422, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 292.7000732421875, \"iteration\": 423, \"epoch\": 2}, {\"training_acc\": 0.53125, \"training_loss\": 306.31170654296875, \"iteration\": 424, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 296.744873046875, \"iteration\": 425, \"epoch\": 2}, {\"training_acc\": 0.46875, \"training_loss\": 271.5515441894531, \"iteration\": 426, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 274.9124755859375, \"iteration\": 427, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 281.5848388671875, \"iteration\": 428, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 302.68951416015625, \"iteration\": 429, \"epoch\": 2}, {\"training_acc\": 0.59375, \"training_loss\": 312.9204406738281, \"iteration\": 430, \"epoch\": 2}, {\"training_acc\": 0.5625, \"training_loss\": 336.886474609375, \"iteration\": 431, \"epoch\": 2}, {\"training_acc\": 0.5859375, \"training_loss\": 322.4791259765625, \"iteration\": 432, \"epoch\": 2}, {\"training_acc\": 0.5390625, \"training_loss\": 303.79815673828125, \"iteration\": 433, \"epoch\": 2}, {\"training_acc\": 0.4609375, \"training_loss\": 252.43438720703125, \"iteration\": 434, \"epoch\": 2}, {\"training_acc\": 0.5234375, \"training_loss\": 296.0194396972656, \"iteration\": 435, \"epoch\": 2}, {\"training_acc\": 0.5390625, \"training_loss\": 305.83319091796875, \"iteration\": 436, \"epoch\": 2}, {\"training_acc\": 0.40625, \"training_loss\": 220.25637817382812, \"iteration\": 437, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 364.55682373046875, \"iteration\": 438, \"epoch\": 2}, {\"training_acc\": 0.4765625, \"training_loss\": 282.2462158203125, \"iteration\": 439, \"epoch\": 2}, {\"training_acc\": 0.5, \"training_loss\": 286.15216064453125, \"iteration\": 440, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 308.36181640625, \"iteration\": 441, \"epoch\": 2}, {\"training_acc\": 0.5078125, \"training_loss\": 302.2333984375, \"iteration\": 442, \"epoch\": 2}, {\"training_acc\": 0.4765625, \"training_loss\": 282.52154541015625, \"iteration\": 443, \"epoch\": 2}, {\"training_acc\": 0.42857142857142855, \"training_loss\": 78.9268798828125, \"iteration\": 444, \"epoch\": 2}, {\"training_acc\": 0.453125, \"training_loss\": 260.5926513671875, \"iteration\": 445, \"epoch\": 3}, {\"training_acc\": 0.4296875, \"training_loss\": 245.71902465820312, \"iteration\": 446, \"epoch\": 3}, {\"training_acc\": 0.4453125, \"training_loss\": 258.46197509765625, \"iteration\": 447, \"epoch\": 3}, {\"training_acc\": 0.5, \"training_loss\": 292.01409912109375, \"iteration\": 448, \"epoch\": 3}, {\"training_acc\": 0.5234375, \"training_loss\": 302.61016845703125, \"iteration\": 449, \"epoch\": 3}, {\"training_acc\": 0.5703125, \"training_loss\": 327.8721923828125, \"iteration\": 450, \"epoch\": 3}, {\"training_acc\": 0.4765625, \"training_loss\": 269.1167297363281, \"iteration\": 451, \"epoch\": 3}, {\"training_acc\": 0.515625, \"training_loss\": 290.89031982421875, \"iteration\": 452, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 274.471435546875, \"iteration\": 453, \"epoch\": 3}, {\"training_acc\": 0.46875, \"training_loss\": 243.96002197265625, \"iteration\": 454, \"epoch\": 3}, {\"training_acc\": 0.546875, \"training_loss\": 271.09588623046875, \"iteration\": 455, \"epoch\": 3}, {\"training_acc\": 0.609375, \"training_loss\": 258.3378601074219, \"iteration\": 456, \"epoch\": 3}, {\"training_acc\": 0.5390625, \"training_loss\": 259.8624267578125, \"iteration\": 457, \"epoch\": 3}, {\"training_acc\": 0.6328125, \"training_loss\": 310.9666748046875, \"iteration\": 458, \"epoch\": 3}, {\"training_acc\": 0.4921875, \"training_loss\": 226.6332244873047, \"iteration\": 459, \"epoch\": 3}, {\"training_acc\": 0.625, \"training_loss\": 270.34771728515625, \"iteration\": 460, \"epoch\": 3}, {\"training_acc\": 0.6796875, \"training_loss\": 277.642578125, \"iteration\": 461, \"epoch\": 3}, {\"training_acc\": 0.640625, \"training_loss\": 268.4493408203125, \"iteration\": 462, \"epoch\": 3}, {\"training_acc\": 0.625, \"training_loss\": 251.52447509765625, \"iteration\": 463, \"epoch\": 3}, {\"training_acc\": 0.6796875, \"training_loss\": 329.38421630859375, \"iteration\": 464, \"epoch\": 3}, {\"training_acc\": 0.5703125, \"training_loss\": 295.37042236328125, \"iteration\": 465, \"epoch\": 3}, {\"training_acc\": 0.53125, \"training_loss\": 286.7794494628906, \"iteration\": 466, \"epoch\": 3}, {\"training_acc\": 0.5703125, \"training_loss\": 319.31768798828125, \"iteration\": 467, \"epoch\": 3}, {\"training_acc\": 0.4921875, \"training_loss\": 285.48004150390625, \"iteration\": 468, \"epoch\": 3}, {\"training_acc\": 0.4921875, \"training_loss\": 282.50604248046875, \"iteration\": 469, \"epoch\": 3}, {\"training_acc\": 0.578125, \"training_loss\": 341.6083984375, \"iteration\": 470, \"epoch\": 3}, {\"training_acc\": 0.53125, \"training_loss\": 308.3606262207031, \"iteration\": 471, \"epoch\": 3}, {\"training_acc\": 0.4140625, \"training_loss\": 232.8485107421875, \"iteration\": 472, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 299.31201171875, \"iteration\": 473, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 278.5246887207031, \"iteration\": 474, \"epoch\": 3}, {\"training_acc\": 0.5390625, \"training_loss\": 296.99957275390625, \"iteration\": 475, \"epoch\": 3}, {\"training_acc\": 0.5234375, \"training_loss\": 287.6246643066406, \"iteration\": 476, \"epoch\": 3}, {\"training_acc\": 0.515625, \"training_loss\": 298.4356689453125, \"iteration\": 477, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 303.7976989746094, \"iteration\": 478, \"epoch\": 3}, {\"training_acc\": 0.4921875, \"training_loss\": 259.9388427734375, \"iteration\": 479, \"epoch\": 3}, {\"training_acc\": 0.484375, \"training_loss\": 234.56915283203125, \"iteration\": 480, \"epoch\": 3}, {\"training_acc\": 0.390625, \"training_loss\": 193.25877380371094, \"iteration\": 481, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 254.06137084960938, \"iteration\": 482, \"epoch\": 3}, {\"training_acc\": 0.5859375, \"training_loss\": 309.36907958984375, \"iteration\": 483, \"epoch\": 3}, {\"training_acc\": 0.6015625, \"training_loss\": 271.9588623046875, \"iteration\": 484, \"epoch\": 3}, {\"training_acc\": 0.609375, \"training_loss\": 292.2309875488281, \"iteration\": 485, \"epoch\": 3}, {\"training_acc\": 0.53125, \"training_loss\": 261.665283203125, \"iteration\": 486, \"epoch\": 3}, {\"training_acc\": 0.4765625, \"training_loss\": 225.46694946289062, \"iteration\": 487, \"epoch\": 3}, {\"training_acc\": 0.734375, \"training_loss\": 395.728271484375, \"iteration\": 488, \"epoch\": 3}, {\"training_acc\": 0.40625, \"training_loss\": 195.7035369873047, \"iteration\": 489, \"epoch\": 3}, {\"training_acc\": 0.5703125, \"training_loss\": 284.7687683105469, \"iteration\": 490, \"epoch\": 3}, {\"training_acc\": 0.4765625, \"training_loss\": 241.30905151367188, \"iteration\": 491, \"epoch\": 3}, {\"training_acc\": 0.46875, \"training_loss\": 249.6156768798828, \"iteration\": 492, \"epoch\": 3}, {\"training_acc\": 0.546875, \"training_loss\": 286.86614990234375, \"iteration\": 493, \"epoch\": 3}, {\"training_acc\": 0.5234375, \"training_loss\": 271.521728515625, \"iteration\": 494, \"epoch\": 3}, {\"training_acc\": 0.578125, \"training_loss\": 275.7147521972656, \"iteration\": 495, \"epoch\": 3}, {\"training_acc\": 0.6328125, \"training_loss\": 323.01739501953125, \"iteration\": 496, \"epoch\": 3}, {\"training_acc\": 0.625, \"training_loss\": 343.98358154296875, \"iteration\": 497, \"epoch\": 3}, {\"training_acc\": 0.609375, \"training_loss\": 283.41998291015625, \"iteration\": 498, \"epoch\": 3}, {\"training_acc\": 0.59375, \"training_loss\": 263.11956787109375, \"iteration\": 499, \"epoch\": 3}, {\"training_acc\": 0.625, \"training_loss\": 271.93084716796875, \"iteration\": 500, \"epoch\": 3}, {\"training_acc\": 0.59375, \"training_loss\": 290.45574951171875, \"iteration\": 501, \"epoch\": 3}, {\"training_acc\": 0.5703125, \"training_loss\": 225.90133666992188, \"iteration\": 502, \"epoch\": 3}, {\"training_acc\": 0.671875, \"training_loss\": 271.59649658203125, \"iteration\": 503, \"epoch\": 3}, {\"training_acc\": 0.671875, \"training_loss\": 281.8704833984375, \"iteration\": 504, \"epoch\": 3}, {\"training_acc\": 0.6640625, \"training_loss\": 278.8024597167969, \"iteration\": 505, \"epoch\": 3}, {\"training_acc\": 0.7109375, \"training_loss\": 329.1402587890625, \"iteration\": 506, \"epoch\": 3}, {\"training_acc\": 0.546875, \"training_loss\": 277.6080322265625, \"iteration\": 507, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 261.66534423828125, \"iteration\": 508, \"epoch\": 3}, {\"training_acc\": 0.5859375, \"training_loss\": 329.357177734375, \"iteration\": 509, \"epoch\": 3}, {\"training_acc\": 0.578125, \"training_loss\": 282.80072021484375, \"iteration\": 510, \"epoch\": 3}, {\"training_acc\": 0.6015625, \"training_loss\": 321.54193115234375, \"iteration\": 511, \"epoch\": 3}, {\"training_acc\": 0.5390625, \"training_loss\": 255.37771606445312, \"iteration\": 512, \"epoch\": 3}, {\"training_acc\": 0.5859375, \"training_loss\": 289.8726806640625, \"iteration\": 513, \"epoch\": 3}, {\"training_acc\": 0.59375, \"training_loss\": 297.85418701171875, \"iteration\": 514, \"epoch\": 3}, {\"training_acc\": 0.515625, \"training_loss\": 262.2090759277344, \"iteration\": 515, \"epoch\": 3}, {\"training_acc\": 0.484375, \"training_loss\": 258.1171569824219, \"iteration\": 516, \"epoch\": 3}, {\"training_acc\": 0.6015625, \"training_loss\": 333.1166076660156, \"iteration\": 517, \"epoch\": 3}, {\"training_acc\": 0.4921875, \"training_loss\": 275.129638671875, \"iteration\": 518, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 317.6370849609375, \"iteration\": 519, \"epoch\": 3}, {\"training_acc\": 0.4296875, \"training_loss\": 253.14134216308594, \"iteration\": 520, \"epoch\": 3}, {\"training_acc\": 0.453125, \"training_loss\": 241.8579864501953, \"iteration\": 521, \"epoch\": 3}, {\"training_acc\": 0.6015625, \"training_loss\": 336.7801208496094, \"iteration\": 522, \"epoch\": 3}, {\"training_acc\": 0.5703125, \"training_loss\": 322.9073486328125, \"iteration\": 523, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 276.7540588378906, \"iteration\": 524, \"epoch\": 3}, {\"training_acc\": 0.3984375, \"training_loss\": 179.113525390625, \"iteration\": 525, \"epoch\": 3}, {\"training_acc\": 0.5546875, \"training_loss\": 252.36878967285156, \"iteration\": 526, \"epoch\": 3}, {\"training_acc\": 0.6875, \"training_loss\": 344.57476806640625, \"iteration\": 527, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 299.06427001953125, \"iteration\": 528, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 257.5769348144531, \"iteration\": 529, \"epoch\": 3}, {\"training_acc\": 0.4453125, \"training_loss\": 225.5677490234375, \"iteration\": 530, \"epoch\": 3}, {\"training_acc\": 0.4609375, \"training_loss\": 235.65023803710938, \"iteration\": 531, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 315.34075927734375, \"iteration\": 532, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 268.52984619140625, \"iteration\": 533, \"epoch\": 3}, {\"training_acc\": 0.5859375, \"training_loss\": 296.65594482421875, \"iteration\": 534, \"epoch\": 3}, {\"training_acc\": 0.53125, \"training_loss\": 302.12408447265625, \"iteration\": 535, \"epoch\": 3}, {\"training_acc\": 0.609375, \"training_loss\": 299.1510314941406, \"iteration\": 536, \"epoch\": 3}, {\"training_acc\": 0.5546875, \"training_loss\": 270.37017822265625, \"iteration\": 537, \"epoch\": 3}, {\"training_acc\": 0.609375, \"training_loss\": 292.2562255859375, \"iteration\": 538, \"epoch\": 3}, {\"training_acc\": 0.515625, \"training_loss\": 238.09347534179688, \"iteration\": 539, \"epoch\": 3}, {\"training_acc\": 0.5546875, \"training_loss\": 291.2972412109375, \"iteration\": 540, \"epoch\": 3}, {\"training_acc\": 0.6484375, \"training_loss\": 319.1473388671875, \"iteration\": 541, \"epoch\": 3}, {\"training_acc\": 0.515625, \"training_loss\": 248.19480895996094, \"iteration\": 542, \"epoch\": 3}, {\"training_acc\": 0.4765625, \"training_loss\": 253.19903564453125, \"iteration\": 543, \"epoch\": 3}, {\"training_acc\": 0.515625, \"training_loss\": 270.28985595703125, \"iteration\": 544, \"epoch\": 3}, {\"training_acc\": 0.65625, \"training_loss\": 320.76043701171875, \"iteration\": 545, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 278.044189453125, \"iteration\": 546, \"epoch\": 3}, {\"training_acc\": 0.4921875, \"training_loss\": 232.10931396484375, \"iteration\": 547, \"epoch\": 3}, {\"training_acc\": 0.5234375, \"training_loss\": 262.2381896972656, \"iteration\": 548, \"epoch\": 3}, {\"training_acc\": 0.625, \"training_loss\": 328.66424560546875, \"iteration\": 549, \"epoch\": 3}, {\"training_acc\": 0.5234375, \"training_loss\": 274.2411193847656, \"iteration\": 550, \"epoch\": 3}, {\"training_acc\": 0.5390625, \"training_loss\": 293.2911376953125, \"iteration\": 551, \"epoch\": 3}, {\"training_acc\": 0.4921875, \"training_loss\": 269.5238342285156, \"iteration\": 552, \"epoch\": 3}, {\"training_acc\": 0.484375, \"training_loss\": 266.8427429199219, \"iteration\": 553, \"epoch\": 3}, {\"training_acc\": 0.3828125, \"training_loss\": 213.58157348632812, \"iteration\": 554, \"epoch\": 3}, {\"training_acc\": 0.578125, \"training_loss\": 298.9761962890625, \"iteration\": 555, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 268.4022216796875, \"iteration\": 556, \"epoch\": 3}, {\"training_acc\": 0.5, \"training_loss\": 267.2566223144531, \"iteration\": 557, \"epoch\": 3}, {\"training_acc\": 0.4609375, \"training_loss\": 248.31732177734375, \"iteration\": 558, \"epoch\": 3}, {\"training_acc\": 0.625, \"training_loss\": 321.3698425292969, \"iteration\": 559, \"epoch\": 3}, {\"training_acc\": 0.5546875, \"training_loss\": 294.930419921875, \"iteration\": 560, \"epoch\": 3}, {\"training_acc\": 0.640625, \"training_loss\": 312.04248046875, \"iteration\": 561, \"epoch\": 3}, {\"training_acc\": 0.5546875, \"training_loss\": 261.2233581542969, \"iteration\": 562, \"epoch\": 3}, {\"training_acc\": 0.546875, \"training_loss\": 251.74212646484375, \"iteration\": 563, \"epoch\": 3}, {\"training_acc\": 0.625, \"training_loss\": 297.4438171386719, \"iteration\": 564, \"epoch\": 3}, {\"training_acc\": 0.65625, \"training_loss\": 359.2581787109375, \"iteration\": 565, \"epoch\": 3}, {\"training_acc\": 0.5546875, \"training_loss\": 294.0831298828125, \"iteration\": 566, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 302.05902099609375, \"iteration\": 567, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 310.6617431640625, \"iteration\": 568, \"epoch\": 3}, {\"training_acc\": 0.4609375, \"training_loss\": 249.6509552001953, \"iteration\": 569, \"epoch\": 3}, {\"training_acc\": 0.59375, \"training_loss\": 319.8412170410156, \"iteration\": 570, \"epoch\": 3}, {\"training_acc\": 0.4921875, \"training_loss\": 267.30072021484375, \"iteration\": 571, \"epoch\": 3}, {\"training_acc\": 0.546875, \"training_loss\": 273.5898742675781, \"iteration\": 572, \"epoch\": 3}, {\"training_acc\": 0.625, \"training_loss\": 287.10333251953125, \"iteration\": 573, \"epoch\": 3}, {\"training_acc\": 0.5390625, \"training_loss\": 279.07171630859375, \"iteration\": 574, \"epoch\": 3}, {\"training_acc\": 0.65625, \"training_loss\": 332.9036865234375, \"iteration\": 575, \"epoch\": 3}, {\"training_acc\": 0.640625, \"training_loss\": 309.60052490234375, \"iteration\": 576, \"epoch\": 3}, {\"training_acc\": 0.625, \"training_loss\": 295.24267578125, \"iteration\": 577, \"epoch\": 3}, {\"training_acc\": 0.59375, \"training_loss\": 301.86419677734375, \"iteration\": 578, \"epoch\": 3}, {\"training_acc\": 0.5390625, \"training_loss\": 260.1169128417969, \"iteration\": 579, \"epoch\": 3}, {\"training_acc\": 0.5703125, \"training_loss\": 272.897216796875, \"iteration\": 580, \"epoch\": 3}, {\"training_acc\": 0.4453125, \"training_loss\": 217.62269592285156, \"iteration\": 581, \"epoch\": 3}, {\"training_acc\": 0.5546875, \"training_loss\": 295.58203125, \"iteration\": 582, \"epoch\": 3}, {\"training_acc\": 0.59375, \"training_loss\": 314.80181884765625, \"iteration\": 583, \"epoch\": 3}, {\"training_acc\": 0.5234375, \"training_loss\": 265.834716796875, \"iteration\": 584, \"epoch\": 3}, {\"training_acc\": 0.53125, \"training_loss\": 265.4652099609375, \"iteration\": 585, \"epoch\": 3}, {\"training_acc\": 0.546875, \"training_loss\": 279.659912109375, \"iteration\": 586, \"epoch\": 3}, {\"training_acc\": 0.59375, \"training_loss\": 303.6353759765625, \"iteration\": 587, \"epoch\": 3}, {\"training_acc\": 0.578125, \"training_loss\": 292.31878662109375, \"iteration\": 588, \"epoch\": 3}, {\"training_acc\": 0.59375, \"training_loss\": 272.2135314941406, \"iteration\": 589, \"epoch\": 3}, {\"training_acc\": 0.484375, \"training_loss\": 238.82102966308594, \"iteration\": 590, \"epoch\": 3}, {\"training_acc\": 0.6015625, \"training_loss\": 309.6759033203125, \"iteration\": 591, \"epoch\": 3}, {\"training_acc\": 0.5546875, \"training_loss\": 278.980712890625, \"iteration\": 592, \"epoch\": 3}, {\"training_acc\": 0.5703125, \"training_loss\": 289.6595458984375, \"iteration\": 593, \"epoch\": 3}, {\"training_acc\": 0.5859375, \"training_loss\": 294.8124694824219, \"iteration\": 594, \"epoch\": 3}, {\"training_acc\": 0.515625, \"training_loss\": 282.7060546875, \"iteration\": 595, \"epoch\": 3}, {\"training_acc\": 0.53125, \"training_loss\": 289.4538269042969, \"iteration\": 596, \"epoch\": 3}, {\"training_acc\": 0.5390625, \"training_loss\": 260.83837890625, \"iteration\": 597, \"epoch\": 3}, {\"training_acc\": 0.578125, \"training_loss\": 274.93194580078125, \"iteration\": 598, \"epoch\": 3}, {\"training_acc\": 0.5859375, \"training_loss\": 258.14385986328125, \"iteration\": 599, \"epoch\": 3}, {\"training_acc\": 0.625, \"training_loss\": 283.70208740234375, \"iteration\": 600, \"epoch\": 3}, {\"training_acc\": 0.6484375, \"training_loss\": 293.257568359375, \"iteration\": 601, \"epoch\": 3}, {\"training_acc\": 0.6484375, \"training_loss\": 297.85894775390625, \"iteration\": 602, \"epoch\": 3}, {\"training_acc\": 0.6171875, \"training_loss\": 279.8130798339844, \"iteration\": 603, \"epoch\": 3}, {\"training_acc\": 0.6484375, \"training_loss\": 298.54193115234375, \"iteration\": 604, \"epoch\": 3}, {\"training_acc\": 0.6640625, \"training_loss\": 321.375732421875, \"iteration\": 605, \"epoch\": 3}, {\"training_acc\": 0.609375, \"training_loss\": 258.07843017578125, \"iteration\": 606, \"epoch\": 3}, {\"training_acc\": 0.6171875, \"training_loss\": 311.81768798828125, \"iteration\": 607, \"epoch\": 3}, {\"training_acc\": 0.6640625, \"training_loss\": 348.13873291015625, \"iteration\": 608, \"epoch\": 3}, {\"training_acc\": 0.5703125, \"training_loss\": 290.29522705078125, \"iteration\": 609, \"epoch\": 3}, {\"training_acc\": 0.421875, \"training_loss\": 220.9380645751953, \"iteration\": 610, \"epoch\": 3}, {\"training_acc\": 0.5234375, \"training_loss\": 282.2502746582031, \"iteration\": 611, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 286.0298156738281, \"iteration\": 612, \"epoch\": 3}, {\"training_acc\": 0.4375, \"training_loss\": 246.45074462890625, \"iteration\": 613, \"epoch\": 3}, {\"training_acc\": 0.59375, \"training_loss\": 346.56744384765625, \"iteration\": 614, \"epoch\": 3}, {\"training_acc\": 0.5, \"training_loss\": 275.0457763671875, \"iteration\": 615, \"epoch\": 3}, {\"training_acc\": 0.625, \"training_loss\": 333.0341491699219, \"iteration\": 616, \"epoch\": 3}, {\"training_acc\": 0.515625, \"training_loss\": 257.74371337890625, \"iteration\": 617, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 275.826171875, \"iteration\": 618, \"epoch\": 3}, {\"training_acc\": 0.609375, \"training_loss\": 289.459716796875, \"iteration\": 619, \"epoch\": 3}, {\"training_acc\": 0.6171875, \"training_loss\": 271.5499267578125, \"iteration\": 620, \"epoch\": 3}, {\"training_acc\": 0.671875, \"training_loss\": 299.835693359375, \"iteration\": 621, \"epoch\": 3}, {\"training_acc\": 0.7109375, \"training_loss\": 294.4329528808594, \"iteration\": 622, \"epoch\": 3}, {\"training_acc\": 0.65625, \"training_loss\": 308.16351318359375, \"iteration\": 623, \"epoch\": 3}, {\"training_acc\": 0.6953125, \"training_loss\": 292.900146484375, \"iteration\": 624, \"epoch\": 3}, {\"training_acc\": 0.6875, \"training_loss\": 246.95101928710938, \"iteration\": 625, \"epoch\": 3}, {\"training_acc\": 0.6953125, \"training_loss\": 280.6954345703125, \"iteration\": 626, \"epoch\": 3}, {\"training_acc\": 0.703125, \"training_loss\": 280.1100158691406, \"iteration\": 627, \"epoch\": 3}, {\"training_acc\": 0.71875, \"training_loss\": 299.6697082519531, \"iteration\": 628, \"epoch\": 3}, {\"training_acc\": 0.6171875, \"training_loss\": 257.50909423828125, \"iteration\": 629, \"epoch\": 3}, {\"training_acc\": 0.5859375, \"training_loss\": 276.241943359375, \"iteration\": 630, \"epoch\": 3}, {\"training_acc\": 0.5234375, \"training_loss\": 250.41761779785156, \"iteration\": 631, \"epoch\": 3}, {\"training_acc\": 0.4921875, \"training_loss\": 244.26126098632812, \"iteration\": 632, \"epoch\": 3}, {\"training_acc\": 0.5546875, \"training_loss\": 291.890380859375, \"iteration\": 633, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 282.7438049316406, \"iteration\": 634, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 255.1158905029297, \"iteration\": 635, \"epoch\": 3}, {\"training_acc\": 0.59375, \"training_loss\": 324.6286315917969, \"iteration\": 636, \"epoch\": 3}, {\"training_acc\": 0.5390625, \"training_loss\": 291.1495361328125, \"iteration\": 637, \"epoch\": 3}, {\"training_acc\": 0.546875, \"training_loss\": 284.72369384765625, \"iteration\": 638, \"epoch\": 3}, {\"training_acc\": 0.5, \"training_loss\": 267.07452392578125, \"iteration\": 639, \"epoch\": 3}, {\"training_acc\": 0.546875, \"training_loss\": 307.4451904296875, \"iteration\": 640, \"epoch\": 3}, {\"training_acc\": 0.5859375, \"training_loss\": 323.9391784667969, \"iteration\": 641, \"epoch\": 3}, {\"training_acc\": 0.4765625, \"training_loss\": 272.0702209472656, \"iteration\": 642, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 287.036865234375, \"iteration\": 643, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 294.3634033203125, \"iteration\": 644, \"epoch\": 3}, {\"training_acc\": 0.5234375, \"training_loss\": 303.7477111816406, \"iteration\": 645, \"epoch\": 3}, {\"training_acc\": 0.546875, \"training_loss\": 310.59332275390625, \"iteration\": 646, \"epoch\": 3}, {\"training_acc\": 0.453125, \"training_loss\": 255.09500122070312, \"iteration\": 647, \"epoch\": 3}, {\"training_acc\": 0.53125, \"training_loss\": 300.2223815917969, \"iteration\": 648, \"epoch\": 3}, {\"training_acc\": 0.484375, \"training_loss\": 267.6050109863281, \"iteration\": 649, \"epoch\": 3}, {\"training_acc\": 0.46875, \"training_loss\": 265.82330322265625, \"iteration\": 650, \"epoch\": 3}, {\"training_acc\": 0.515625, \"training_loss\": 290.5152282714844, \"iteration\": 651, \"epoch\": 3}, {\"training_acc\": 0.4765625, \"training_loss\": 270.73553466796875, \"iteration\": 652, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 320.142578125, \"iteration\": 653, \"epoch\": 3}, {\"training_acc\": 0.59375, \"training_loss\": 338.61083984375, \"iteration\": 654, \"epoch\": 3}, {\"training_acc\": 0.578125, \"training_loss\": 326.3909912109375, \"iteration\": 655, \"epoch\": 3}, {\"training_acc\": 0.5078125, \"training_loss\": 283.3515319824219, \"iteration\": 656, \"epoch\": 3}, {\"training_acc\": 0.53125, \"training_loss\": 282.94677734375, \"iteration\": 657, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 303.987548828125, \"iteration\": 658, \"epoch\": 3}, {\"training_acc\": 0.546875, \"training_loss\": 307.8436279296875, \"iteration\": 659, \"epoch\": 3}, {\"training_acc\": 0.5234375, \"training_loss\": 283.19512939453125, \"iteration\": 660, \"epoch\": 3}, {\"training_acc\": 0.5390625, \"training_loss\": 308.6011962890625, \"iteration\": 661, \"epoch\": 3}, {\"training_acc\": 0.53125, \"training_loss\": 295.6641540527344, \"iteration\": 662, \"epoch\": 3}, {\"training_acc\": 0.5625, \"training_loss\": 328.0179443359375, \"iteration\": 663, \"epoch\": 3}, {\"training_acc\": 0.5, \"training_loss\": 275.54510498046875, \"iteration\": 664, \"epoch\": 3}, {\"training_acc\": 0.5, \"training_loss\": 274.23712158203125, \"iteration\": 665, \"epoch\": 3}, {\"training_acc\": 0.5102040816326531, \"training_loss\": 84.48069763183594, \"iteration\": 666, \"epoch\": 3}, {\"training_acc\": 0.5234375, \"training_loss\": 274.6703796386719, \"iteration\": 667, \"epoch\": 4}, {\"training_acc\": 0.5625, \"training_loss\": 266.5604248046875, \"iteration\": 668, \"epoch\": 4}, {\"training_acc\": 0.671875, \"training_loss\": 294.80963134765625, \"iteration\": 669, \"epoch\": 4}, {\"training_acc\": 0.6796875, \"training_loss\": 292.4583740234375, \"iteration\": 670, \"epoch\": 4}, {\"training_acc\": 0.6640625, \"training_loss\": 273.9975891113281, \"iteration\": 671, \"epoch\": 4}, {\"training_acc\": 0.71875, \"training_loss\": 269.0223083496094, \"iteration\": 672, \"epoch\": 4}, {\"training_acc\": 0.6953125, \"training_loss\": 313.73480224609375, \"iteration\": 673, \"epoch\": 4}, {\"training_acc\": 0.7265625, \"training_loss\": 258.41680908203125, \"iteration\": 674, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 270.94268798828125, \"iteration\": 675, \"epoch\": 4}, {\"training_acc\": 0.765625, \"training_loss\": 296.77001953125, \"iteration\": 676, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 311.5368957519531, \"iteration\": 677, \"epoch\": 4}, {\"training_acc\": 0.703125, \"training_loss\": 261.6029052734375, \"iteration\": 678, \"epoch\": 4}, {\"training_acc\": 0.796875, \"training_loss\": 279.8662414550781, \"iteration\": 679, \"epoch\": 4}, {\"training_acc\": 0.796875, \"training_loss\": 326.65985107421875, \"iteration\": 680, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 237.178955078125, \"iteration\": 681, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 327.6360168457031, \"iteration\": 682, \"epoch\": 4}, {\"training_acc\": 0.6953125, \"training_loss\": 283.9503173828125, \"iteration\": 683, \"epoch\": 4}, {\"training_acc\": 0.5, \"training_loss\": 256.12646484375, \"iteration\": 684, \"epoch\": 4}, {\"training_acc\": 0.5703125, \"training_loss\": 287.22052001953125, \"iteration\": 685, \"epoch\": 4}, {\"training_acc\": 0.6484375, \"training_loss\": 330.050048828125, \"iteration\": 686, \"epoch\": 4}, {\"training_acc\": 0.5390625, \"training_loss\": 281.40850830078125, \"iteration\": 687, \"epoch\": 4}, {\"training_acc\": 0.5703125, \"training_loss\": 289.7603759765625, \"iteration\": 688, \"epoch\": 4}, {\"training_acc\": 0.515625, \"training_loss\": 246.47476196289062, \"iteration\": 689, \"epoch\": 4}, {\"training_acc\": 0.59375, \"training_loss\": 273.86077880859375, \"iteration\": 690, \"epoch\": 4}, {\"training_acc\": 0.53125, \"training_loss\": 219.92703247070312, \"iteration\": 691, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 316.168701171875, \"iteration\": 692, \"epoch\": 4}, {\"training_acc\": 0.59375, \"training_loss\": 297.28094482421875, \"iteration\": 693, \"epoch\": 4}, {\"training_acc\": 0.5859375, \"training_loss\": 267.30828857421875, \"iteration\": 694, \"epoch\": 4}, {\"training_acc\": 0.6484375, \"training_loss\": 267.8797302246094, \"iteration\": 695, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 313.4491271972656, \"iteration\": 696, \"epoch\": 4}, {\"training_acc\": 0.5546875, \"training_loss\": 265.0949401855469, \"iteration\": 697, \"epoch\": 4}, {\"training_acc\": 0.5390625, \"training_loss\": 264.3467102050781, \"iteration\": 698, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 320.1231994628906, \"iteration\": 699, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 295.3974914550781, \"iteration\": 700, \"epoch\": 4}, {\"training_acc\": 0.578125, \"training_loss\": 262.6243896484375, \"iteration\": 701, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 272.974853515625, \"iteration\": 702, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 241.08946228027344, \"iteration\": 703, \"epoch\": 4}, {\"training_acc\": 0.609375, \"training_loss\": 268.7874755859375, \"iteration\": 704, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 239.1474151611328, \"iteration\": 705, \"epoch\": 4}, {\"training_acc\": 0.609375, \"training_loss\": 235.854736328125, \"iteration\": 706, \"epoch\": 4}, {\"training_acc\": 0.6953125, \"training_loss\": 333.74658203125, \"iteration\": 707, \"epoch\": 4}, {\"training_acc\": 0.6796875, \"training_loss\": 296.7104187011719, \"iteration\": 708, \"epoch\": 4}, {\"training_acc\": 0.546875, \"training_loss\": 272.91595458984375, \"iteration\": 709, \"epoch\": 4}, {\"training_acc\": 0.578125, \"training_loss\": 278.9400939941406, \"iteration\": 710, \"epoch\": 4}, {\"training_acc\": 0.515625, \"training_loss\": 249.56480407714844, \"iteration\": 711, \"epoch\": 4}, {\"training_acc\": 0.546875, \"training_loss\": 221.61666870117188, \"iteration\": 712, \"epoch\": 4}, {\"training_acc\": 0.6484375, \"training_loss\": 308.2533874511719, \"iteration\": 713, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 277.69036865234375, \"iteration\": 714, \"epoch\": 4}, {\"training_acc\": 0.578125, \"training_loss\": 258.52557373046875, \"iteration\": 715, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 273.1470947265625, \"iteration\": 716, \"epoch\": 4}, {\"training_acc\": 0.6484375, \"training_loss\": 277.94305419921875, \"iteration\": 717, \"epoch\": 4}, {\"training_acc\": 0.6875, \"training_loss\": 322.2982177734375, \"iteration\": 718, \"epoch\": 4}, {\"training_acc\": 0.703125, \"training_loss\": 290.8967590332031, \"iteration\": 719, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 292.3916015625, \"iteration\": 720, \"epoch\": 4}, {\"training_acc\": 0.671875, \"training_loss\": 258.93084716796875, \"iteration\": 721, \"epoch\": 4}, {\"training_acc\": 0.7578125, \"training_loss\": 271.46136474609375, \"iteration\": 722, \"epoch\": 4}, {\"training_acc\": 0.734375, \"training_loss\": 301.67584228515625, \"iteration\": 723, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 271.03643798828125, \"iteration\": 724, \"epoch\": 4}, {\"training_acc\": 0.71875, \"training_loss\": 310.25537109375, \"iteration\": 725, \"epoch\": 4}, {\"training_acc\": 0.640625, \"training_loss\": 263.5081787109375, \"iteration\": 726, \"epoch\": 4}, {\"training_acc\": 0.6875, \"training_loss\": 297.3155212402344, \"iteration\": 727, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 288.8890075683594, \"iteration\": 728, \"epoch\": 4}, {\"training_acc\": 0.5625, \"training_loss\": 289.94677734375, \"iteration\": 729, \"epoch\": 4}, {\"training_acc\": 0.578125, \"training_loss\": 301.20086669921875, \"iteration\": 730, \"epoch\": 4}, {\"training_acc\": 0.5625, \"training_loss\": 287.0580749511719, \"iteration\": 731, \"epoch\": 4}, {\"training_acc\": 0.46875, \"training_loss\": 209.2312774658203, \"iteration\": 732, \"epoch\": 4}, {\"training_acc\": 0.59375, \"training_loss\": 263.1981201171875, \"iteration\": 733, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 302.6509094238281, \"iteration\": 734, \"epoch\": 4}, {\"training_acc\": 0.5625, \"training_loss\": 239.0312957763672, \"iteration\": 735, \"epoch\": 4}, {\"training_acc\": 0.59375, \"training_loss\": 274.6373596191406, \"iteration\": 736, \"epoch\": 4}, {\"training_acc\": 0.65625, \"training_loss\": 291.1810302734375, \"iteration\": 737, \"epoch\": 4}, {\"training_acc\": 0.640625, \"training_loss\": 206.79302978515625, \"iteration\": 738, \"epoch\": 4}, {\"training_acc\": 0.71875, \"training_loss\": 299.088134765625, \"iteration\": 739, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 279.2101745605469, \"iteration\": 740, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 285.8167724609375, \"iteration\": 741, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 271.4629211425781, \"iteration\": 742, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 295.38433837890625, \"iteration\": 743, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 253.6727294921875, \"iteration\": 744, \"epoch\": 4}, {\"training_acc\": 0.578125, \"training_loss\": 293.52813720703125, \"iteration\": 745, \"epoch\": 4}, {\"training_acc\": 0.578125, \"training_loss\": 277.2505187988281, \"iteration\": 746, \"epoch\": 4}, {\"training_acc\": 0.546875, \"training_loss\": 273.8697204589844, \"iteration\": 747, \"epoch\": 4}, {\"training_acc\": 0.546875, \"training_loss\": 282.3539123535156, \"iteration\": 748, \"epoch\": 4}, {\"training_acc\": 0.5703125, \"training_loss\": 260.32037353515625, \"iteration\": 749, \"epoch\": 4}, {\"training_acc\": 0.5625, \"training_loss\": 258.7179870605469, \"iteration\": 750, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 276.4361267089844, \"iteration\": 751, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 300.5171203613281, \"iteration\": 752, \"epoch\": 4}, {\"training_acc\": 0.578125, \"training_loss\": 308.4754943847656, \"iteration\": 753, \"epoch\": 4}, {\"training_acc\": 0.4765625, \"training_loss\": 213.99258422851562, \"iteration\": 754, \"epoch\": 4}, {\"training_acc\": 0.546875, \"training_loss\": 263.606201171875, \"iteration\": 755, \"epoch\": 4}, {\"training_acc\": 0.6484375, \"training_loss\": 282.4765625, \"iteration\": 756, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 285.4070129394531, \"iteration\": 757, \"epoch\": 4}, {\"training_acc\": 0.5859375, \"training_loss\": 243.05039978027344, \"iteration\": 758, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 255.2565155029297, \"iteration\": 759, \"epoch\": 4}, {\"training_acc\": 0.609375, \"training_loss\": 269.8606262207031, \"iteration\": 760, \"epoch\": 4}, {\"training_acc\": 0.6796875, \"training_loss\": 252.1019287109375, \"iteration\": 761, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 278.92169189453125, \"iteration\": 762, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 275.2030944824219, \"iteration\": 763, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 290.39129638671875, \"iteration\": 764, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 280.2104187011719, \"iteration\": 765, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 291.90118408203125, \"iteration\": 766, \"epoch\": 4}, {\"training_acc\": 0.5546875, \"training_loss\": 255.48594665527344, \"iteration\": 767, \"epoch\": 4}, {\"training_acc\": 0.5390625, \"training_loss\": 233.61056518554688, \"iteration\": 768, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 307.26385498046875, \"iteration\": 769, \"epoch\": 4}, {\"training_acc\": 0.5859375, \"training_loss\": 252.9238739013672, \"iteration\": 770, \"epoch\": 4}, {\"training_acc\": 0.59375, \"training_loss\": 237.33236694335938, \"iteration\": 771, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 239.33494567871094, \"iteration\": 772, \"epoch\": 4}, {\"training_acc\": 0.71875, \"training_loss\": 308.556884765625, \"iteration\": 773, \"epoch\": 4}, {\"training_acc\": 0.7265625, \"training_loss\": 284.52447509765625, \"iteration\": 774, \"epoch\": 4}, {\"training_acc\": 0.6875, \"training_loss\": 294.4977722167969, \"iteration\": 775, \"epoch\": 4}, {\"training_acc\": 0.7109375, \"training_loss\": 283.34765625, \"iteration\": 776, \"epoch\": 4}, {\"training_acc\": 0.6796875, \"training_loss\": 302.6163330078125, \"iteration\": 777, \"epoch\": 4}, {\"training_acc\": 0.7109375, \"training_loss\": 300.3502197265625, \"iteration\": 778, \"epoch\": 4}, {\"training_acc\": 0.6875, \"training_loss\": 297.930908203125, \"iteration\": 779, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 284.5124816894531, \"iteration\": 780, \"epoch\": 4}, {\"training_acc\": 0.5859375, \"training_loss\": 297.7998046875, \"iteration\": 781, \"epoch\": 4}, {\"training_acc\": 0.4921875, \"training_loss\": 218.01266479492188, \"iteration\": 782, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 287.826416015625, \"iteration\": 783, \"epoch\": 4}, {\"training_acc\": 0.5859375, \"training_loss\": 274.5600891113281, \"iteration\": 784, \"epoch\": 4}, {\"training_acc\": 0.65625, \"training_loss\": 314.9544677734375, \"iteration\": 785, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 275.52056884765625, \"iteration\": 786, \"epoch\": 4}, {\"training_acc\": 0.640625, \"training_loss\": 311.6522216796875, \"iteration\": 787, \"epoch\": 4}, {\"training_acc\": 0.609375, \"training_loss\": 270.9548034667969, \"iteration\": 788, \"epoch\": 4}, {\"training_acc\": 0.59375, \"training_loss\": 310.77008056640625, \"iteration\": 789, \"epoch\": 4}, {\"training_acc\": 0.546875, \"training_loss\": 255.7395782470703, \"iteration\": 790, \"epoch\": 4}, {\"training_acc\": 0.5234375, \"training_loss\": 218.52346801757812, \"iteration\": 791, \"epoch\": 4}, {\"training_acc\": 0.59375, \"training_loss\": 270.0, \"iteration\": 792, \"epoch\": 4}, {\"training_acc\": 0.5859375, \"training_loss\": 237.7537841796875, \"iteration\": 793, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 248.6927032470703, \"iteration\": 794, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 293.8863525390625, \"iteration\": 795, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 274.68646240234375, \"iteration\": 796, \"epoch\": 4}, {\"training_acc\": 0.5546875, \"training_loss\": 251.68460083007812, \"iteration\": 797, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 272.9691162109375, \"iteration\": 798, \"epoch\": 4}, {\"training_acc\": 0.5078125, \"training_loss\": 255.73143005371094, \"iteration\": 799, \"epoch\": 4}, {\"training_acc\": 0.53125, \"training_loss\": 270.8188781738281, \"iteration\": 800, \"epoch\": 4}, {\"training_acc\": 0.53125, \"training_loss\": 270.2010498046875, \"iteration\": 801, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 301.5535888671875, \"iteration\": 802, \"epoch\": 4}, {\"training_acc\": 0.53125, \"training_loss\": 245.92662048339844, \"iteration\": 803, \"epoch\": 4}, {\"training_acc\": 0.5625, \"training_loss\": 278.67584228515625, \"iteration\": 804, \"epoch\": 4}, {\"training_acc\": 0.640625, \"training_loss\": 343.0962829589844, \"iteration\": 805, \"epoch\": 4}, {\"training_acc\": 0.5234375, \"training_loss\": 245.1943359375, \"iteration\": 806, \"epoch\": 4}, {\"training_acc\": 0.5703125, \"training_loss\": 265.87225341796875, \"iteration\": 807, \"epoch\": 4}, {\"training_acc\": 0.671875, \"training_loss\": 338.9650573730469, \"iteration\": 808, \"epoch\": 4}, {\"training_acc\": 0.515625, \"training_loss\": 218.74244689941406, \"iteration\": 809, \"epoch\": 4}, {\"training_acc\": 0.609375, \"training_loss\": 244.3831329345703, \"iteration\": 810, \"epoch\": 4}, {\"training_acc\": 0.5625, \"training_loss\": 252.04061889648438, \"iteration\": 811, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 309.07452392578125, \"iteration\": 812, \"epoch\": 4}, {\"training_acc\": 0.6484375, \"training_loss\": 282.5290222167969, \"iteration\": 813, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 311.7991638183594, \"iteration\": 814, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 252.79306030273438, \"iteration\": 815, \"epoch\": 4}, {\"training_acc\": 0.703125, \"training_loss\": 311.97796630859375, \"iteration\": 816, \"epoch\": 4}, {\"training_acc\": 0.578125, \"training_loss\": 262.46148681640625, \"iteration\": 817, \"epoch\": 4}, {\"training_acc\": 0.6875, \"training_loss\": 345.6476135253906, \"iteration\": 818, \"epoch\": 4}, {\"training_acc\": 0.671875, \"training_loss\": 301.1251220703125, \"iteration\": 819, \"epoch\": 4}, {\"training_acc\": 0.53125, \"training_loss\": 246.43310546875, \"iteration\": 820, \"epoch\": 4}, {\"training_acc\": 0.5703125, \"training_loss\": 236.0799102783203, \"iteration\": 821, \"epoch\": 4}, {\"training_acc\": 0.5859375, \"training_loss\": 271.0335693359375, \"iteration\": 822, \"epoch\": 4}, {\"training_acc\": 0.671875, \"training_loss\": 289.6182861328125, \"iteration\": 823, \"epoch\": 4}, {\"training_acc\": 0.59375, \"training_loss\": 249.25192260742188, \"iteration\": 824, \"epoch\": 4}, {\"training_acc\": 0.609375, \"training_loss\": 268.31195068359375, \"iteration\": 825, \"epoch\": 4}, {\"training_acc\": 0.5546875, \"training_loss\": 250.14501953125, \"iteration\": 826, \"epoch\": 4}, {\"training_acc\": 0.609375, \"training_loss\": 243.11871337890625, \"iteration\": 827, \"epoch\": 4}, {\"training_acc\": 0.609375, \"training_loss\": 302.39141845703125, \"iteration\": 828, \"epoch\": 4}, {\"training_acc\": 0.640625, \"training_loss\": 313.2353515625, \"iteration\": 829, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 300.48175048828125, \"iteration\": 830, \"epoch\": 4}, {\"training_acc\": 0.5546875, \"training_loss\": 239.18795776367188, \"iteration\": 831, \"epoch\": 4}, {\"training_acc\": 0.546875, \"training_loss\": 265.7314147949219, \"iteration\": 832, \"epoch\": 4}, {\"training_acc\": 0.5546875, \"training_loss\": 275.96221923828125, \"iteration\": 833, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 278.2999572753906, \"iteration\": 834, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 292.09649658203125, \"iteration\": 835, \"epoch\": 4}, {\"training_acc\": 0.5703125, \"training_loss\": 259.7877502441406, \"iteration\": 836, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 282.3019714355469, \"iteration\": 837, \"epoch\": 4}, {\"training_acc\": 0.65625, \"training_loss\": 237.00135803222656, \"iteration\": 838, \"epoch\": 4}, {\"training_acc\": 0.6640625, \"training_loss\": 280.22271728515625, \"iteration\": 839, \"epoch\": 4}, {\"training_acc\": 0.59375, \"training_loss\": 239.33187866210938, \"iteration\": 840, \"epoch\": 4}, {\"training_acc\": 0.59375, \"training_loss\": 242.8406524658203, \"iteration\": 841, \"epoch\": 4}, {\"training_acc\": 0.578125, \"training_loss\": 262.32086181640625, \"iteration\": 842, \"epoch\": 4}, {\"training_acc\": 0.671875, \"training_loss\": 250.90957641601562, \"iteration\": 843, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 280.4073486328125, \"iteration\": 844, \"epoch\": 4}, {\"training_acc\": 0.5546875, \"training_loss\": 253.5146026611328, \"iteration\": 845, \"epoch\": 4}, {\"training_acc\": 0.7109375, \"training_loss\": 336.77691650390625, \"iteration\": 846, \"epoch\": 4}, {\"training_acc\": 0.5625, \"training_loss\": 207.45455932617188, \"iteration\": 847, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 285.63067626953125, \"iteration\": 848, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 218.16448974609375, \"iteration\": 849, \"epoch\": 4}, {\"training_acc\": 0.6796875, \"training_loss\": 291.6357116699219, \"iteration\": 850, \"epoch\": 4}, {\"training_acc\": 0.7109375, \"training_loss\": 296.73419189453125, \"iteration\": 851, \"epoch\": 4}, {\"training_acc\": 0.6484375, \"training_loss\": 280.08245849609375, \"iteration\": 852, \"epoch\": 4}, {\"training_acc\": 0.609375, \"training_loss\": 236.2393798828125, \"iteration\": 853, \"epoch\": 4}, {\"training_acc\": 0.6640625, \"training_loss\": 292.8016052246094, \"iteration\": 854, \"epoch\": 4}, {\"training_acc\": 0.6484375, \"training_loss\": 268.333740234375, \"iteration\": 855, \"epoch\": 4}, {\"training_acc\": 0.5703125, \"training_loss\": 250.79254150390625, \"iteration\": 856, \"epoch\": 4}, {\"training_acc\": 0.6484375, \"training_loss\": 265.1507568359375, \"iteration\": 857, \"epoch\": 4}, {\"training_acc\": 0.578125, \"training_loss\": 257.5887145996094, \"iteration\": 858, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 307.5447692871094, \"iteration\": 859, \"epoch\": 4}, {\"training_acc\": 0.65625, \"training_loss\": 313.32489013671875, \"iteration\": 860, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 276.0452880859375, \"iteration\": 861, \"epoch\": 4}, {\"training_acc\": 0.65625, \"training_loss\": 298.1808166503906, \"iteration\": 862, \"epoch\": 4}, {\"training_acc\": 0.6875, \"training_loss\": 316.0685729980469, \"iteration\": 863, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 264.19708251953125, \"iteration\": 864, \"epoch\": 4}, {\"training_acc\": 0.609375, \"training_loss\": 276.53759765625, \"iteration\": 865, \"epoch\": 4}, {\"training_acc\": 0.6953125, \"training_loss\": 318.6442565917969, \"iteration\": 866, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 291.8297119140625, \"iteration\": 867, \"epoch\": 4}, {\"training_acc\": 0.703125, \"training_loss\": 276.7704772949219, \"iteration\": 868, \"epoch\": 4}, {\"training_acc\": 0.7109375, \"training_loss\": 327.2432861328125, \"iteration\": 869, \"epoch\": 4}, {\"training_acc\": 0.6796875, \"training_loss\": 279.1089782714844, \"iteration\": 870, \"epoch\": 4}, {\"training_acc\": 0.6484375, \"training_loss\": 257.331787109375, \"iteration\": 871, \"epoch\": 4}, {\"training_acc\": 0.640625, \"training_loss\": 255.29698181152344, \"iteration\": 872, \"epoch\": 4}, {\"training_acc\": 0.71875, \"training_loss\": 295.39239501953125, \"iteration\": 873, \"epoch\": 4}, {\"training_acc\": 0.703125, \"training_loss\": 295.0594177246094, \"iteration\": 874, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 242.22900390625, \"iteration\": 875, \"epoch\": 4}, {\"training_acc\": 0.6484375, \"training_loss\": 288.1546325683594, \"iteration\": 876, \"epoch\": 4}, {\"training_acc\": 0.5390625, \"training_loss\": 209.829833984375, \"iteration\": 877, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 263.0082702636719, \"iteration\": 878, \"epoch\": 4}, {\"training_acc\": 0.6171875, \"training_loss\": 231.9560089111328, \"iteration\": 879, \"epoch\": 4}, {\"training_acc\": 0.71875, \"training_loss\": 336.52227783203125, \"iteration\": 880, \"epoch\": 4}, {\"training_acc\": 0.6796875, \"training_loss\": 317.8128662109375, \"iteration\": 881, \"epoch\": 4}, {\"training_acc\": 0.6953125, \"training_loss\": 300.95220947265625, \"iteration\": 882, \"epoch\": 4}, {\"training_acc\": 0.640625, \"training_loss\": 286.1085510253906, \"iteration\": 883, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 289.597900390625, \"iteration\": 884, \"epoch\": 4}, {\"training_acc\": 0.625, \"training_loss\": 281.5589294433594, \"iteration\": 885, \"epoch\": 4}, {\"training_acc\": 0.6015625, \"training_loss\": 261.50421142578125, \"iteration\": 886, \"epoch\": 4}, {\"training_acc\": 0.6328125, \"training_loss\": 300.77789306640625, \"iteration\": 887, \"epoch\": 4}, {\"training_acc\": 0.5306122448979592, \"training_loss\": 73.703369140625, \"iteration\": 888, \"epoch\": 4}, {\"training_acc\": 0.5546875, \"training_loss\": 267.99176025390625, \"iteration\": 889, \"epoch\": 5}, {\"training_acc\": 0.6171875, \"training_loss\": 293.6545715332031, \"iteration\": 890, \"epoch\": 5}, {\"training_acc\": 0.6328125, \"training_loss\": 314.3815002441406, \"iteration\": 891, \"epoch\": 5}, {\"training_acc\": 0.6015625, \"training_loss\": 262.3554992675781, \"iteration\": 892, \"epoch\": 5}, {\"training_acc\": 0.671875, \"training_loss\": 311.6258239746094, \"iteration\": 893, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 261.3900146484375, \"iteration\": 894, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 344.21636962890625, \"iteration\": 895, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 302.86639404296875, \"iteration\": 896, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 278.97723388671875, \"iteration\": 897, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 289.3919677734375, \"iteration\": 898, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 264.302001953125, \"iteration\": 899, \"epoch\": 5}, {\"training_acc\": 0.6953125, \"training_loss\": 265.51287841796875, \"iteration\": 900, \"epoch\": 5}, {\"training_acc\": 0.6796875, \"training_loss\": 255.72288513183594, \"iteration\": 901, \"epoch\": 5}, {\"training_acc\": 0.6796875, \"training_loss\": 289.5309143066406, \"iteration\": 902, \"epoch\": 5}, {\"training_acc\": 0.703125, \"training_loss\": 249.01795959472656, \"iteration\": 903, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 281.82843017578125, \"iteration\": 904, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 277.7359313964844, \"iteration\": 905, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 286.31610107421875, \"iteration\": 906, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 279.2196044921875, \"iteration\": 907, \"epoch\": 5}, {\"training_acc\": 0.90625, \"training_loss\": 266.2802734375, \"iteration\": 908, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 252.57591247558594, \"iteration\": 909, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 240.67161560058594, \"iteration\": 910, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 257.3376770019531, \"iteration\": 911, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 308.2002868652344, \"iteration\": 912, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 246.933349609375, \"iteration\": 913, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 276.32452392578125, \"iteration\": 914, \"epoch\": 5}, {\"training_acc\": 0.7578125, \"training_loss\": 278.751708984375, \"iteration\": 915, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 273.8282775878906, \"iteration\": 916, \"epoch\": 5}, {\"training_acc\": 0.7265625, \"training_loss\": 267.81365966796875, \"iteration\": 917, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 235.9897918701172, \"iteration\": 918, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 273.356201171875, \"iteration\": 919, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 255.80813598632812, \"iteration\": 920, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 295.82464599609375, \"iteration\": 921, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 218.11122131347656, \"iteration\": 922, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 282.7821960449219, \"iteration\": 923, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 267.3269348144531, \"iteration\": 924, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 281.38330078125, \"iteration\": 925, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 308.2264709472656, \"iteration\": 926, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 245.0215301513672, \"iteration\": 927, \"epoch\": 5}, {\"training_acc\": 0.6796875, \"training_loss\": 227.846435546875, \"iteration\": 928, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 295.12860107421875, \"iteration\": 929, \"epoch\": 5}, {\"training_acc\": 0.6796875, \"training_loss\": 225.88705444335938, \"iteration\": 930, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 304.57958984375, \"iteration\": 931, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 255.89120483398438, \"iteration\": 932, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 301.6660461425781, \"iteration\": 933, \"epoch\": 5}, {\"training_acc\": 0.921875, \"training_loss\": 249.39678955078125, \"iteration\": 934, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 279.90216064453125, \"iteration\": 935, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 291.77142333984375, \"iteration\": 936, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 249.76193237304688, \"iteration\": 937, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 261.91033935546875, \"iteration\": 938, \"epoch\": 5}, {\"training_acc\": 0.640625, \"training_loss\": 226.40908813476562, \"iteration\": 939, \"epoch\": 5}, {\"training_acc\": 0.59375, \"training_loss\": 240.56185913085938, \"iteration\": 940, \"epoch\": 5}, {\"training_acc\": 0.6640625, \"training_loss\": 285.2303466796875, \"iteration\": 941, \"epoch\": 5}, {\"training_acc\": 0.671875, \"training_loss\": 233.10006713867188, \"iteration\": 942, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 321.61328125, \"iteration\": 943, \"epoch\": 5}, {\"training_acc\": 0.7265625, \"training_loss\": 249.28179931640625, \"iteration\": 944, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 253.85157775878906, \"iteration\": 945, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 248.8855743408203, \"iteration\": 946, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 297.6705627441406, \"iteration\": 947, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 233.60931396484375, \"iteration\": 948, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 307.16998291015625, \"iteration\": 949, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 293.92559814453125, \"iteration\": 950, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 290.9525451660156, \"iteration\": 951, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 310.24560546875, \"iteration\": 952, \"epoch\": 5}, {\"training_acc\": 0.7578125, \"training_loss\": 228.18453979492188, \"iteration\": 953, \"epoch\": 5}, {\"training_acc\": 0.7265625, \"training_loss\": 240.3579864501953, \"iteration\": 954, \"epoch\": 5}, {\"training_acc\": 0.7265625, \"training_loss\": 340.72930908203125, \"iteration\": 955, \"epoch\": 5}, {\"training_acc\": 0.71875, \"training_loss\": 257.34429931640625, \"iteration\": 956, \"epoch\": 5}, {\"training_acc\": 0.71875, \"training_loss\": 270.93548583984375, \"iteration\": 957, \"epoch\": 5}, {\"training_acc\": 0.6875, \"training_loss\": 279.8988037109375, \"iteration\": 958, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 283.5172119140625, \"iteration\": 959, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 230.6264190673828, \"iteration\": 960, \"epoch\": 5}, {\"training_acc\": 0.7109375, \"training_loss\": 227.59103393554688, \"iteration\": 961, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 276.64892578125, \"iteration\": 962, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 295.13690185546875, \"iteration\": 963, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 289.791015625, \"iteration\": 964, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 272.1512145996094, \"iteration\": 965, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 270.7807922363281, \"iteration\": 966, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 270.4062194824219, \"iteration\": 967, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 255.94656372070312, \"iteration\": 968, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 298.29327392578125, \"iteration\": 969, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 351.74456787109375, \"iteration\": 970, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 310.4810485839844, \"iteration\": 971, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 268.57568359375, \"iteration\": 972, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 251.25567626953125, \"iteration\": 973, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 310.2227783203125, \"iteration\": 974, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 252.807861328125, \"iteration\": 975, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 267.9464111328125, \"iteration\": 976, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 281.6982421875, \"iteration\": 977, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 219.93655395507812, \"iteration\": 978, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 310.51318359375, \"iteration\": 979, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 276.2894287109375, \"iteration\": 980, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 311.670654296875, \"iteration\": 981, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 266.6961975097656, \"iteration\": 982, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 230.565673828125, \"iteration\": 983, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 273.1654968261719, \"iteration\": 984, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 263.11053466796875, \"iteration\": 985, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 288.4461975097656, \"iteration\": 986, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 271.6743469238281, \"iteration\": 987, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 265.485107421875, \"iteration\": 988, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 259.6065673828125, \"iteration\": 989, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 280.7451171875, \"iteration\": 990, \"epoch\": 5}, {\"training_acc\": 0.7578125, \"training_loss\": 267.7545166015625, \"iteration\": 991, \"epoch\": 5}, {\"training_acc\": 0.7265625, \"training_loss\": 277.58575439453125, \"iteration\": 992, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 326.335693359375, \"iteration\": 993, \"epoch\": 5}, {\"training_acc\": 0.703125, \"training_loss\": 292.07061767578125, \"iteration\": 994, \"epoch\": 5}, {\"training_acc\": 0.6953125, \"training_loss\": 230.98133850097656, \"iteration\": 995, \"epoch\": 5}, {\"training_acc\": 0.7265625, \"training_loss\": 273.80999755859375, \"iteration\": 996, \"epoch\": 5}, {\"training_acc\": 0.65625, \"training_loss\": 239.72100830078125, \"iteration\": 997, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 317.01617431640625, \"iteration\": 998, \"epoch\": 5}, {\"training_acc\": 0.703125, \"training_loss\": 272.056640625, \"iteration\": 999, \"epoch\": 5}, {\"training_acc\": 0.625, \"training_loss\": 243.29701232910156, \"iteration\": 1000, \"epoch\": 5}, {\"training_acc\": 0.6796875, \"training_loss\": 225.43112182617188, \"iteration\": 1001, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 302.1154479980469, \"iteration\": 1002, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 274.1900939941406, \"iteration\": 1003, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 301.4922790527344, \"iteration\": 1004, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 256.997802734375, \"iteration\": 1005, \"epoch\": 5}, {\"training_acc\": 0.6953125, \"training_loss\": 243.65036010742188, \"iteration\": 1006, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 307.47235107421875, \"iteration\": 1007, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 307.8421630859375, \"iteration\": 1008, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 251.55166625976562, \"iteration\": 1009, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 245.85696411132812, \"iteration\": 1010, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 229.103759765625, \"iteration\": 1011, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 291.6689758300781, \"iteration\": 1012, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 287.5723571777344, \"iteration\": 1013, \"epoch\": 5}, {\"training_acc\": 0.71875, \"training_loss\": 277.5031433105469, \"iteration\": 1014, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 276.191162109375, \"iteration\": 1015, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 256.84454345703125, \"iteration\": 1016, \"epoch\": 5}, {\"training_acc\": 0.734375, \"training_loss\": 245.8925323486328, \"iteration\": 1017, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 251.6057586669922, \"iteration\": 1018, \"epoch\": 5}, {\"training_acc\": 0.71875, \"training_loss\": 244.2230987548828, \"iteration\": 1019, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 298.4136962890625, \"iteration\": 1020, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 249.61782836914062, \"iteration\": 1021, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 275.1614074707031, \"iteration\": 1022, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 308.5179138183594, \"iteration\": 1023, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 324.739013671875, \"iteration\": 1024, \"epoch\": 5}, {\"training_acc\": 0.7578125, \"training_loss\": 299.31097412109375, \"iteration\": 1025, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 286.64141845703125, \"iteration\": 1026, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 248.85739135742188, \"iteration\": 1027, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 239.99465942382812, \"iteration\": 1028, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 272.9393310546875, \"iteration\": 1029, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 260.734375, \"iteration\": 1030, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 266.1408386230469, \"iteration\": 1031, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 265.5821533203125, \"iteration\": 1032, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 240.2624969482422, \"iteration\": 1033, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 311.50152587890625, \"iteration\": 1034, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 306.8959655761719, \"iteration\": 1035, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 331.79669189453125, \"iteration\": 1036, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 284.1474914550781, \"iteration\": 1037, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 254.85855102539062, \"iteration\": 1038, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 261.4533386230469, \"iteration\": 1039, \"epoch\": 5}, {\"training_acc\": 0.7265625, \"training_loss\": 304.3867492675781, \"iteration\": 1040, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 322.15966796875, \"iteration\": 1041, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 279.2445373535156, \"iteration\": 1042, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 271.44549560546875, \"iteration\": 1043, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 278.3873291015625, \"iteration\": 1044, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 288.7554931640625, \"iteration\": 1045, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 260.2574462890625, \"iteration\": 1046, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 306.617919921875, \"iteration\": 1047, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 228.0701904296875, \"iteration\": 1048, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 261.27166748046875, \"iteration\": 1049, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 282.140625, \"iteration\": 1050, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 267.04693603515625, \"iteration\": 1051, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 276.9847106933594, \"iteration\": 1052, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 255.57423400878906, \"iteration\": 1053, \"epoch\": 5}, {\"training_acc\": 0.734375, \"training_loss\": 285.306640625, \"iteration\": 1054, \"epoch\": 5}, {\"training_acc\": 0.734375, \"training_loss\": 245.92623901367188, \"iteration\": 1055, \"epoch\": 5}, {\"training_acc\": 0.6875, \"training_loss\": 264.7945556640625, \"iteration\": 1056, \"epoch\": 5}, {\"training_acc\": 0.7265625, \"training_loss\": 328.9450988769531, \"iteration\": 1057, \"epoch\": 5}, {\"training_acc\": 0.734375, \"training_loss\": 228.36460876464844, \"iteration\": 1058, \"epoch\": 5}, {\"training_acc\": 0.7578125, \"training_loss\": 314.3686828613281, \"iteration\": 1059, \"epoch\": 5}, {\"training_acc\": 0.7578125, \"training_loss\": 268.1584167480469, \"iteration\": 1060, \"epoch\": 5}, {\"training_acc\": 0.65625, \"training_loss\": 230.74905395507812, \"iteration\": 1061, \"epoch\": 5}, {\"training_acc\": 0.6875, \"training_loss\": 216.52401733398438, \"iteration\": 1062, \"epoch\": 5}, {\"training_acc\": 0.640625, \"training_loss\": 193.68496704101562, \"iteration\": 1063, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 260.8994445800781, \"iteration\": 1064, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 246.00770568847656, \"iteration\": 1065, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 246.5069580078125, \"iteration\": 1066, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 250.3375701904297, \"iteration\": 1067, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 288.2518615722656, \"iteration\": 1068, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 283.993408203125, \"iteration\": 1069, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 280.19244384765625, \"iteration\": 1070, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 287.3491516113281, \"iteration\": 1071, \"epoch\": 5}, {\"training_acc\": 0.734375, \"training_loss\": 217.9127197265625, \"iteration\": 1072, \"epoch\": 5}, {\"training_acc\": 0.703125, \"training_loss\": 270.3041687011719, \"iteration\": 1073, \"epoch\": 5}, {\"training_acc\": 0.734375, \"training_loss\": 242.91506958007812, \"iteration\": 1074, \"epoch\": 5}, {\"training_acc\": 0.734375, \"training_loss\": 271.18603515625, \"iteration\": 1075, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 264.98974609375, \"iteration\": 1076, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 244.73519897460938, \"iteration\": 1077, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 310.797607421875, \"iteration\": 1078, \"epoch\": 5}, {\"training_acc\": 0.7578125, \"training_loss\": 265.56329345703125, \"iteration\": 1079, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 292.6554870605469, \"iteration\": 1080, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 270.70556640625, \"iteration\": 1081, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 221.66285705566406, \"iteration\": 1082, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 248.2245635986328, \"iteration\": 1083, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 280.7139892578125, \"iteration\": 1084, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 330.1556701660156, \"iteration\": 1085, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 269.60028076171875, \"iteration\": 1086, \"epoch\": 5}, {\"training_acc\": 0.7578125, \"training_loss\": 243.0161895751953, \"iteration\": 1087, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 263.89532470703125, \"iteration\": 1088, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 247.1485595703125, \"iteration\": 1089, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 330.51373291015625, \"iteration\": 1090, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 221.62655639648438, \"iteration\": 1091, \"epoch\": 5}, {\"training_acc\": 0.7578125, \"training_loss\": 272.5538635253906, \"iteration\": 1092, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 273.7207946777344, \"iteration\": 1093, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 277.05975341796875, \"iteration\": 1094, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 274.08612060546875, \"iteration\": 1095, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 234.6388397216797, \"iteration\": 1096, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 237.728515625, \"iteration\": 1097, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 285.2251892089844, \"iteration\": 1098, \"epoch\": 5}, {\"training_acc\": 0.7578125, \"training_loss\": 286.7037353515625, \"iteration\": 1099, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 266.692138671875, \"iteration\": 1100, \"epoch\": 5}, {\"training_acc\": 0.734375, \"training_loss\": 251.1065673828125, \"iteration\": 1101, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 315.691162109375, \"iteration\": 1102, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 274.0918273925781, \"iteration\": 1103, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 290.81781005859375, \"iteration\": 1104, \"epoch\": 5}, {\"training_acc\": 0.6953125, \"training_loss\": 196.20584106445312, \"iteration\": 1105, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 284.6263122558594, \"iteration\": 1106, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 238.65188598632812, \"iteration\": 1107, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 298.40234375, \"iteration\": 1108, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 276.677490234375, \"iteration\": 1109, \"epoch\": 5}, {\"training_acc\": 0.8775510204081632, \"training_loss\": 45.57094192504883, \"iteration\": 1110, \"epoch\": 5}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.HConcatChart(...)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_results(model_lstm, train_config, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 222/222 [01:38<00:00,  2.25batch/s, batch_accuracy=0.49, loss=61.3]\n",
            "Epoch 2: 100%|██████████| 222/222 [01:37<00:00,  2.27batch/s, batch_accuracy=0.653, loss=79.5]\n",
            "Epoch 3: 100%|██████████| 222/222 [01:38<00:00,  2.26batch/s, batch_accuracy=0.837, loss=79.8]\n",
            "Epoch 4: 100%|██████████| 222/222 [01:38<00:00,  2.26batch/s, batch_accuracy=0.796, loss=84.6]\n",
            "Epoch 5: 100%|██████████| 222/222 [01:38<00:00,  2.26batch/s, batch_accuracy=0.755, loss=76.1]\n",
            "Epoch 6: 100%|██████████| 222/222 [01:37<00:00,  2.27batch/s, batch_accuracy=0.816, loss=84.2]\n",
            "Epoch 7: 100%|██████████| 222/222 [01:37<00:00,  2.27batch/s, batch_accuracy=0.837, loss=77.6]\n",
            "Epoch 8: 100%|██████████| 222/222 [01:38<00:00,  2.25batch/s, batch_accuracy=0.796, loss=60.8]\n",
            "Epoch 9: 100%|██████████| 222/222 [01:37<00:00,  2.28batch/s, batch_accuracy=0.776, loss=87.5]\n",
            "Epoch 10: 100%|██████████| 222/222 [01:37<00:00,  2.29batch/s, batch_accuracy=0.796, loss=74.4]\n",
            "100%|██████████| 71/71 [00:16<00:00,  4.43batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.6836327314376831, 0.9433239102363586, 0.7662816643714905)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Prepare baseline config\n",
        "gru_train_config = TrainConfig(\n",
        "    optimizer_params = {'lr': 0.01},\n",
        "    num_epochs       = 10,\n",
        "    early_stop       = False,\n",
        "    violation_limit  = 5\n",
        ")\n",
        "\n",
        "# Train baseline model\n",
        "model_gru = RNNClassifier(\n",
        "    rnn_network         = nn.GRU,\n",
        "    word_embedding_dim  = 32,\n",
        "    hidden_dim          = 64,\n",
        "    bidirectional       = False,\n",
        "    dropout             = 0,\n",
        "    encoder             = train_encoder,\n",
        "    device              = 'cuda'\n",
        ")\n",
        "\n",
        "if Path('models/model_gru.pt').exists():\n",
        "    model_gru = load_model(model_gru, 'model_gru')\n",
        "else:\n",
        "    model_gru.fit(train_dataloader, gru_train_config, no_progress_bar=False)\n",
        "    save_model(model_gru, \"model_gru\")\n",
        "\n",
        "\n",
        "model_gru_result = evaluate_rnn_model(model_gru, test_dataloader, train_encoder)\n",
        "print(model_gru_result)\n",
        "\n",
        "np.save('models/model_gru_results.npy', model_gru_result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-7a0d08bb175b4a68a7991f1ee5009478.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-7a0d08bb175b4a68a7991f1ee5009478.vega-embed details,\n",
              "  #altair-viz-7a0d08bb175b4a68a7991f1ee5009478.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-7a0d08bb175b4a68a7991f1ee5009478\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-7a0d08bb175b4a68a7991f1ee5009478\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-7a0d08bb175b4a68a7991f1ee5009478\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-d1eda0c528c0288fa61287555f41d753\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-d1eda0c528c0288fa61287555f41d753\": [{\"training_acc\": 0.5234375, \"training_loss\": 339.59600830078125, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 336.9617919921875, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 377.3802490234375, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 275.8096008300781, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 244.98631286621094, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 325.0491027832031, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 337.840576171875, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 318.2275390625, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 286.37152099609375, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 289.21466064453125, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 292.2132568359375, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 281.1318359375, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 312.6672668457031, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 303.19219970703125, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 304.8081970214844, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 342.03155517578125, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 298.23382568359375, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.453125, \"training_loss\": 345.5050048828125, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 318.87353515625, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 317.94097900390625, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 299.1771240234375, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 311.51812744140625, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 274.6265869140625, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 294.2221984863281, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 292.7083740234375, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 314.7091369628906, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 303.574462890625, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 292.8836669921875, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 354.7623291015625, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 351.915283203125, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 322.07415771484375, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 293.41552734375, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 313.8338623046875, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 331.89312744140625, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 224.33865356445312, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 302.21490478515625, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 321.76751708984375, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 308.1085510253906, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 241.043701171875, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 287.2477722167969, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 308.94610595703125, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 295.17626953125, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 327.8937683105469, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 322.9924011230469, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 279.6224365234375, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 271.08990478515625, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 296.5057373046875, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 289.8728942871094, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 289.2767333984375, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 313.5668640136719, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 318.673095703125, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 282.87237548828125, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 322.7662658691406, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 293.5877685546875, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 337.80279541015625, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 291.4127197265625, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 312.6101379394531, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 289.5699462890625, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 271.26641845703125, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 279.8577880859375, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 314.0076904296875, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.4453125, \"training_loss\": 343.1014404296875, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 289.43408203125, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 313.90234375, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 308.2658996582031, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.46875, \"training_loss\": 329.0364990234375, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.4453125, \"training_loss\": 347.21124267578125, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 274.7207946777344, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 324.22125244140625, \"iteration\": 69, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 332.61529541015625, \"iteration\": 70, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 268.5947265625, \"iteration\": 71, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 309.14697265625, \"iteration\": 72, \"epoch\": 1}, {\"training_acc\": 0.4609375, \"training_loss\": 348.7087097167969, \"iteration\": 73, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 272.3299255371094, \"iteration\": 74, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 318.931884765625, \"iteration\": 75, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 301.9411926269531, \"iteration\": 76, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 321.13818359375, \"iteration\": 77, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 324.22698974609375, \"iteration\": 78, \"epoch\": 1}, {\"training_acc\": 0.46875, \"training_loss\": 352.8002624511719, \"iteration\": 79, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 287.36932373046875, \"iteration\": 80, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 257.3031005859375, \"iteration\": 81, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 333.34088134765625, \"iteration\": 82, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 292.0556945800781, \"iteration\": 83, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 266.1279296875, \"iteration\": 84, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 320.33599853515625, \"iteration\": 85, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 279.4635925292969, \"iteration\": 86, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 322.9301452636719, \"iteration\": 87, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 319.5679016113281, \"iteration\": 88, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 302.7718505859375, \"iteration\": 89, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 287.3973388671875, \"iteration\": 90, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 301.6376953125, \"iteration\": 91, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 308.283447265625, \"iteration\": 92, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 314.98455810546875, \"iteration\": 93, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 336.818359375, \"iteration\": 94, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 266.84716796875, \"iteration\": 95, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 309.5891418457031, \"iteration\": 96, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 298.24420166015625, \"iteration\": 97, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 289.3236999511719, \"iteration\": 98, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 306.7882080078125, \"iteration\": 99, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 328.37457275390625, \"iteration\": 100, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 273.85418701171875, \"iteration\": 101, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 271.7052917480469, \"iteration\": 102, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 267.00006103515625, \"iteration\": 103, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 278.078857421875, \"iteration\": 104, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 296.26123046875, \"iteration\": 105, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 246.43341064453125, \"iteration\": 106, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 254.1313934326172, \"iteration\": 107, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 286.00921630859375, \"iteration\": 108, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 302.44921875, \"iteration\": 109, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 230.91748046875, \"iteration\": 110, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 296.1346740722656, \"iteration\": 111, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 325.6455078125, \"iteration\": 112, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 308.0463562011719, \"iteration\": 113, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 271.4016418457031, \"iteration\": 114, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 270.4139404296875, \"iteration\": 115, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 287.0196228027344, \"iteration\": 116, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 307.4819641113281, \"iteration\": 117, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 260.2498779296875, \"iteration\": 118, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 297.9742431640625, \"iteration\": 119, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 331.2882080078125, \"iteration\": 120, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 278.8627014160156, \"iteration\": 121, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 283.853759765625, \"iteration\": 122, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 325.61199951171875, \"iteration\": 123, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 307.0650634765625, \"iteration\": 124, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 341.5440673828125, \"iteration\": 125, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 248.8978729248047, \"iteration\": 126, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 306.60809326171875, \"iteration\": 127, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 335.2016906738281, \"iteration\": 128, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 308.16656494140625, \"iteration\": 129, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 297.49127197265625, \"iteration\": 130, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 305.26776123046875, \"iteration\": 131, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 258.65240478515625, \"iteration\": 132, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 286.25799560546875, \"iteration\": 133, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 308.5343322753906, \"iteration\": 134, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 322.72418212890625, \"iteration\": 135, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 296.56890869140625, \"iteration\": 136, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 284.83453369140625, \"iteration\": 137, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 286.4815673828125, \"iteration\": 138, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 280.7431640625, \"iteration\": 139, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 263.89508056640625, \"iteration\": 140, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 237.24253845214844, \"iteration\": 141, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 314.0116882324219, \"iteration\": 142, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 296.2093505859375, \"iteration\": 143, \"epoch\": 1}, {\"training_acc\": 0.484375, \"training_loss\": 265.4068298339844, \"iteration\": 144, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 289.208984375, \"iteration\": 145, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 279.11639404296875, \"iteration\": 146, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 305.041748046875, \"iteration\": 147, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 291.5393371582031, \"iteration\": 148, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 279.7523193359375, \"iteration\": 149, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 255.01181030273438, \"iteration\": 150, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 300.70367431640625, \"iteration\": 151, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 290.9443054199219, \"iteration\": 152, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 310.827880859375, \"iteration\": 153, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 344.8680419921875, \"iteration\": 154, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 339.9103698730469, \"iteration\": 155, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 368.42340087890625, \"iteration\": 156, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 304.2565002441406, \"iteration\": 157, \"epoch\": 1}, {\"training_acc\": 0.4765625, \"training_loss\": 287.803466796875, \"iteration\": 158, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 355.0291748046875, \"iteration\": 159, \"epoch\": 1}, {\"training_acc\": 0.453125, \"training_loss\": 272.11328125, \"iteration\": 160, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 296.2304992675781, \"iteration\": 161, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 275.431884765625, \"iteration\": 162, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 285.6851806640625, \"iteration\": 163, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 287.63916015625, \"iteration\": 164, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 281.754638671875, \"iteration\": 165, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 306.82269287109375, \"iteration\": 166, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 289.86834716796875, \"iteration\": 167, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 334.9845275878906, \"iteration\": 168, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 298.26531982421875, \"iteration\": 169, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 297.6699523925781, \"iteration\": 170, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 250.1199493408203, \"iteration\": 171, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 324.99365234375, \"iteration\": 172, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 298.27618408203125, \"iteration\": 173, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 277.7558288574219, \"iteration\": 174, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 326.69732666015625, \"iteration\": 175, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 261.4847717285156, \"iteration\": 176, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 254.01083374023438, \"iteration\": 177, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 277.0995788574219, \"iteration\": 178, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 301.5035095214844, \"iteration\": 179, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 267.3856201171875, \"iteration\": 180, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 276.62841796875, \"iteration\": 181, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 302.5008850097656, \"iteration\": 182, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 320.19921875, \"iteration\": 183, \"epoch\": 1}, {\"training_acc\": 0.4765625, \"training_loss\": 248.74453735351562, \"iteration\": 184, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 253.32101440429688, \"iteration\": 185, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 246.823974609375, \"iteration\": 186, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 301.70916748046875, \"iteration\": 187, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 284.64898681640625, \"iteration\": 188, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 288.61590576171875, \"iteration\": 189, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 267.370361328125, \"iteration\": 190, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 313.2674560546875, \"iteration\": 191, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 285.51812744140625, \"iteration\": 192, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 350.1134033203125, \"iteration\": 193, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 299.097900390625, \"iteration\": 194, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 265.5215148925781, \"iteration\": 195, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 319.97100830078125, \"iteration\": 196, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 322.88055419921875, \"iteration\": 197, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 270.64111328125, \"iteration\": 198, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 312.78887939453125, \"iteration\": 199, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 286.88922119140625, \"iteration\": 200, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 279.6788330078125, \"iteration\": 201, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 301.9361877441406, \"iteration\": 202, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 300.7537841796875, \"iteration\": 203, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 252.90972900390625, \"iteration\": 204, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 294.141845703125, \"iteration\": 205, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 294.4515686035156, \"iteration\": 206, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 319.3614501953125, \"iteration\": 207, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 255.85133361816406, \"iteration\": 208, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 305.70050048828125, \"iteration\": 209, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 314.4984130859375, \"iteration\": 210, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 302.2537841796875, \"iteration\": 211, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 258.53082275390625, \"iteration\": 212, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 299.6146240234375, \"iteration\": 213, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 349.57958984375, \"iteration\": 214, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 350.341064453125, \"iteration\": 215, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 232.96539306640625, \"iteration\": 216, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 298.69232177734375, \"iteration\": 217, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 322.65545654296875, \"iteration\": 218, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 282.42730712890625, \"iteration\": 219, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 242.27197265625, \"iteration\": 220, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 309.7628173828125, \"iteration\": 221, \"epoch\": 1}, {\"training_acc\": 0.4897959183673469, \"training_loss\": 61.278648376464844, \"iteration\": 222, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 314.50775146484375, \"iteration\": 223, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 298.7729187011719, \"iteration\": 224, \"epoch\": 2}, {\"training_acc\": 0.609375, \"training_loss\": 244.1814727783203, \"iteration\": 225, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 265.6390380859375, \"iteration\": 226, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 256.21636962890625, \"iteration\": 227, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 305.41998291015625, \"iteration\": 228, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 286.66961669921875, \"iteration\": 229, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 228.56900024414062, \"iteration\": 230, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 307.09228515625, \"iteration\": 231, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 282.03070068359375, \"iteration\": 232, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 332.8974609375, \"iteration\": 233, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 334.2361755371094, \"iteration\": 234, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 280.22686767578125, \"iteration\": 235, \"epoch\": 2}, {\"training_acc\": 0.6015625, \"training_loss\": 303.2662658691406, \"iteration\": 236, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 298.3548583984375, \"iteration\": 237, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 348.6624450683594, \"iteration\": 238, \"epoch\": 2}, {\"training_acc\": 0.6796875, \"training_loss\": 260.3157958984375, \"iteration\": 239, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 246.54214477539062, \"iteration\": 240, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 298.0185241699219, \"iteration\": 241, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 316.1561279296875, \"iteration\": 242, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 239.85647583007812, \"iteration\": 243, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 310.34954833984375, \"iteration\": 244, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 296.61871337890625, \"iteration\": 245, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 261.5916748046875, \"iteration\": 246, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 255.7484893798828, \"iteration\": 247, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 245.05897521972656, \"iteration\": 248, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 278.9882507324219, \"iteration\": 249, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 295.26202392578125, \"iteration\": 250, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 347.3119812011719, \"iteration\": 251, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 272.89764404296875, \"iteration\": 252, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 316.76287841796875, \"iteration\": 253, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 284.17218017578125, \"iteration\": 254, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 342.8328857421875, \"iteration\": 255, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 303.22186279296875, \"iteration\": 256, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 237.76683044433594, \"iteration\": 257, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 348.714599609375, \"iteration\": 258, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 324.8768310546875, \"iteration\": 259, \"epoch\": 2}, {\"training_acc\": 0.6640625, \"training_loss\": 276.1678771972656, \"iteration\": 260, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 348.7415466308594, \"iteration\": 261, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 227.05409240722656, \"iteration\": 262, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 355.7257080078125, \"iteration\": 263, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 288.3923645019531, \"iteration\": 264, \"epoch\": 2}, {\"training_acc\": 0.6796875, \"training_loss\": 255.50811767578125, \"iteration\": 265, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 297.68585205078125, \"iteration\": 266, \"epoch\": 2}, {\"training_acc\": 0.6796875, \"training_loss\": 267.7524108886719, \"iteration\": 267, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 273.2493591308594, \"iteration\": 268, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 283.974609375, \"iteration\": 269, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 303.921142578125, \"iteration\": 270, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 269.231689453125, \"iteration\": 271, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 310.39508056640625, \"iteration\": 272, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 303.5452880859375, \"iteration\": 273, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 288.3340759277344, \"iteration\": 274, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 278.96234130859375, \"iteration\": 275, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 264.8628845214844, \"iteration\": 276, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 321.2804870605469, \"iteration\": 277, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 226.61680603027344, \"iteration\": 278, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 263.2010803222656, \"iteration\": 279, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 264.8175964355469, \"iteration\": 280, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 297.50848388671875, \"iteration\": 281, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 258.71697998046875, \"iteration\": 282, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 275.10400390625, \"iteration\": 283, \"epoch\": 2}, {\"training_acc\": 0.6640625, \"training_loss\": 286.3674621582031, \"iteration\": 284, \"epoch\": 2}, {\"training_acc\": 0.59375, \"training_loss\": 271.1302490234375, \"iteration\": 285, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 254.84970092773438, \"iteration\": 286, \"epoch\": 2}, {\"training_acc\": 0.6640625, \"training_loss\": 327.58636474609375, \"iteration\": 287, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 281.98272705078125, \"iteration\": 288, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 275.06622314453125, \"iteration\": 289, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 320.33489990234375, \"iteration\": 290, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 233.46578979492188, \"iteration\": 291, \"epoch\": 2}, {\"training_acc\": 0.6796875, \"training_loss\": 239.18499755859375, \"iteration\": 292, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 246.71310424804688, \"iteration\": 293, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 254.45639038085938, \"iteration\": 294, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 324.44464111328125, \"iteration\": 295, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 301.1043395996094, \"iteration\": 296, \"epoch\": 2}, {\"training_acc\": 0.5859375, \"training_loss\": 277.1830139160156, \"iteration\": 297, \"epoch\": 2}, {\"training_acc\": 0.546875, \"training_loss\": 296.16156005859375, \"iteration\": 298, \"epoch\": 2}, {\"training_acc\": 0.5859375, \"training_loss\": 295.9385986328125, \"iteration\": 299, \"epoch\": 2}, {\"training_acc\": 0.5625, \"training_loss\": 298.96990966796875, \"iteration\": 300, \"epoch\": 2}, {\"training_acc\": 0.59375, \"training_loss\": 309.75970458984375, \"iteration\": 301, \"epoch\": 2}, {\"training_acc\": 0.59375, \"training_loss\": 313.56085205078125, \"iteration\": 302, \"epoch\": 2}, {\"training_acc\": 0.546875, \"training_loss\": 295.093017578125, \"iteration\": 303, \"epoch\": 2}, {\"training_acc\": 0.6484375, \"training_loss\": 307.964111328125, \"iteration\": 304, \"epoch\": 2}, {\"training_acc\": 0.609375, \"training_loss\": 255.82095336914062, \"iteration\": 305, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 246.80734252929688, \"iteration\": 306, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 312.64892578125, \"iteration\": 307, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 226.07870483398438, \"iteration\": 308, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 333.20721435546875, \"iteration\": 309, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 326.062255859375, \"iteration\": 310, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 299.037353515625, \"iteration\": 311, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 320.74822998046875, \"iteration\": 312, \"epoch\": 2}, {\"training_acc\": 0.609375, \"training_loss\": 287.596435546875, \"iteration\": 313, \"epoch\": 2}, {\"training_acc\": 0.53125, \"training_loss\": 237.32781982421875, \"iteration\": 314, \"epoch\": 2}, {\"training_acc\": 0.5390625, \"training_loss\": 272.4633483886719, \"iteration\": 315, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 334.1755676269531, \"iteration\": 316, \"epoch\": 2}, {\"training_acc\": 0.6171875, \"training_loss\": 311.18011474609375, \"iteration\": 317, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 294.29388427734375, \"iteration\": 318, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 333.20196533203125, \"iteration\": 319, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 300.36920166015625, \"iteration\": 320, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 240.414306640625, \"iteration\": 321, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 312.0071716308594, \"iteration\": 322, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 248.15243530273438, \"iteration\": 323, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 325.4561462402344, \"iteration\": 324, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 342.7814025878906, \"iteration\": 325, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 261.4041748046875, \"iteration\": 326, \"epoch\": 2}, {\"training_acc\": 0.65625, \"training_loss\": 310.0426025390625, \"iteration\": 327, \"epoch\": 2}, {\"training_acc\": 0.6015625, \"training_loss\": 260.78778076171875, \"iteration\": 328, \"epoch\": 2}, {\"training_acc\": 0.6171875, \"training_loss\": 263.3341064453125, \"iteration\": 329, \"epoch\": 2}, {\"training_acc\": 0.6640625, \"training_loss\": 262.44500732421875, \"iteration\": 330, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 262.7822265625, \"iteration\": 331, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 296.17620849609375, \"iteration\": 332, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 258.95831298828125, \"iteration\": 333, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 321.01422119140625, \"iteration\": 334, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 231.66079711914062, \"iteration\": 335, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 321.18438720703125, \"iteration\": 336, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 286.4832763671875, \"iteration\": 337, \"epoch\": 2}, {\"training_acc\": 0.65625, \"training_loss\": 284.5102844238281, \"iteration\": 338, \"epoch\": 2}, {\"training_acc\": 0.6796875, \"training_loss\": 283.74395751953125, \"iteration\": 339, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 320.32427978515625, \"iteration\": 340, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 290.76348876953125, \"iteration\": 341, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 316.79876708984375, \"iteration\": 342, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 286.8726501464844, \"iteration\": 343, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 283.05462646484375, \"iteration\": 344, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 297.9071350097656, \"iteration\": 345, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 280.80401611328125, \"iteration\": 346, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 341.7666015625, \"iteration\": 347, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 293.5506591796875, \"iteration\": 348, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 288.3609619140625, \"iteration\": 349, \"epoch\": 2}, {\"training_acc\": 0.6484375, \"training_loss\": 239.81077575683594, \"iteration\": 350, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 310.7021484375, \"iteration\": 351, \"epoch\": 2}, {\"training_acc\": 0.6640625, \"training_loss\": 325.25604248046875, \"iteration\": 352, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 320.5421142578125, \"iteration\": 353, \"epoch\": 2}, {\"training_acc\": 0.5703125, \"training_loss\": 251.7443084716797, \"iteration\": 354, \"epoch\": 2}, {\"training_acc\": 0.65625, \"training_loss\": 282.6788635253906, \"iteration\": 355, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 257.62200927734375, \"iteration\": 356, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 239.5023651123047, \"iteration\": 357, \"epoch\": 2}, {\"training_acc\": 0.6484375, \"training_loss\": 251.64822387695312, \"iteration\": 358, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 260.1076965332031, \"iteration\": 359, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 296.7849426269531, \"iteration\": 360, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 261.11981201171875, \"iteration\": 361, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 296.2398681640625, \"iteration\": 362, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 228.41139221191406, \"iteration\": 363, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 303.9730529785156, \"iteration\": 364, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 298.6658020019531, \"iteration\": 365, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 300.9840087890625, \"iteration\": 366, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 260.7933349609375, \"iteration\": 367, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 300.1859130859375, \"iteration\": 368, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 276.616455078125, \"iteration\": 369, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 257.7445068359375, \"iteration\": 370, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 269.96820068359375, \"iteration\": 371, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 337.04248046875, \"iteration\": 372, \"epoch\": 2}, {\"training_acc\": 0.5703125, \"training_loss\": 257.0791931152344, \"iteration\": 373, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 338.6114196777344, \"iteration\": 374, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 267.2205810546875, \"iteration\": 375, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 305.2937316894531, \"iteration\": 376, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 262.00347900390625, \"iteration\": 377, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 296.7534484863281, \"iteration\": 378, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 305.01617431640625, \"iteration\": 379, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 269.60577392578125, \"iteration\": 380, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 249.721435546875, \"iteration\": 381, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 287.63421630859375, \"iteration\": 382, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 277.916259765625, \"iteration\": 383, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 324.49346923828125, \"iteration\": 384, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 303.10498046875, \"iteration\": 385, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 263.2193298339844, \"iteration\": 386, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 325.3939208984375, \"iteration\": 387, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 291.5169677734375, \"iteration\": 388, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 328.2437744140625, \"iteration\": 389, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 320.2095947265625, \"iteration\": 390, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 263.48675537109375, \"iteration\": 391, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 264.2740478515625, \"iteration\": 392, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 285.5499267578125, \"iteration\": 393, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 279.60369873046875, \"iteration\": 394, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 322.189208984375, \"iteration\": 395, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 322.23956298828125, \"iteration\": 396, \"epoch\": 2}, {\"training_acc\": 0.671875, \"training_loss\": 304.5914611816406, \"iteration\": 397, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 292.5598449707031, \"iteration\": 398, \"epoch\": 2}, {\"training_acc\": 0.65625, \"training_loss\": 252.73377990722656, \"iteration\": 399, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 292.4066162109375, \"iteration\": 400, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 294.0473937988281, \"iteration\": 401, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 243.5922088623047, \"iteration\": 402, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 276.40142822265625, \"iteration\": 403, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 254.83160400390625, \"iteration\": 404, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 325.29656982421875, \"iteration\": 405, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 272.6857604980469, \"iteration\": 406, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 281.1122741699219, \"iteration\": 407, \"epoch\": 2}, {\"training_acc\": 0.6484375, \"training_loss\": 300.73486328125, \"iteration\": 408, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 289.9405517578125, \"iteration\": 409, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 224.06259155273438, \"iteration\": 410, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 331.19293212890625, \"iteration\": 411, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 297.4515380859375, \"iteration\": 412, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 277.24407958984375, \"iteration\": 413, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 249.5270233154297, \"iteration\": 414, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 309.3045654296875, \"iteration\": 415, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 336.87066650390625, \"iteration\": 416, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 252.06239318847656, \"iteration\": 417, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 320.5318298339844, \"iteration\": 418, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 299.39154052734375, \"iteration\": 419, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 246.7606201171875, \"iteration\": 420, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 268.7784118652344, \"iteration\": 421, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 268.53936767578125, \"iteration\": 422, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 252.84283447265625, \"iteration\": 423, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 287.7646789550781, \"iteration\": 424, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 280.7926940917969, \"iteration\": 425, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 314.3212890625, \"iteration\": 426, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 289.677490234375, \"iteration\": 427, \"epoch\": 2}, {\"training_acc\": 0.6640625, \"training_loss\": 244.337890625, \"iteration\": 428, \"epoch\": 2}, {\"training_acc\": 0.6953125, \"training_loss\": 270.2908630371094, \"iteration\": 429, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 272.1031494140625, \"iteration\": 430, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 279.06939697265625, \"iteration\": 431, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 333.4191589355469, \"iteration\": 432, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 274.884033203125, \"iteration\": 433, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 267.58636474609375, \"iteration\": 434, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 294.06256103515625, \"iteration\": 435, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 264.81195068359375, \"iteration\": 436, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 305.55792236328125, \"iteration\": 437, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 285.4634704589844, \"iteration\": 438, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 273.57965087890625, \"iteration\": 439, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 270.6064453125, \"iteration\": 440, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 340.92413330078125, \"iteration\": 441, \"epoch\": 2}, {\"training_acc\": 0.6015625, \"training_loss\": 245.55203247070312, \"iteration\": 442, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 301.71435546875, \"iteration\": 443, \"epoch\": 2}, {\"training_acc\": 0.6530612244897959, \"training_loss\": 79.5328598022461, \"iteration\": 444, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 284.1075134277344, \"iteration\": 445, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 266.4053649902344, \"iteration\": 446, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 241.59466552734375, \"iteration\": 447, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 237.58145141601562, \"iteration\": 448, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 314.19891357421875, \"iteration\": 449, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 296.68267822265625, \"iteration\": 450, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 293.4722900390625, \"iteration\": 451, \"epoch\": 3}, {\"training_acc\": 0.75, \"training_loss\": 323.00457763671875, \"iteration\": 452, \"epoch\": 3}, {\"training_acc\": 0.7109375, \"training_loss\": 299.72467041015625, \"iteration\": 453, \"epoch\": 3}, {\"training_acc\": 0.71875, \"training_loss\": 252.6222381591797, \"iteration\": 454, \"epoch\": 3}, {\"training_acc\": 0.6796875, \"training_loss\": 224.45852661132812, \"iteration\": 455, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 266.421142578125, \"iteration\": 456, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 274.7711486816406, \"iteration\": 457, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 290.8714599609375, \"iteration\": 458, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 315.2788391113281, \"iteration\": 459, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 284.0418395996094, \"iteration\": 460, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 277.97821044921875, \"iteration\": 461, \"epoch\": 3}, {\"training_acc\": 0.765625, \"training_loss\": 234.19158935546875, \"iteration\": 462, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 300.71697998046875, \"iteration\": 463, \"epoch\": 3}, {\"training_acc\": 0.734375, \"training_loss\": 268.07122802734375, \"iteration\": 464, \"epoch\": 3}, {\"training_acc\": 0.7109375, \"training_loss\": 265.06671142578125, \"iteration\": 465, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 248.89678955078125, \"iteration\": 466, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 278.1620178222656, \"iteration\": 467, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 282.5242004394531, \"iteration\": 468, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 239.75436401367188, \"iteration\": 469, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 280.84783935546875, \"iteration\": 470, \"epoch\": 3}, {\"training_acc\": 0.7890625, \"training_loss\": 289.262939453125, \"iteration\": 471, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 260.11932373046875, \"iteration\": 472, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 286.36322021484375, \"iteration\": 473, \"epoch\": 3}, {\"training_acc\": 0.7890625, \"training_loss\": 260.756103515625, \"iteration\": 474, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 253.34364318847656, \"iteration\": 475, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 274.36016845703125, \"iteration\": 476, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 299.89581298828125, \"iteration\": 477, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 241.463623046875, \"iteration\": 478, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 277.1075439453125, \"iteration\": 479, \"epoch\": 3}, {\"training_acc\": 0.734375, \"training_loss\": 257.21124267578125, \"iteration\": 480, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 283.8749694824219, \"iteration\": 481, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 272.0584716796875, \"iteration\": 482, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 272.82696533203125, \"iteration\": 483, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 217.69760131835938, \"iteration\": 484, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 294.57147216796875, \"iteration\": 485, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 283.6661071777344, \"iteration\": 486, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 266.998291015625, \"iteration\": 487, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 255.64862060546875, \"iteration\": 488, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 236.9971923828125, \"iteration\": 489, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 300.8634033203125, \"iteration\": 490, \"epoch\": 3}, {\"training_acc\": 0.7421875, \"training_loss\": 319.186279296875, \"iteration\": 491, \"epoch\": 3}, {\"training_acc\": 0.6171875, \"training_loss\": 254.11529541015625, \"iteration\": 492, \"epoch\": 3}, {\"training_acc\": 0.6640625, \"training_loss\": 296.1607666015625, \"iteration\": 493, \"epoch\": 3}, {\"training_acc\": 0.609375, \"training_loss\": 280.1410827636719, \"iteration\": 494, \"epoch\": 3}, {\"training_acc\": 0.71875, \"training_loss\": 286.80889892578125, \"iteration\": 495, \"epoch\": 3}, {\"training_acc\": 0.7578125, \"training_loss\": 273.9639587402344, \"iteration\": 496, \"epoch\": 3}, {\"training_acc\": 0.7578125, \"training_loss\": 271.54241943359375, \"iteration\": 497, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 307.17266845703125, \"iteration\": 498, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 246.14511108398438, \"iteration\": 499, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 282.6612854003906, \"iteration\": 500, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 319.6655578613281, \"iteration\": 501, \"epoch\": 3}, {\"training_acc\": 0.765625, \"training_loss\": 266.2962646484375, \"iteration\": 502, \"epoch\": 3}, {\"training_acc\": 0.765625, \"training_loss\": 301.94561767578125, \"iteration\": 503, \"epoch\": 3}, {\"training_acc\": 0.703125, \"training_loss\": 309.28204345703125, \"iteration\": 504, \"epoch\": 3}, {\"training_acc\": 0.71875, \"training_loss\": 288.87652587890625, \"iteration\": 505, \"epoch\": 3}, {\"training_acc\": 0.7265625, \"training_loss\": 294.2218017578125, \"iteration\": 506, \"epoch\": 3}, {\"training_acc\": 0.6015625, \"training_loss\": 271.8580322265625, \"iteration\": 507, \"epoch\": 3}, {\"training_acc\": 0.71875, \"training_loss\": 254.37704467773438, \"iteration\": 508, \"epoch\": 3}, {\"training_acc\": 0.7578125, \"training_loss\": 320.70989990234375, \"iteration\": 509, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 269.6164245605469, \"iteration\": 510, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 273.34765625, \"iteration\": 511, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 294.25189208984375, \"iteration\": 512, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 301.54266357421875, \"iteration\": 513, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 256.2216796875, \"iteration\": 514, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 276.971435546875, \"iteration\": 515, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 281.2447509765625, \"iteration\": 516, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 333.8858642578125, \"iteration\": 517, \"epoch\": 3}, {\"training_acc\": 0.75, \"training_loss\": 253.84950256347656, \"iteration\": 518, \"epoch\": 3}, {\"training_acc\": 0.75, \"training_loss\": 280.3058776855469, \"iteration\": 519, \"epoch\": 3}, {\"training_acc\": 0.7265625, \"training_loss\": 273.7486877441406, \"iteration\": 520, \"epoch\": 3}, {\"training_acc\": 0.7265625, \"training_loss\": 315.615234375, \"iteration\": 521, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 324.053466796875, \"iteration\": 522, \"epoch\": 3}, {\"training_acc\": 0.7890625, \"training_loss\": 238.64218139648438, \"iteration\": 523, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 242.39968872070312, \"iteration\": 524, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 233.94268798828125, \"iteration\": 525, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 212.08355712890625, \"iteration\": 526, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 300.1939697265625, \"iteration\": 527, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 273.86572265625, \"iteration\": 528, \"epoch\": 3}, {\"training_acc\": 0.765625, \"training_loss\": 293.00421142578125, \"iteration\": 529, \"epoch\": 3}, {\"training_acc\": 0.7578125, \"training_loss\": 251.404052734375, \"iteration\": 530, \"epoch\": 3}, {\"training_acc\": 0.7265625, \"training_loss\": 253.119140625, \"iteration\": 531, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 264.9405212402344, \"iteration\": 532, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 266.6923828125, \"iteration\": 533, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 315.93084716796875, \"iteration\": 534, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 264.35455322265625, \"iteration\": 535, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 324.96270751953125, \"iteration\": 536, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 289.96295166015625, \"iteration\": 537, \"epoch\": 3}, {\"training_acc\": 0.7421875, \"training_loss\": 281.2028503417969, \"iteration\": 538, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 333.0909423828125, \"iteration\": 539, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 286.94378662109375, \"iteration\": 540, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 263.2591247558594, \"iteration\": 541, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 287.73663330078125, \"iteration\": 542, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 286.994384765625, \"iteration\": 543, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 232.02577209472656, \"iteration\": 544, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 261.6003112792969, \"iteration\": 545, \"epoch\": 3}, {\"training_acc\": 0.7265625, \"training_loss\": 273.02093505859375, \"iteration\": 546, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 304.8348388671875, \"iteration\": 547, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 315.46038818359375, \"iteration\": 548, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 255.46990966796875, \"iteration\": 549, \"epoch\": 3}, {\"training_acc\": 0.7578125, \"training_loss\": 313.23504638671875, \"iteration\": 550, \"epoch\": 3}, {\"training_acc\": 0.7890625, \"training_loss\": 315.2515563964844, \"iteration\": 551, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 283.7462463378906, \"iteration\": 552, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 293.52398681640625, \"iteration\": 553, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 305.79473876953125, \"iteration\": 554, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 288.63360595703125, \"iteration\": 555, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 284.2682800292969, \"iteration\": 556, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 270.7478942871094, \"iteration\": 557, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 280.6009826660156, \"iteration\": 558, \"epoch\": 3}, {\"training_acc\": 0.765625, \"training_loss\": 297.4885559082031, \"iteration\": 559, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 240.1157684326172, \"iteration\": 560, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 256.4319152832031, \"iteration\": 561, \"epoch\": 3}, {\"training_acc\": 0.71875, \"training_loss\": 262.226318359375, \"iteration\": 562, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 235.09567260742188, \"iteration\": 563, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 314.51275634765625, \"iteration\": 564, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 313.21490478515625, \"iteration\": 565, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 295.7066345214844, \"iteration\": 566, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 263.3543701171875, \"iteration\": 567, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 286.2736511230469, \"iteration\": 568, \"epoch\": 3}, {\"training_acc\": 0.7265625, \"training_loss\": 261.5433349609375, \"iteration\": 569, \"epoch\": 3}, {\"training_acc\": 0.765625, \"training_loss\": 288.2396240234375, \"iteration\": 570, \"epoch\": 3}, {\"training_acc\": 0.71875, \"training_loss\": 271.26947021484375, \"iteration\": 571, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 294.9185791015625, \"iteration\": 572, \"epoch\": 3}, {\"training_acc\": 0.734375, \"training_loss\": 265.2947692871094, \"iteration\": 573, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 252.56741333007812, \"iteration\": 574, \"epoch\": 3}, {\"training_acc\": 0.7890625, \"training_loss\": 208.52333068847656, \"iteration\": 575, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 289.3673095703125, \"iteration\": 576, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 283.1041564941406, \"iteration\": 577, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 258.6813659667969, \"iteration\": 578, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 243.27862548828125, \"iteration\": 579, \"epoch\": 3}, {\"training_acc\": 0.7109375, \"training_loss\": 312.8070068359375, \"iteration\": 580, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 309.94573974609375, \"iteration\": 581, \"epoch\": 3}, {\"training_acc\": 0.578125, \"training_loss\": 201.0535125732422, \"iteration\": 582, \"epoch\": 3}, {\"training_acc\": 0.75, \"training_loss\": 282.71038818359375, \"iteration\": 583, \"epoch\": 3}, {\"training_acc\": 0.703125, \"training_loss\": 251.4031524658203, \"iteration\": 584, \"epoch\": 3}, {\"training_acc\": 0.7890625, \"training_loss\": 295.82940673828125, \"iteration\": 585, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 288.2959899902344, \"iteration\": 586, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 269.0799560546875, \"iteration\": 587, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 268.7312927246094, \"iteration\": 588, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 289.64984130859375, \"iteration\": 589, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 224.2286376953125, \"iteration\": 590, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 250.99542236328125, \"iteration\": 591, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 307.650390625, \"iteration\": 592, \"epoch\": 3}, {\"training_acc\": 0.7109375, \"training_loss\": 241.49810791015625, \"iteration\": 593, \"epoch\": 3}, {\"training_acc\": 0.7578125, \"training_loss\": 329.49884033203125, \"iteration\": 594, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 299.93017578125, \"iteration\": 595, \"epoch\": 3}, {\"training_acc\": 0.75, \"training_loss\": 275.32733154296875, \"iteration\": 596, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 321.6948547363281, \"iteration\": 597, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 251.86099243164062, \"iteration\": 598, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 260.19891357421875, \"iteration\": 599, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 304.8360595703125, \"iteration\": 600, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 321.0645751953125, \"iteration\": 601, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 316.02783203125, \"iteration\": 602, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 237.0838165283203, \"iteration\": 603, \"epoch\": 3}, {\"training_acc\": 0.7890625, \"training_loss\": 281.5996398925781, \"iteration\": 604, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 307.60235595703125, \"iteration\": 605, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 264.526123046875, \"iteration\": 606, \"epoch\": 3}, {\"training_acc\": 0.71875, \"training_loss\": 239.23431396484375, \"iteration\": 607, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 316.8269348144531, \"iteration\": 608, \"epoch\": 3}, {\"training_acc\": 0.7421875, \"training_loss\": 215.68954467773438, \"iteration\": 609, \"epoch\": 3}, {\"training_acc\": 0.765625, \"training_loss\": 237.47398376464844, \"iteration\": 610, \"epoch\": 3}, {\"training_acc\": 0.7421875, \"training_loss\": 285.3242492675781, \"iteration\": 611, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 337.9499816894531, \"iteration\": 612, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 305.6842041015625, \"iteration\": 613, \"epoch\": 3}, {\"training_acc\": 0.75, \"training_loss\": 308.3711853027344, \"iteration\": 614, \"epoch\": 3}, {\"training_acc\": 0.703125, \"training_loss\": 291.95855712890625, \"iteration\": 615, \"epoch\": 3}, {\"training_acc\": 0.71875, \"training_loss\": 291.7416076660156, \"iteration\": 616, \"epoch\": 3}, {\"training_acc\": 0.71875, \"training_loss\": 251.56277465820312, \"iteration\": 617, \"epoch\": 3}, {\"training_acc\": 0.7109375, \"training_loss\": 257.99505615234375, \"iteration\": 618, \"epoch\": 3}, {\"training_acc\": 0.75, \"training_loss\": 301.98968505859375, \"iteration\": 619, \"epoch\": 3}, {\"training_acc\": 0.6875, \"training_loss\": 285.52032470703125, \"iteration\": 620, \"epoch\": 3}, {\"training_acc\": 0.734375, \"training_loss\": 272.05810546875, \"iteration\": 621, \"epoch\": 3}, {\"training_acc\": 0.765625, \"training_loss\": 200.02838134765625, \"iteration\": 622, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 234.32984924316406, \"iteration\": 623, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 277.472412109375, \"iteration\": 624, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 315.52972412109375, \"iteration\": 625, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 250.84048461914062, \"iteration\": 626, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 291.9427795410156, \"iteration\": 627, \"epoch\": 3}, {\"training_acc\": 0.7421875, \"training_loss\": 239.86962890625, \"iteration\": 628, \"epoch\": 3}, {\"training_acc\": 0.71875, \"training_loss\": 242.46363830566406, \"iteration\": 629, \"epoch\": 3}, {\"training_acc\": 0.6953125, \"training_loss\": 273.711181640625, \"iteration\": 630, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 276.5628662109375, \"iteration\": 631, \"epoch\": 3}, {\"training_acc\": 0.7421875, \"training_loss\": 302.2403869628906, \"iteration\": 632, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 311.2296142578125, \"iteration\": 633, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 324.92138671875, \"iteration\": 634, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 325.8984069824219, \"iteration\": 635, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 285.56365966796875, \"iteration\": 636, \"epoch\": 3}, {\"training_acc\": 0.7421875, \"training_loss\": 286.12738037109375, \"iteration\": 637, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 263.19580078125, \"iteration\": 638, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 289.68548583984375, \"iteration\": 639, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 279.2830505371094, \"iteration\": 640, \"epoch\": 3}, {\"training_acc\": 0.7890625, \"training_loss\": 231.72079467773438, \"iteration\": 641, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 275.8143310546875, \"iteration\": 642, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 310.2139892578125, \"iteration\": 643, \"epoch\": 3}, {\"training_acc\": 0.75, \"training_loss\": 270.30169677734375, \"iteration\": 644, \"epoch\": 3}, {\"training_acc\": 0.796875, \"training_loss\": 268.95208740234375, \"iteration\": 645, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 268.60467529296875, \"iteration\": 646, \"epoch\": 3}, {\"training_acc\": 0.7890625, \"training_loss\": 293.11297607421875, \"iteration\": 647, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 276.94989013671875, \"iteration\": 648, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 316.3700256347656, \"iteration\": 649, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 259.23779296875, \"iteration\": 650, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 314.71649169921875, \"iteration\": 651, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 285.16461181640625, \"iteration\": 652, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 296.4532470703125, \"iteration\": 653, \"epoch\": 3}, {\"training_acc\": 0.7734375, \"training_loss\": 305.9443359375, \"iteration\": 654, \"epoch\": 3}, {\"training_acc\": 0.765625, \"training_loss\": 281.96337890625, \"iteration\": 655, \"epoch\": 3}, {\"training_acc\": 0.7890625, \"training_loss\": 309.9833984375, \"iteration\": 656, \"epoch\": 3}, {\"training_acc\": 0.765625, \"training_loss\": 308.0788269042969, \"iteration\": 657, \"epoch\": 3}, {\"training_acc\": 0.7109375, \"training_loss\": 267.0497741699219, \"iteration\": 658, \"epoch\": 3}, {\"training_acc\": 0.7265625, \"training_loss\": 278.08050537109375, \"iteration\": 659, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 290.98486328125, \"iteration\": 660, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 322.9680480957031, \"iteration\": 661, \"epoch\": 3}, {\"training_acc\": 0.78125, \"training_loss\": 279.7683410644531, \"iteration\": 662, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 250.31561279296875, \"iteration\": 663, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 319.8103942871094, \"iteration\": 664, \"epoch\": 3}, {\"training_acc\": 0.703125, \"training_loss\": 238.02175903320312, \"iteration\": 665, \"epoch\": 3}, {\"training_acc\": 0.8367346938775511, \"training_loss\": 79.79219055175781, \"iteration\": 666, \"epoch\": 3}, {\"training_acc\": 0.765625, \"training_loss\": 270.723388671875, \"iteration\": 667, \"epoch\": 4}, {\"training_acc\": 0.8125, \"training_loss\": 254.56039428710938, \"iteration\": 668, \"epoch\": 4}, {\"training_acc\": 0.8515625, \"training_loss\": 288.44525146484375, \"iteration\": 669, \"epoch\": 4}, {\"training_acc\": 0.796875, \"training_loss\": 248.74755859375, \"iteration\": 670, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 250.8643035888672, \"iteration\": 671, \"epoch\": 4}, {\"training_acc\": 0.8671875, \"training_loss\": 285.4460754394531, \"iteration\": 672, \"epoch\": 4}, {\"training_acc\": 0.875, \"training_loss\": 297.319091796875, \"iteration\": 673, \"epoch\": 4}, {\"training_acc\": 0.84375, \"training_loss\": 279.35882568359375, \"iteration\": 674, \"epoch\": 4}, {\"training_acc\": 0.8671875, \"training_loss\": 260.2248229980469, \"iteration\": 675, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 230.65753173828125, \"iteration\": 676, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 255.31182861328125, \"iteration\": 677, \"epoch\": 4}, {\"training_acc\": 0.8125, \"training_loss\": 298.5911560058594, \"iteration\": 678, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 251.23507690429688, \"iteration\": 679, \"epoch\": 4}, {\"training_acc\": 0.8125, \"training_loss\": 268.66925048828125, \"iteration\": 680, \"epoch\": 4}, {\"training_acc\": 0.8125, \"training_loss\": 300.3590087890625, \"iteration\": 681, \"epoch\": 4}, {\"training_acc\": 0.8515625, \"training_loss\": 275.44927978515625, \"iteration\": 682, \"epoch\": 4}, {\"training_acc\": 0.828125, \"training_loss\": 272.6056823730469, \"iteration\": 683, \"epoch\": 4}, {\"training_acc\": 0.84375, \"training_loss\": 266.450439453125, \"iteration\": 684, \"epoch\": 4}, {\"training_acc\": 0.859375, \"training_loss\": 228.2528839111328, \"iteration\": 685, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 259.1424865722656, \"iteration\": 686, \"epoch\": 4}, {\"training_acc\": 0.8671875, \"training_loss\": 287.13116455078125, \"iteration\": 687, \"epoch\": 4}, {\"training_acc\": 0.875, \"training_loss\": 282.9869384765625, \"iteration\": 688, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 212.2317352294922, \"iteration\": 689, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 282.96258544921875, \"iteration\": 690, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 247.2711181640625, \"iteration\": 691, \"epoch\": 4}, {\"training_acc\": 0.84375, \"training_loss\": 284.7564392089844, \"iteration\": 692, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 241.9985809326172, \"iteration\": 693, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 292.5220947265625, \"iteration\": 694, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 255.911865234375, \"iteration\": 695, \"epoch\": 4}, {\"training_acc\": 0.875, \"training_loss\": 285.0774841308594, \"iteration\": 696, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 330.42901611328125, \"iteration\": 697, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 286.30657958984375, \"iteration\": 698, \"epoch\": 4}, {\"training_acc\": 0.8125, \"training_loss\": 241.07659912109375, \"iteration\": 699, \"epoch\": 4}, {\"training_acc\": 0.828125, \"training_loss\": 274.54888916015625, \"iteration\": 700, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 310.1856689453125, \"iteration\": 701, \"epoch\": 4}, {\"training_acc\": 0.828125, \"training_loss\": 276.83526611328125, \"iteration\": 702, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 311.072021484375, \"iteration\": 703, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 290.5008850097656, \"iteration\": 704, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 258.660400390625, \"iteration\": 705, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 316.83502197265625, \"iteration\": 706, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 279.19757080078125, \"iteration\": 707, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 213.31210327148438, \"iteration\": 708, \"epoch\": 4}, {\"training_acc\": 0.859375, \"training_loss\": 263.92529296875, \"iteration\": 709, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 226.57452392578125, \"iteration\": 710, \"epoch\": 4}, {\"training_acc\": 0.875, \"training_loss\": 260.8461608886719, \"iteration\": 711, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 264.03082275390625, \"iteration\": 712, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 272.85498046875, \"iteration\": 713, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 308.43536376953125, \"iteration\": 714, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 336.56280517578125, \"iteration\": 715, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 308.3870849609375, \"iteration\": 716, \"epoch\": 4}, {\"training_acc\": 0.8671875, \"training_loss\": 246.57852172851562, \"iteration\": 717, \"epoch\": 4}, {\"training_acc\": 0.78125, \"training_loss\": 250.99977111816406, \"iteration\": 718, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 244.745849609375, \"iteration\": 719, \"epoch\": 4}, {\"training_acc\": 0.859375, \"training_loss\": 259.75604248046875, \"iteration\": 720, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 293.94659423828125, \"iteration\": 721, \"epoch\": 4}, {\"training_acc\": 0.8125, \"training_loss\": 246.57228088378906, \"iteration\": 722, \"epoch\": 4}, {\"training_acc\": 0.84375, \"training_loss\": 294.7829895019531, \"iteration\": 723, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 310.1254577636719, \"iteration\": 724, \"epoch\": 4}, {\"training_acc\": 0.875, \"training_loss\": 263.21783447265625, \"iteration\": 725, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 300.87445068359375, \"iteration\": 726, \"epoch\": 4}, {\"training_acc\": 0.859375, \"training_loss\": 267.3297119140625, \"iteration\": 727, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 284.63140869140625, \"iteration\": 728, \"epoch\": 4}, {\"training_acc\": 0.8671875, \"training_loss\": 263.7701416015625, \"iteration\": 729, \"epoch\": 4}, {\"training_acc\": 0.8671875, \"training_loss\": 289.6164245605469, \"iteration\": 730, \"epoch\": 4}, {\"training_acc\": 0.8125, \"training_loss\": 265.92218017578125, \"iteration\": 731, \"epoch\": 4}, {\"training_acc\": 0.765625, \"training_loss\": 211.86895751953125, \"iteration\": 732, \"epoch\": 4}, {\"training_acc\": 0.859375, \"training_loss\": 286.76861572265625, \"iteration\": 733, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 264.69366455078125, \"iteration\": 734, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 299.97259521484375, \"iteration\": 735, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 273.6950988769531, \"iteration\": 736, \"epoch\": 4}, {\"training_acc\": 0.859375, \"training_loss\": 262.947998046875, \"iteration\": 737, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 295.7109680175781, \"iteration\": 738, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 261.4495849609375, \"iteration\": 739, \"epoch\": 4}, {\"training_acc\": 0.8671875, \"training_loss\": 263.5311279296875, \"iteration\": 740, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 273.885498046875, \"iteration\": 741, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 290.1748046875, \"iteration\": 742, \"epoch\": 4}, {\"training_acc\": 0.8515625, \"training_loss\": 294.336181640625, \"iteration\": 743, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 274.6431884765625, \"iteration\": 744, \"epoch\": 4}, {\"training_acc\": 0.796875, \"training_loss\": 230.14120483398438, \"iteration\": 745, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 260.13421630859375, \"iteration\": 746, \"epoch\": 4}, {\"training_acc\": 0.84375, \"training_loss\": 261.978759765625, \"iteration\": 747, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 316.365966796875, \"iteration\": 748, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 291.2472839355469, \"iteration\": 749, \"epoch\": 4}, {\"training_acc\": 0.828125, \"training_loss\": 272.10113525390625, \"iteration\": 750, \"epoch\": 4}, {\"training_acc\": 0.8515625, \"training_loss\": 273.459228515625, \"iteration\": 751, \"epoch\": 4}, {\"training_acc\": 0.78125, \"training_loss\": 258.46630859375, \"iteration\": 752, \"epoch\": 4}, {\"training_acc\": 0.765625, \"training_loss\": 270.78900146484375, \"iteration\": 753, \"epoch\": 4}, {\"training_acc\": 0.7421875, \"training_loss\": 239.98008728027344, \"iteration\": 754, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 255.34762573242188, \"iteration\": 755, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 261.7439270019531, \"iteration\": 756, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 278.210693359375, \"iteration\": 757, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 263.95428466796875, \"iteration\": 758, \"epoch\": 4}, {\"training_acc\": 0.8671875, \"training_loss\": 279.4522705078125, \"iteration\": 759, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 293.96832275390625, \"iteration\": 760, \"epoch\": 4}, {\"training_acc\": 0.7265625, \"training_loss\": 230.25454711914062, \"iteration\": 761, \"epoch\": 4}, {\"training_acc\": 0.7265625, \"training_loss\": 264.23663330078125, \"iteration\": 762, \"epoch\": 4}, {\"training_acc\": 0.7421875, \"training_loss\": 305.0899658203125, \"iteration\": 763, \"epoch\": 4}, {\"training_acc\": 0.734375, \"training_loss\": 327.3994140625, \"iteration\": 764, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 332.78143310546875, \"iteration\": 765, \"epoch\": 4}, {\"training_acc\": 0.6953125, \"training_loss\": 261.83648681640625, \"iteration\": 766, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 282.5628967285156, \"iteration\": 767, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 289.360107421875, \"iteration\": 768, \"epoch\": 4}, {\"training_acc\": 0.8515625, \"training_loss\": 297.32586669921875, \"iteration\": 769, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 311.57122802734375, \"iteration\": 770, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 250.96124267578125, \"iteration\": 771, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 231.76580810546875, \"iteration\": 772, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 248.156982421875, \"iteration\": 773, \"epoch\": 4}, {\"training_acc\": 0.734375, \"training_loss\": 270.31817626953125, \"iteration\": 774, \"epoch\": 4}, {\"training_acc\": 0.765625, \"training_loss\": 269.62713623046875, \"iteration\": 775, \"epoch\": 4}, {\"training_acc\": 0.734375, \"training_loss\": 243.33798217773438, \"iteration\": 776, \"epoch\": 4}, {\"training_acc\": 0.7265625, \"training_loss\": 238.55950927734375, \"iteration\": 777, \"epoch\": 4}, {\"training_acc\": 0.796875, \"training_loss\": 309.5397644042969, \"iteration\": 778, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 304.5609130859375, \"iteration\": 779, \"epoch\": 4}, {\"training_acc\": 0.84375, \"training_loss\": 236.3868865966797, \"iteration\": 780, \"epoch\": 4}, {\"training_acc\": 0.859375, \"training_loss\": 268.08917236328125, \"iteration\": 781, \"epoch\": 4}, {\"training_acc\": 0.8671875, \"training_loss\": 262.17144775390625, \"iteration\": 782, \"epoch\": 4}, {\"training_acc\": 0.859375, \"training_loss\": 265.62139892578125, \"iteration\": 783, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 275.1881103515625, \"iteration\": 784, \"epoch\": 4}, {\"training_acc\": 0.796875, \"training_loss\": 246.67068481445312, \"iteration\": 785, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 240.9796600341797, \"iteration\": 786, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 254.2317657470703, \"iteration\": 787, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 234.55210876464844, \"iteration\": 788, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 292.1354675292969, \"iteration\": 789, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 197.22206115722656, \"iteration\": 790, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 269.7515869140625, \"iteration\": 791, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 230.50634765625, \"iteration\": 792, \"epoch\": 4}, {\"training_acc\": 0.796875, \"training_loss\": 308.10406494140625, \"iteration\": 793, \"epoch\": 4}, {\"training_acc\": 0.7421875, \"training_loss\": 264.2492980957031, \"iteration\": 794, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 265.4815368652344, \"iteration\": 795, \"epoch\": 4}, {\"training_acc\": 0.7578125, \"training_loss\": 277.6639404296875, \"iteration\": 796, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 278.7627258300781, \"iteration\": 797, \"epoch\": 4}, {\"training_acc\": 0.84375, \"training_loss\": 300.0941162109375, \"iteration\": 798, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 210.3253936767578, \"iteration\": 799, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 313.8264465332031, \"iteration\": 800, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 205.8922119140625, \"iteration\": 801, \"epoch\": 4}, {\"training_acc\": 0.84375, \"training_loss\": 290.6485900878906, \"iteration\": 802, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 295.2344970703125, \"iteration\": 803, \"epoch\": 4}, {\"training_acc\": 0.7421875, \"training_loss\": 240.95086669921875, \"iteration\": 804, \"epoch\": 4}, {\"training_acc\": 0.765625, \"training_loss\": 240.99879455566406, \"iteration\": 805, \"epoch\": 4}, {\"training_acc\": 0.7578125, \"training_loss\": 297.1339111328125, \"iteration\": 806, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 257.18145751953125, \"iteration\": 807, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 251.0377197265625, \"iteration\": 808, \"epoch\": 4}, {\"training_acc\": 0.859375, \"training_loss\": 312.4085693359375, \"iteration\": 809, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 336.601806640625, \"iteration\": 810, \"epoch\": 4}, {\"training_acc\": 0.75, \"training_loss\": 288.93798828125, \"iteration\": 811, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 257.96539306640625, \"iteration\": 812, \"epoch\": 4}, {\"training_acc\": 0.78125, \"training_loss\": 268.615478515625, \"iteration\": 813, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 270.3315734863281, \"iteration\": 814, \"epoch\": 4}, {\"training_acc\": 0.6953125, \"training_loss\": 314.99407958984375, \"iteration\": 815, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 272.67950439453125, \"iteration\": 816, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 326.67987060546875, \"iteration\": 817, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 312.0136413574219, \"iteration\": 818, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 258.13641357421875, \"iteration\": 819, \"epoch\": 4}, {\"training_acc\": 0.7421875, \"training_loss\": 263.68511962890625, \"iteration\": 820, \"epoch\": 4}, {\"training_acc\": 0.7109375, \"training_loss\": 276.83929443359375, \"iteration\": 821, \"epoch\": 4}, {\"training_acc\": 0.7265625, \"training_loss\": 278.0345458984375, \"iteration\": 822, \"epoch\": 4}, {\"training_acc\": 0.6640625, \"training_loss\": 262.5035095214844, \"iteration\": 823, \"epoch\": 4}, {\"training_acc\": 0.6875, \"training_loss\": 298.12640380859375, \"iteration\": 824, \"epoch\": 4}, {\"training_acc\": 0.6875, \"training_loss\": 322.4981689453125, \"iteration\": 825, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 280.97119140625, \"iteration\": 826, \"epoch\": 4}, {\"training_acc\": 0.734375, \"training_loss\": 265.03277587890625, \"iteration\": 827, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 310.82586669921875, \"iteration\": 828, \"epoch\": 4}, {\"training_acc\": 0.7578125, \"training_loss\": 284.1599426269531, \"iteration\": 829, \"epoch\": 4}, {\"training_acc\": 0.765625, \"training_loss\": 313.6370544433594, \"iteration\": 830, \"epoch\": 4}, {\"training_acc\": 0.765625, \"training_loss\": 249.8994598388672, \"iteration\": 831, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 289.7895812988281, \"iteration\": 832, \"epoch\": 4}, {\"training_acc\": 0.734375, \"training_loss\": 206.4810791015625, \"iteration\": 833, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 313.72271728515625, \"iteration\": 834, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 276.85382080078125, \"iteration\": 835, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 277.2604064941406, \"iteration\": 836, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 246.54762268066406, \"iteration\": 837, \"epoch\": 4}, {\"training_acc\": 0.828125, \"training_loss\": 274.4256591796875, \"iteration\": 838, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 281.9205017089844, \"iteration\": 839, \"epoch\": 4}, {\"training_acc\": 0.78125, \"training_loss\": 274.0250244140625, \"iteration\": 840, \"epoch\": 4}, {\"training_acc\": 0.7421875, \"training_loss\": 271.02825927734375, \"iteration\": 841, \"epoch\": 4}, {\"training_acc\": 0.8125, \"training_loss\": 298.3328857421875, \"iteration\": 842, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 253.2080078125, \"iteration\": 843, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 241.91818237304688, \"iteration\": 844, \"epoch\": 4}, {\"training_acc\": 0.8125, \"training_loss\": 250.34632873535156, \"iteration\": 845, \"epoch\": 4}, {\"training_acc\": 0.84375, \"training_loss\": 253.43344116210938, \"iteration\": 846, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 274.1861572265625, \"iteration\": 847, \"epoch\": 4}, {\"training_acc\": 0.828125, \"training_loss\": 250.03160095214844, \"iteration\": 848, \"epoch\": 4}, {\"training_acc\": 0.8046875, \"training_loss\": 269.196533203125, \"iteration\": 849, \"epoch\": 4}, {\"training_acc\": 0.796875, \"training_loss\": 299.7879943847656, \"iteration\": 850, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 287.2762145996094, \"iteration\": 851, \"epoch\": 4}, {\"training_acc\": 0.71875, \"training_loss\": 264.9664001464844, \"iteration\": 852, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 252.212646484375, \"iteration\": 853, \"epoch\": 4}, {\"training_acc\": 0.78125, \"training_loss\": 322.6849060058594, \"iteration\": 854, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 269.9283752441406, \"iteration\": 855, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 244.9778289794922, \"iteration\": 856, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 224.4747314453125, \"iteration\": 857, \"epoch\": 4}, {\"training_acc\": 0.78125, \"training_loss\": 300.74517822265625, \"iteration\": 858, \"epoch\": 4}, {\"training_acc\": 0.796875, \"training_loss\": 319.7274169921875, \"iteration\": 859, \"epoch\": 4}, {\"training_acc\": 0.7734375, \"training_loss\": 293.7170104980469, \"iteration\": 860, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 213.77935791015625, \"iteration\": 861, \"epoch\": 4}, {\"training_acc\": 0.7578125, \"training_loss\": 256.1184997558594, \"iteration\": 862, \"epoch\": 4}, {\"training_acc\": 0.78125, \"training_loss\": 260.5194396972656, \"iteration\": 863, \"epoch\": 4}, {\"training_acc\": 0.796875, \"training_loss\": 273.59735107421875, \"iteration\": 864, \"epoch\": 4}, {\"training_acc\": 0.7578125, \"training_loss\": 209.84378051757812, \"iteration\": 865, \"epoch\": 4}, {\"training_acc\": 0.8125, \"training_loss\": 323.64202880859375, \"iteration\": 866, \"epoch\": 4}, {\"training_acc\": 0.7890625, \"training_loss\": 258.63677978515625, \"iteration\": 867, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 284.310302734375, \"iteration\": 868, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 276.58905029296875, \"iteration\": 869, \"epoch\": 4}, {\"training_acc\": 0.734375, \"training_loss\": 284.7521667480469, \"iteration\": 870, \"epoch\": 4}, {\"training_acc\": 0.703125, \"training_loss\": 316.9083251953125, \"iteration\": 871, \"epoch\": 4}, {\"training_acc\": 0.703125, \"training_loss\": 268.58966064453125, \"iteration\": 872, \"epoch\": 4}, {\"training_acc\": 0.671875, \"training_loss\": 275.214599609375, \"iteration\": 873, \"epoch\": 4}, {\"training_acc\": 0.75, \"training_loss\": 314.684326171875, \"iteration\": 874, \"epoch\": 4}, {\"training_acc\": 0.7421875, \"training_loss\": 301.8575744628906, \"iteration\": 875, \"epoch\": 4}, {\"training_acc\": 0.75, \"training_loss\": 307.0457763671875, \"iteration\": 876, \"epoch\": 4}, {\"training_acc\": 0.796875, \"training_loss\": 267.548095703125, \"iteration\": 877, \"epoch\": 4}, {\"training_acc\": 0.8125, \"training_loss\": 272.6554870605469, \"iteration\": 878, \"epoch\": 4}, {\"training_acc\": 0.8515625, \"training_loss\": 285.06036376953125, \"iteration\": 879, \"epoch\": 4}, {\"training_acc\": 0.8203125, \"training_loss\": 279.35443115234375, \"iteration\": 880, \"epoch\": 4}, {\"training_acc\": 0.828125, \"training_loss\": 286.4180908203125, \"iteration\": 881, \"epoch\": 4}, {\"training_acc\": 0.6953125, \"training_loss\": 259.8216552734375, \"iteration\": 882, \"epoch\": 4}, {\"training_acc\": 0.7265625, \"training_loss\": 277.504150390625, \"iteration\": 883, \"epoch\": 4}, {\"training_acc\": 0.6640625, \"training_loss\": 247.2989044189453, \"iteration\": 884, \"epoch\": 4}, {\"training_acc\": 0.7265625, \"training_loss\": 288.3736877441406, \"iteration\": 885, \"epoch\": 4}, {\"training_acc\": 0.703125, \"training_loss\": 258.37939453125, \"iteration\": 886, \"epoch\": 4}, {\"training_acc\": 0.703125, \"training_loss\": 251.68426513671875, \"iteration\": 887, \"epoch\": 4}, {\"training_acc\": 0.7959183673469388, \"training_loss\": 84.56059265136719, \"iteration\": 888, \"epoch\": 4}, {\"training_acc\": 0.875, \"training_loss\": 252.7655487060547, \"iteration\": 889, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 268.35650634765625, \"iteration\": 890, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 273.04779052734375, \"iteration\": 891, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 327.9019470214844, \"iteration\": 892, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 298.915771484375, \"iteration\": 893, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 271.8362731933594, \"iteration\": 894, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 278.4500732421875, \"iteration\": 895, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 298.95135498046875, \"iteration\": 896, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 309.65545654296875, \"iteration\": 897, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 258.11798095703125, \"iteration\": 898, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 279.9488525390625, \"iteration\": 899, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 286.15802001953125, \"iteration\": 900, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 261.205810546875, \"iteration\": 901, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 312.3835754394531, \"iteration\": 902, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 280.2182922363281, \"iteration\": 903, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 276.3074951171875, \"iteration\": 904, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 301.4837646484375, \"iteration\": 905, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 282.3758239746094, \"iteration\": 906, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 273.7806396484375, \"iteration\": 907, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 240.70587158203125, \"iteration\": 908, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 245.32908630371094, \"iteration\": 909, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 247.35406494140625, \"iteration\": 910, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 230.2569122314453, \"iteration\": 911, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 281.83050537109375, \"iteration\": 912, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 310.9549560546875, \"iteration\": 913, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 289.7537536621094, \"iteration\": 914, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 255.31178283691406, \"iteration\": 915, \"epoch\": 5}, {\"training_acc\": 0.90625, \"training_loss\": 314.09686279296875, \"iteration\": 916, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 253.51123046875, \"iteration\": 917, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 251.516357421875, \"iteration\": 918, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 262.61907958984375, \"iteration\": 919, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 245.1753387451172, \"iteration\": 920, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 265.6695251464844, \"iteration\": 921, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 209.55691528320312, \"iteration\": 922, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 255.57928466796875, \"iteration\": 923, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 264.19537353515625, \"iteration\": 924, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 276.66143798828125, \"iteration\": 925, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 279.61083984375, \"iteration\": 926, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 255.82852172851562, \"iteration\": 927, \"epoch\": 5}, {\"training_acc\": 0.9296875, \"training_loss\": 326.40234375, \"iteration\": 928, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 279.45660400390625, \"iteration\": 929, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 268.4423828125, \"iteration\": 930, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 289.44122314453125, \"iteration\": 931, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 248.90023803710938, \"iteration\": 932, \"epoch\": 5}, {\"training_acc\": 0.9140625, \"training_loss\": 266.036865234375, \"iteration\": 933, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 231.8658905029297, \"iteration\": 934, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 285.32293701171875, \"iteration\": 935, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 285.50885009765625, \"iteration\": 936, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 227.54673767089844, \"iteration\": 937, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 235.09951782226562, \"iteration\": 938, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 278.59161376953125, \"iteration\": 939, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 303.88458251953125, \"iteration\": 940, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 282.72161865234375, \"iteration\": 941, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 256.4468994140625, \"iteration\": 942, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 253.0787353515625, \"iteration\": 943, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 265.0048828125, \"iteration\": 944, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 273.3961486816406, \"iteration\": 945, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 218.69558715820312, \"iteration\": 946, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 249.64585876464844, \"iteration\": 947, \"epoch\": 5}, {\"training_acc\": 0.9140625, \"training_loss\": 322.5245361328125, \"iteration\": 948, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 255.9886474609375, \"iteration\": 949, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 224.9820098876953, \"iteration\": 950, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 234.11219787597656, \"iteration\": 951, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 268.9522399902344, \"iteration\": 952, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 279.1692810058594, \"iteration\": 953, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 229.08152770996094, \"iteration\": 954, \"epoch\": 5}, {\"training_acc\": 0.9140625, \"training_loss\": 309.5997314453125, \"iteration\": 955, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 238.23818969726562, \"iteration\": 956, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 218.1483154296875, \"iteration\": 957, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 272.561767578125, \"iteration\": 958, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 229.3118133544922, \"iteration\": 959, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 242.15884399414062, \"iteration\": 960, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 268.39215087890625, \"iteration\": 961, \"epoch\": 5}, {\"training_acc\": 0.9140625, \"training_loss\": 318.58575439453125, \"iteration\": 962, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 241.9844207763672, \"iteration\": 963, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 276.8603515625, \"iteration\": 964, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 249.06373596191406, \"iteration\": 965, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 277.8833312988281, \"iteration\": 966, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 303.13372802734375, \"iteration\": 967, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 245.4261474609375, \"iteration\": 968, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 274.4515686035156, \"iteration\": 969, \"epoch\": 5}, {\"training_acc\": 0.9296875, \"training_loss\": 254.83177185058594, \"iteration\": 970, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 228.53346252441406, \"iteration\": 971, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 229.7879638671875, \"iteration\": 972, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 222.12847900390625, \"iteration\": 973, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 275.33074951171875, \"iteration\": 974, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 261.1805419921875, \"iteration\": 975, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 272.466064453125, \"iteration\": 976, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 236.34585571289062, \"iteration\": 977, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 316.5754699707031, \"iteration\": 978, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 308.04180908203125, \"iteration\": 979, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 274.5435791015625, \"iteration\": 980, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 288.0186767578125, \"iteration\": 981, \"epoch\": 5}, {\"training_acc\": 0.921875, \"training_loss\": 280.55108642578125, \"iteration\": 982, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 269.3183898925781, \"iteration\": 983, \"epoch\": 5}, {\"training_acc\": 0.9296875, \"training_loss\": 239.3567352294922, \"iteration\": 984, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 250.23487854003906, \"iteration\": 985, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 265.2025146484375, \"iteration\": 986, \"epoch\": 5}, {\"training_acc\": 0.9296875, \"training_loss\": 248.37652587890625, \"iteration\": 987, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 266.68060302734375, \"iteration\": 988, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 303.32391357421875, \"iteration\": 989, \"epoch\": 5}, {\"training_acc\": 0.7578125, \"training_loss\": 271.9483337402344, \"iteration\": 990, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 281.8070068359375, \"iteration\": 991, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 261.29852294921875, \"iteration\": 992, \"epoch\": 5}, {\"training_acc\": 0.7421875, \"training_loss\": 243.78530883789062, \"iteration\": 993, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 247.3092041015625, \"iteration\": 994, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 287.01153564453125, \"iteration\": 995, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 259.1162109375, \"iteration\": 996, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 297.114990234375, \"iteration\": 997, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 246.30841064453125, \"iteration\": 998, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 248.95448303222656, \"iteration\": 999, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 250.7322998046875, \"iteration\": 1000, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 277.1810607910156, \"iteration\": 1001, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 293.12823486328125, \"iteration\": 1002, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 300.48175048828125, \"iteration\": 1003, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 340.51458740234375, \"iteration\": 1004, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 257.13787841796875, \"iteration\": 1005, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 264.9367370605469, \"iteration\": 1006, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 337.0421142578125, \"iteration\": 1007, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 332.87908935546875, \"iteration\": 1008, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 315.49212646484375, \"iteration\": 1009, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 224.6751251220703, \"iteration\": 1010, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 268.14007568359375, \"iteration\": 1011, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 285.6081848144531, \"iteration\": 1012, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 259.38848876953125, \"iteration\": 1013, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 254.72596740722656, \"iteration\": 1014, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 219.17884826660156, \"iteration\": 1015, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 304.5278625488281, \"iteration\": 1016, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 243.95574951171875, \"iteration\": 1017, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 305.99969482421875, \"iteration\": 1018, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 271.7696533203125, \"iteration\": 1019, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 319.81158447265625, \"iteration\": 1020, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 304.2082214355469, \"iteration\": 1021, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 276.7542724609375, \"iteration\": 1022, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 252.81756591796875, \"iteration\": 1023, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 262.85394287109375, \"iteration\": 1024, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 278.4712829589844, \"iteration\": 1025, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 290.05657958984375, \"iteration\": 1026, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 305.0207214355469, \"iteration\": 1027, \"epoch\": 5}, {\"training_acc\": 0.921875, \"training_loss\": 297.18524169921875, \"iteration\": 1028, \"epoch\": 5}, {\"training_acc\": 0.90625, \"training_loss\": 263.63726806640625, \"iteration\": 1029, \"epoch\": 5}, {\"training_acc\": 0.90625, \"training_loss\": 286.68182373046875, \"iteration\": 1030, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 287.69329833984375, \"iteration\": 1031, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 256.48858642578125, \"iteration\": 1032, \"epoch\": 5}, {\"training_acc\": 0.8125, \"training_loss\": 253.833740234375, \"iteration\": 1033, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 261.3227233886719, \"iteration\": 1034, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 299.5326232910156, \"iteration\": 1035, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 306.06396484375, \"iteration\": 1036, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 281.74029541015625, \"iteration\": 1037, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 251.18527221679688, \"iteration\": 1038, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 330.11083984375, \"iteration\": 1039, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 215.46026611328125, \"iteration\": 1040, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 290.5248107910156, \"iteration\": 1041, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 287.89300537109375, \"iteration\": 1042, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 269.78253173828125, \"iteration\": 1043, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 278.4828186035156, \"iteration\": 1044, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 261.5523986816406, \"iteration\": 1045, \"epoch\": 5}, {\"training_acc\": 0.921875, \"training_loss\": 293.20684814453125, \"iteration\": 1046, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 280.11334228515625, \"iteration\": 1047, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 255.28665161132812, \"iteration\": 1048, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 249.42701721191406, \"iteration\": 1049, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 264.6370849609375, \"iteration\": 1050, \"epoch\": 5}, {\"training_acc\": 0.921875, \"training_loss\": 282.52325439453125, \"iteration\": 1051, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 251.5546417236328, \"iteration\": 1052, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 262.3373107910156, \"iteration\": 1053, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 279.2625732421875, \"iteration\": 1054, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 257.7689208984375, \"iteration\": 1055, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 274.25140380859375, \"iteration\": 1056, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 276.65679931640625, \"iteration\": 1057, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 242.012451171875, \"iteration\": 1058, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 273.35052490234375, \"iteration\": 1059, \"epoch\": 5}, {\"training_acc\": 0.9140625, \"training_loss\": 256.5809326171875, \"iteration\": 1060, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 246.8803253173828, \"iteration\": 1061, \"epoch\": 5}, {\"training_acc\": 0.9140625, \"training_loss\": 240.15078735351562, \"iteration\": 1062, \"epoch\": 5}, {\"training_acc\": 0.8359375, \"training_loss\": 274.7229309082031, \"iteration\": 1063, \"epoch\": 5}, {\"training_acc\": 0.859375, \"training_loss\": 269.17034912109375, \"iteration\": 1064, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 283.0257263183594, \"iteration\": 1065, \"epoch\": 5}, {\"training_acc\": 0.8515625, \"training_loss\": 236.572021484375, \"iteration\": 1066, \"epoch\": 5}, {\"training_acc\": 0.84375, \"training_loss\": 293.56158447265625, \"iteration\": 1067, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 284.5474853515625, \"iteration\": 1068, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 306.9682312011719, \"iteration\": 1069, \"epoch\": 5}, {\"training_acc\": 0.765625, \"training_loss\": 258.403076171875, \"iteration\": 1070, \"epoch\": 5}, {\"training_acc\": 0.6953125, \"training_loss\": 272.15478515625, \"iteration\": 1071, \"epoch\": 5}, {\"training_acc\": 0.734375, \"training_loss\": 224.4055633544922, \"iteration\": 1072, \"epoch\": 5}, {\"training_acc\": 0.796875, \"training_loss\": 249.10787963867188, \"iteration\": 1073, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 248.12074279785156, \"iteration\": 1074, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 249.6964111328125, \"iteration\": 1075, \"epoch\": 5}, {\"training_acc\": 0.8828125, \"training_loss\": 281.321044921875, \"iteration\": 1076, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 216.45419311523438, \"iteration\": 1077, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 287.3873291015625, \"iteration\": 1078, \"epoch\": 5}, {\"training_acc\": 0.890625, \"training_loss\": 266.08380126953125, \"iteration\": 1079, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 301.7145690917969, \"iteration\": 1080, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 281.4339904785156, \"iteration\": 1081, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 305.8632507324219, \"iteration\": 1082, \"epoch\": 5}, {\"training_acc\": 0.5859375, \"training_loss\": 247.44107055664062, \"iteration\": 1083, \"epoch\": 5}, {\"training_acc\": 0.6171875, \"training_loss\": 244.52828979492188, \"iteration\": 1084, \"epoch\": 5}, {\"training_acc\": 0.65625, \"training_loss\": 279.70947265625, \"iteration\": 1085, \"epoch\": 5}, {\"training_acc\": 0.6171875, \"training_loss\": 236.61265563964844, \"iteration\": 1086, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 250.014892578125, \"iteration\": 1087, \"epoch\": 5}, {\"training_acc\": 0.6875, \"training_loss\": 220.730712890625, \"iteration\": 1088, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 283.3009033203125, \"iteration\": 1089, \"epoch\": 5}, {\"training_acc\": 0.8203125, \"training_loss\": 261.2237243652344, \"iteration\": 1090, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 266.7236328125, \"iteration\": 1091, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 218.13172912597656, \"iteration\": 1092, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 239.198486328125, \"iteration\": 1093, \"epoch\": 5}, {\"training_acc\": 0.71875, \"training_loss\": 240.6509246826172, \"iteration\": 1094, \"epoch\": 5}, {\"training_acc\": 0.78125, \"training_loss\": 328.2405700683594, \"iteration\": 1095, \"epoch\": 5}, {\"training_acc\": 0.71875, \"training_loss\": 239.04458618164062, \"iteration\": 1096, \"epoch\": 5}, {\"training_acc\": 0.7109375, \"training_loss\": 323.5918273925781, \"iteration\": 1097, \"epoch\": 5}, {\"training_acc\": 0.71875, \"training_loss\": 233.74188232421875, \"iteration\": 1098, \"epoch\": 5}, {\"training_acc\": 0.6640625, \"training_loss\": 245.65469360351562, \"iteration\": 1099, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 374.487060546875, \"iteration\": 1100, \"epoch\": 5}, {\"training_acc\": 0.75, \"training_loss\": 270.74578857421875, \"iteration\": 1101, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 245.78024291992188, \"iteration\": 1102, \"epoch\": 5}, {\"training_acc\": 0.875, \"training_loss\": 270.2306213378906, \"iteration\": 1103, \"epoch\": 5}, {\"training_acc\": 0.7890625, \"training_loss\": 336.4969482421875, \"iteration\": 1104, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 251.7468719482422, \"iteration\": 1105, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 254.8125457763672, \"iteration\": 1106, \"epoch\": 5}, {\"training_acc\": 0.828125, \"training_loss\": 295.523193359375, \"iteration\": 1107, \"epoch\": 5}, {\"training_acc\": 0.8671875, \"training_loss\": 293.85479736328125, \"iteration\": 1108, \"epoch\": 5}, {\"training_acc\": 0.8046875, \"training_loss\": 266.24163818359375, \"iteration\": 1109, \"epoch\": 5}, {\"training_acc\": 0.7551020408163265, \"training_loss\": 76.14512634277344, \"iteration\": 1110, \"epoch\": 5}, {\"training_acc\": 0.7734375, \"training_loss\": 227.83932495117188, \"iteration\": 1111, \"epoch\": 6}, {\"training_acc\": 0.7578125, \"training_loss\": 227.1161346435547, \"iteration\": 1112, \"epoch\": 6}, {\"training_acc\": 0.7890625, \"training_loss\": 285.38739013671875, \"iteration\": 1113, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 270.43865966796875, \"iteration\": 1114, \"epoch\": 6}, {\"training_acc\": 0.828125, \"training_loss\": 264.7952575683594, \"iteration\": 1115, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 281.29656982421875, \"iteration\": 1116, \"epoch\": 6}, {\"training_acc\": 0.9375, \"training_loss\": 294.74517822265625, \"iteration\": 1117, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 265.9902038574219, \"iteration\": 1118, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 305.57220458984375, \"iteration\": 1119, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 250.1416778564453, \"iteration\": 1120, \"epoch\": 6}, {\"training_acc\": 0.8984375, \"training_loss\": 282.89111328125, \"iteration\": 1121, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 283.26312255859375, \"iteration\": 1122, \"epoch\": 6}, {\"training_acc\": 0.8515625, \"training_loss\": 285.1421203613281, \"iteration\": 1123, \"epoch\": 6}, {\"training_acc\": 0.828125, \"training_loss\": 284.9246826171875, \"iteration\": 1124, \"epoch\": 6}, {\"training_acc\": 0.7734375, \"training_loss\": 234.8956298828125, \"iteration\": 1125, \"epoch\": 6}, {\"training_acc\": 0.734375, \"training_loss\": 234.760986328125, \"iteration\": 1126, \"epoch\": 6}, {\"training_acc\": 0.796875, \"training_loss\": 295.3670349121094, \"iteration\": 1127, \"epoch\": 6}, {\"training_acc\": 0.8125, \"training_loss\": 248.79330444335938, \"iteration\": 1128, \"epoch\": 6}, {\"training_acc\": 0.8125, \"training_loss\": 222.8026123046875, \"iteration\": 1129, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 302.64703369140625, \"iteration\": 1130, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 216.95570373535156, \"iteration\": 1131, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 254.2877960205078, \"iteration\": 1132, \"epoch\": 6}, {\"training_acc\": 0.921875, \"training_loss\": 220.1544189453125, \"iteration\": 1133, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 247.19378662109375, \"iteration\": 1134, \"epoch\": 6}, {\"training_acc\": 0.953125, \"training_loss\": 329.15545654296875, \"iteration\": 1135, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 286.4417419433594, \"iteration\": 1136, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 277.0069580078125, \"iteration\": 1137, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 236.274658203125, \"iteration\": 1138, \"epoch\": 6}, {\"training_acc\": 0.8203125, \"training_loss\": 265.92083740234375, \"iteration\": 1139, \"epoch\": 6}, {\"training_acc\": 0.8125, \"training_loss\": 271.9770202636719, \"iteration\": 1140, \"epoch\": 6}, {\"training_acc\": 0.8203125, \"training_loss\": 310.3670654296875, \"iteration\": 1141, \"epoch\": 6}, {\"training_acc\": 0.8125, \"training_loss\": 291.9532165527344, \"iteration\": 1142, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 223.15078735351562, \"iteration\": 1143, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 257.99169921875, \"iteration\": 1144, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 265.7134704589844, \"iteration\": 1145, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 285.98822021484375, \"iteration\": 1146, \"epoch\": 6}, {\"training_acc\": 0.8984375, \"training_loss\": 308.5926513671875, \"iteration\": 1147, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 248.3099822998047, \"iteration\": 1148, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 257.9815979003906, \"iteration\": 1149, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 217.92434692382812, \"iteration\": 1150, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 333.10626220703125, \"iteration\": 1151, \"epoch\": 6}, {\"training_acc\": 0.90625, \"training_loss\": 298.07379150390625, \"iteration\": 1152, \"epoch\": 6}, {\"training_acc\": 0.8125, \"training_loss\": 240.2244110107422, \"iteration\": 1153, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 292.2257080078125, \"iteration\": 1154, \"epoch\": 6}, {\"training_acc\": 0.8515625, \"training_loss\": 250.61627197265625, \"iteration\": 1155, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 276.3475341796875, \"iteration\": 1156, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 317.2613525390625, \"iteration\": 1157, \"epoch\": 6}, {\"training_acc\": 0.921875, \"training_loss\": 229.4931182861328, \"iteration\": 1158, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 349.74346923828125, \"iteration\": 1159, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 297.89373779296875, \"iteration\": 1160, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 272.2541809082031, \"iteration\": 1161, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 235.4134521484375, \"iteration\": 1162, \"epoch\": 6}, {\"training_acc\": 0.828125, \"training_loss\": 279.59759521484375, \"iteration\": 1163, \"epoch\": 6}, {\"training_acc\": 0.8984375, \"training_loss\": 239.03045654296875, \"iteration\": 1164, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 274.01324462890625, \"iteration\": 1165, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 258.8148498535156, \"iteration\": 1166, \"epoch\": 6}, {\"training_acc\": 0.953125, \"training_loss\": 282.75152587890625, \"iteration\": 1167, \"epoch\": 6}, {\"training_acc\": 0.90625, \"training_loss\": 271.43695068359375, \"iteration\": 1168, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 282.9781494140625, \"iteration\": 1169, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 249.74191284179688, \"iteration\": 1170, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 266.1423034667969, \"iteration\": 1171, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 287.5345458984375, \"iteration\": 1172, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 297.38519287109375, \"iteration\": 1173, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 286.74072265625, \"iteration\": 1174, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 227.54139709472656, \"iteration\": 1175, \"epoch\": 6}, {\"training_acc\": 0.8984375, \"training_loss\": 253.67340087890625, \"iteration\": 1176, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 265.3719787597656, \"iteration\": 1177, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 301.85052490234375, \"iteration\": 1178, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 261.38067626953125, \"iteration\": 1179, \"epoch\": 6}, {\"training_acc\": 0.90625, \"training_loss\": 250.52304077148438, \"iteration\": 1180, \"epoch\": 6}, {\"training_acc\": 0.8515625, \"training_loss\": 267.06378173828125, \"iteration\": 1181, \"epoch\": 6}, {\"training_acc\": 0.921875, \"training_loss\": 303.5755310058594, \"iteration\": 1182, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 224.45458984375, \"iteration\": 1183, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 284.25164794921875, \"iteration\": 1184, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 318.88616943359375, \"iteration\": 1185, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 277.2541198730469, \"iteration\": 1186, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 243.3081817626953, \"iteration\": 1187, \"epoch\": 6}, {\"training_acc\": 0.8359375, \"training_loss\": 247.42929077148438, \"iteration\": 1188, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 234.93609619140625, \"iteration\": 1189, \"epoch\": 6}, {\"training_acc\": 0.921875, \"training_loss\": 285.0700378417969, \"iteration\": 1190, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 253.0116729736328, \"iteration\": 1191, \"epoch\": 6}, {\"training_acc\": 0.78125, \"training_loss\": 238.6471405029297, \"iteration\": 1192, \"epoch\": 6}, {\"training_acc\": 0.8984375, \"training_loss\": 286.24920654296875, \"iteration\": 1193, \"epoch\": 6}, {\"training_acc\": 0.828125, \"training_loss\": 278.619873046875, \"iteration\": 1194, \"epoch\": 6}, {\"training_acc\": 0.8984375, \"training_loss\": 271.823974609375, \"iteration\": 1195, \"epoch\": 6}, {\"training_acc\": 0.8359375, \"training_loss\": 286.53826904296875, \"iteration\": 1196, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 294.1382141113281, \"iteration\": 1197, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 311.2823486328125, \"iteration\": 1198, \"epoch\": 6}, {\"training_acc\": 0.8984375, \"training_loss\": 296.15185546875, \"iteration\": 1199, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 282.25726318359375, \"iteration\": 1200, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 285.15606689453125, \"iteration\": 1201, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 277.290771484375, \"iteration\": 1202, \"epoch\": 6}, {\"training_acc\": 0.90625, \"training_loss\": 245.9247589111328, \"iteration\": 1203, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 304.8585205078125, \"iteration\": 1204, \"epoch\": 6}, {\"training_acc\": 0.921875, \"training_loss\": 294.9054870605469, \"iteration\": 1205, \"epoch\": 6}, {\"training_acc\": 0.90625, \"training_loss\": 294.08135986328125, \"iteration\": 1206, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 258.49560546875, \"iteration\": 1207, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 298.805419921875, \"iteration\": 1208, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 236.0766143798828, \"iteration\": 1209, \"epoch\": 6}, {\"training_acc\": 0.8046875, \"training_loss\": 262.45159912109375, \"iteration\": 1210, \"epoch\": 6}, {\"training_acc\": 0.8203125, \"training_loss\": 271.99285888671875, \"iteration\": 1211, \"epoch\": 6}, {\"training_acc\": 0.8125, \"training_loss\": 268.51519775390625, \"iteration\": 1212, \"epoch\": 6}, {\"training_acc\": 0.8203125, \"training_loss\": 278.9211730957031, \"iteration\": 1213, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 245.66030883789062, \"iteration\": 1214, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 233.66796875, \"iteration\": 1215, \"epoch\": 6}, {\"training_acc\": 0.8984375, \"training_loss\": 303.11285400390625, \"iteration\": 1216, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 261.33648681640625, \"iteration\": 1217, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 233.49758911132812, \"iteration\": 1218, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 282.249755859375, \"iteration\": 1219, \"epoch\": 6}, {\"training_acc\": 0.9453125, \"training_loss\": 243.68943786621094, \"iteration\": 1220, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 292.34912109375, \"iteration\": 1221, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 277.632568359375, \"iteration\": 1222, \"epoch\": 6}, {\"training_acc\": 0.8515625, \"training_loss\": 264.895751953125, \"iteration\": 1223, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 202.43234252929688, \"iteration\": 1224, \"epoch\": 6}, {\"training_acc\": 0.8203125, \"training_loss\": 234.921630859375, \"iteration\": 1225, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 244.74154663085938, \"iteration\": 1226, \"epoch\": 6}, {\"training_acc\": 0.8203125, \"training_loss\": 278.18817138671875, \"iteration\": 1227, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 339.2020263671875, \"iteration\": 1228, \"epoch\": 6}, {\"training_acc\": 0.7890625, \"training_loss\": 210.5263214111328, \"iteration\": 1229, \"epoch\": 6}, {\"training_acc\": 0.8359375, \"training_loss\": 263.501220703125, \"iteration\": 1230, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 239.7532958984375, \"iteration\": 1231, \"epoch\": 6}, {\"training_acc\": 0.8359375, \"training_loss\": 268.079345703125, \"iteration\": 1232, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 279.03857421875, \"iteration\": 1233, \"epoch\": 6}, {\"training_acc\": 0.828125, \"training_loss\": 278.2232666015625, \"iteration\": 1234, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 235.23501586914062, \"iteration\": 1235, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 310.9261474609375, \"iteration\": 1236, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 213.24356079101562, \"iteration\": 1237, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 234.515380859375, \"iteration\": 1238, \"epoch\": 6}, {\"training_acc\": 0.7578125, \"training_loss\": 258.2077331542969, \"iteration\": 1239, \"epoch\": 6}, {\"training_acc\": 0.7578125, \"training_loss\": 263.63482666015625, \"iteration\": 1240, \"epoch\": 6}, {\"training_acc\": 0.8046875, \"training_loss\": 278.1706237792969, \"iteration\": 1241, \"epoch\": 6}, {\"training_acc\": 0.796875, \"training_loss\": 308.1167297363281, \"iteration\": 1242, \"epoch\": 6}, {\"training_acc\": 0.796875, \"training_loss\": 304.7575988769531, \"iteration\": 1243, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 269.33709716796875, \"iteration\": 1244, \"epoch\": 6}, {\"training_acc\": 0.78125, \"training_loss\": 193.80441284179688, \"iteration\": 1245, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 261.083251953125, \"iteration\": 1246, \"epoch\": 6}, {\"training_acc\": 0.8359375, \"training_loss\": 273.0494079589844, \"iteration\": 1247, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 279.5931091308594, \"iteration\": 1248, \"epoch\": 6}, {\"training_acc\": 0.78125, \"training_loss\": 243.93893432617188, \"iteration\": 1249, \"epoch\": 6}, {\"training_acc\": 0.7890625, \"training_loss\": 216.71829223632812, \"iteration\": 1250, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 270.35784912109375, \"iteration\": 1251, \"epoch\": 6}, {\"training_acc\": 0.9140625, \"training_loss\": 271.4154052734375, \"iteration\": 1252, \"epoch\": 6}, {\"training_acc\": 0.796875, \"training_loss\": 318.460205078125, \"iteration\": 1253, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 279.2667541503906, \"iteration\": 1254, \"epoch\": 6}, {\"training_acc\": 0.8046875, \"training_loss\": 323.84539794921875, \"iteration\": 1255, \"epoch\": 6}, {\"training_acc\": 0.7578125, \"training_loss\": 259.0991516113281, \"iteration\": 1256, \"epoch\": 6}, {\"training_acc\": 0.8046875, \"training_loss\": 254.3705291748047, \"iteration\": 1257, \"epoch\": 6}, {\"training_acc\": 0.8515625, \"training_loss\": 322.5518798828125, \"iteration\": 1258, \"epoch\": 6}, {\"training_acc\": 0.828125, \"training_loss\": 235.418212890625, \"iteration\": 1259, \"epoch\": 6}, {\"training_acc\": 0.8125, \"training_loss\": 264.471435546875, \"iteration\": 1260, \"epoch\": 6}, {\"training_acc\": 0.8359375, \"training_loss\": 312.5375061035156, \"iteration\": 1261, \"epoch\": 6}, {\"training_acc\": 0.8046875, \"training_loss\": 257.3839111328125, \"iteration\": 1262, \"epoch\": 6}, {\"training_acc\": 0.7421875, \"training_loss\": 245.27090454101562, \"iteration\": 1263, \"epoch\": 6}, {\"training_acc\": 0.8046875, \"training_loss\": 258.6201477050781, \"iteration\": 1264, \"epoch\": 6}, {\"training_acc\": 0.796875, \"training_loss\": 260.63116455078125, \"iteration\": 1265, \"epoch\": 6}, {\"training_acc\": 0.8125, \"training_loss\": 240.78564453125, \"iteration\": 1266, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 235.48171997070312, \"iteration\": 1267, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 266.33258056640625, \"iteration\": 1268, \"epoch\": 6}, {\"training_acc\": 0.796875, \"training_loss\": 253.12356567382812, \"iteration\": 1269, \"epoch\": 6}, {\"training_acc\": 0.921875, \"training_loss\": 257.2386474609375, \"iteration\": 1270, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 257.7903137207031, \"iteration\": 1271, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 280.52203369140625, \"iteration\": 1272, \"epoch\": 6}, {\"training_acc\": 0.8515625, \"training_loss\": 227.63128662109375, \"iteration\": 1273, \"epoch\": 6}, {\"training_acc\": 0.796875, \"training_loss\": 259.41650390625, \"iteration\": 1274, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 318.62054443359375, \"iteration\": 1275, \"epoch\": 6}, {\"training_acc\": 0.78125, \"training_loss\": 234.03713989257812, \"iteration\": 1276, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 255.9933624267578, \"iteration\": 1277, \"epoch\": 6}, {\"training_acc\": 0.8203125, \"training_loss\": 283.4119873046875, \"iteration\": 1278, \"epoch\": 6}, {\"training_acc\": 0.8515625, \"training_loss\": 227.08299255371094, \"iteration\": 1279, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 260.0467834472656, \"iteration\": 1280, \"epoch\": 6}, {\"training_acc\": 0.796875, \"training_loss\": 254.9476776123047, \"iteration\": 1281, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 287.70245361328125, \"iteration\": 1282, \"epoch\": 6}, {\"training_acc\": 0.8125, \"training_loss\": 275.0653381347656, \"iteration\": 1283, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 246.9081268310547, \"iteration\": 1284, \"epoch\": 6}, {\"training_acc\": 0.875, \"training_loss\": 252.86534118652344, \"iteration\": 1285, \"epoch\": 6}, {\"training_acc\": 0.8984375, \"training_loss\": 249.56060791015625, \"iteration\": 1286, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 267.04412841796875, \"iteration\": 1287, \"epoch\": 6}, {\"training_acc\": 0.8515625, \"training_loss\": 273.0758056640625, \"iteration\": 1288, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 240.27084350585938, \"iteration\": 1289, \"epoch\": 6}, {\"training_acc\": 0.765625, \"training_loss\": 265.91259765625, \"iteration\": 1290, \"epoch\": 6}, {\"training_acc\": 0.828125, \"training_loss\": 308.1370849609375, \"iteration\": 1291, \"epoch\": 6}, {\"training_acc\": 0.7734375, \"training_loss\": 292.4395751953125, \"iteration\": 1292, \"epoch\": 6}, {\"training_acc\": 0.7578125, \"training_loss\": 300.9552001953125, \"iteration\": 1293, \"epoch\": 6}, {\"training_acc\": 0.765625, \"training_loss\": 238.97943115234375, \"iteration\": 1294, \"epoch\": 6}, {\"training_acc\": 0.7890625, \"training_loss\": 244.9183349609375, \"iteration\": 1295, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 244.95516967773438, \"iteration\": 1296, \"epoch\": 6}, {\"training_acc\": 0.90625, \"training_loss\": 279.0438537597656, \"iteration\": 1297, \"epoch\": 6}, {\"training_acc\": 0.9296875, \"training_loss\": 286.4991760253906, \"iteration\": 1298, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 267.8197021484375, \"iteration\": 1299, \"epoch\": 6}, {\"training_acc\": 0.90625, \"training_loss\": 298.5469665527344, \"iteration\": 1300, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 255.036376953125, \"iteration\": 1301, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 276.76953125, \"iteration\": 1302, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 299.7066650390625, \"iteration\": 1303, \"epoch\": 6}, {\"training_acc\": 0.8515625, \"training_loss\": 246.80093383789062, \"iteration\": 1304, \"epoch\": 6}, {\"training_acc\": 0.828125, \"training_loss\": 253.78575134277344, \"iteration\": 1305, \"epoch\": 6}, {\"training_acc\": 0.78125, \"training_loss\": 264.53155517578125, \"iteration\": 1306, \"epoch\": 6}, {\"training_acc\": 0.7578125, \"training_loss\": 290.51898193359375, \"iteration\": 1307, \"epoch\": 6}, {\"training_acc\": 0.7890625, \"training_loss\": 267.2105407714844, \"iteration\": 1308, \"epoch\": 6}, {\"training_acc\": 0.7890625, \"training_loss\": 276.08251953125, \"iteration\": 1309, \"epoch\": 6}, {\"training_acc\": 0.84375, \"training_loss\": 318.8963623046875, \"iteration\": 1310, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 230.10728454589844, \"iteration\": 1311, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 297.2366027832031, \"iteration\": 1312, \"epoch\": 6}, {\"training_acc\": 0.796875, \"training_loss\": 224.92947387695312, \"iteration\": 1313, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 211.36459350585938, \"iteration\": 1314, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 236.8035888671875, \"iteration\": 1315, \"epoch\": 6}, {\"training_acc\": 0.890625, \"training_loss\": 196.02713012695312, \"iteration\": 1316, \"epoch\": 6}, {\"training_acc\": 0.8671875, \"training_loss\": 288.3734130859375, \"iteration\": 1317, \"epoch\": 6}, {\"training_acc\": 0.8828125, \"training_loss\": 289.7215576171875, \"iteration\": 1318, \"epoch\": 6}, {\"training_acc\": 0.8984375, \"training_loss\": 322.2220458984375, \"iteration\": 1319, \"epoch\": 6}, {\"training_acc\": 0.8359375, \"training_loss\": 288.57952880859375, \"iteration\": 1320, \"epoch\": 6}, {\"training_acc\": 0.828125, \"training_loss\": 283.42181396484375, \"iteration\": 1321, \"epoch\": 6}, {\"training_acc\": 0.765625, \"training_loss\": 304.5003662109375, \"iteration\": 1322, \"epoch\": 6}, {\"training_acc\": 0.703125, \"training_loss\": 234.25387573242188, \"iteration\": 1323, \"epoch\": 6}, {\"training_acc\": 0.7421875, \"training_loss\": 248.72813415527344, \"iteration\": 1324, \"epoch\": 6}, {\"training_acc\": 0.6875, \"training_loss\": 267.7612609863281, \"iteration\": 1325, \"epoch\": 6}, {\"training_acc\": 0.71875, \"training_loss\": 278.22003173828125, \"iteration\": 1326, \"epoch\": 6}, {\"training_acc\": 0.7890625, \"training_loss\": 249.45338439941406, \"iteration\": 1327, \"epoch\": 6}, {\"training_acc\": 0.7734375, \"training_loss\": 242.66909790039062, \"iteration\": 1328, \"epoch\": 6}, {\"training_acc\": 0.8046875, \"training_loss\": 239.47869873046875, \"iteration\": 1329, \"epoch\": 6}, {\"training_acc\": 0.859375, \"training_loss\": 331.3792724609375, \"iteration\": 1330, \"epoch\": 6}, {\"training_acc\": 0.8125, \"training_loss\": 245.98233032226562, \"iteration\": 1331, \"epoch\": 6}, {\"training_acc\": 0.8163265306122449, \"training_loss\": 84.18266296386719, \"iteration\": 1332, \"epoch\": 6}, {\"training_acc\": 0.8515625, \"training_loss\": 208.56109619140625, \"iteration\": 1333, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 227.46731567382812, \"iteration\": 1334, \"epoch\": 7}, {\"training_acc\": 0.90625, \"training_loss\": 308.99603271484375, \"iteration\": 1335, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 232.6073455810547, \"iteration\": 1336, \"epoch\": 7}, {\"training_acc\": 0.890625, \"training_loss\": 224.64491271972656, \"iteration\": 1337, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 219.01528930664062, \"iteration\": 1338, \"epoch\": 7}, {\"training_acc\": 0.8359375, \"training_loss\": 259.49969482421875, \"iteration\": 1339, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 303.9503479003906, \"iteration\": 1340, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 258.8616943359375, \"iteration\": 1341, \"epoch\": 7}, {\"training_acc\": 0.6875, \"training_loss\": 226.94332885742188, \"iteration\": 1342, \"epoch\": 7}, {\"training_acc\": 0.78125, \"training_loss\": 242.67510986328125, \"iteration\": 1343, \"epoch\": 7}, {\"training_acc\": 0.828125, \"training_loss\": 266.12554931640625, \"iteration\": 1344, \"epoch\": 7}, {\"training_acc\": 0.796875, \"training_loss\": 240.3088836669922, \"iteration\": 1345, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 272.1024169921875, \"iteration\": 1346, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 295.5956115722656, \"iteration\": 1347, \"epoch\": 7}, {\"training_acc\": 0.8984375, \"training_loss\": 274.20733642578125, \"iteration\": 1348, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 276.17901611328125, \"iteration\": 1349, \"epoch\": 7}, {\"training_acc\": 0.90625, \"training_loss\": 289.73974609375, \"iteration\": 1350, \"epoch\": 7}, {\"training_acc\": 0.921875, \"training_loss\": 290.4466247558594, \"iteration\": 1351, \"epoch\": 7}, {\"training_acc\": 0.84375, \"training_loss\": 310.87945556640625, \"iteration\": 1352, \"epoch\": 7}, {\"training_acc\": 0.8984375, \"training_loss\": 339.9052734375, \"iteration\": 1353, \"epoch\": 7}, {\"training_acc\": 0.921875, \"training_loss\": 321.5966796875, \"iteration\": 1354, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 259.91644287109375, \"iteration\": 1355, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 285.1553955078125, \"iteration\": 1356, \"epoch\": 7}, {\"training_acc\": 0.875, \"training_loss\": 239.42771911621094, \"iteration\": 1357, \"epoch\": 7}, {\"training_acc\": 0.890625, \"training_loss\": 239.93173217773438, \"iteration\": 1358, \"epoch\": 7}, {\"training_acc\": 0.890625, \"training_loss\": 276.4135437011719, \"iteration\": 1359, \"epoch\": 7}, {\"training_acc\": 0.8984375, \"training_loss\": 210.08331298828125, \"iteration\": 1360, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 254.1728515625, \"iteration\": 1361, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 260.44549560546875, \"iteration\": 1362, \"epoch\": 7}, {\"training_acc\": 0.96875, \"training_loss\": 298.79864501953125, \"iteration\": 1363, \"epoch\": 7}, {\"training_acc\": 0.921875, \"training_loss\": 281.3089599609375, \"iteration\": 1364, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 239.1106414794922, \"iteration\": 1365, \"epoch\": 7}, {\"training_acc\": 0.953125, \"training_loss\": 271.3360595703125, \"iteration\": 1366, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 275.6824035644531, \"iteration\": 1367, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 255.74166870117188, \"iteration\": 1368, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 239.84674072265625, \"iteration\": 1369, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 228.53533935546875, \"iteration\": 1370, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 259.08880615234375, \"iteration\": 1371, \"epoch\": 7}, {\"training_acc\": 0.8984375, \"training_loss\": 212.48846435546875, \"iteration\": 1372, \"epoch\": 7}, {\"training_acc\": 0.921875, \"training_loss\": 315.30108642578125, \"iteration\": 1373, \"epoch\": 7}, {\"training_acc\": 0.8984375, \"training_loss\": 274.4781799316406, \"iteration\": 1374, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 279.8910827636719, \"iteration\": 1375, \"epoch\": 7}, {\"training_acc\": 0.890625, \"training_loss\": 244.3876953125, \"iteration\": 1376, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 314.99700927734375, \"iteration\": 1377, \"epoch\": 7}, {\"training_acc\": 0.890625, \"training_loss\": 276.23590087890625, \"iteration\": 1378, \"epoch\": 7}, {\"training_acc\": 0.9296875, \"training_loss\": 264.4165954589844, \"iteration\": 1379, \"epoch\": 7}, {\"training_acc\": 0.8984375, \"training_loss\": 275.12677001953125, \"iteration\": 1380, \"epoch\": 7}, {\"training_acc\": 0.8984375, \"training_loss\": 225.55401611328125, \"iteration\": 1381, \"epoch\": 7}, {\"training_acc\": 0.9375, \"training_loss\": 305.51385498046875, \"iteration\": 1382, \"epoch\": 7}, {\"training_acc\": 0.90625, \"training_loss\": 254.34185791015625, \"iteration\": 1383, \"epoch\": 7}, {\"training_acc\": 0.921875, \"training_loss\": 190.07949829101562, \"iteration\": 1384, \"epoch\": 7}, {\"training_acc\": 0.9296875, \"training_loss\": 255.09140014648438, \"iteration\": 1385, \"epoch\": 7}, {\"training_acc\": 0.9453125, \"training_loss\": 263.142578125, \"iteration\": 1386, \"epoch\": 7}, {\"training_acc\": 0.9375, \"training_loss\": 254.1031494140625, \"iteration\": 1387, \"epoch\": 7}, {\"training_acc\": 0.9453125, \"training_loss\": 326.89715576171875, \"iteration\": 1388, \"epoch\": 7}, {\"training_acc\": 0.921875, \"training_loss\": 273.0220947265625, \"iteration\": 1389, \"epoch\": 7}, {\"training_acc\": 0.90625, \"training_loss\": 292.0215148925781, \"iteration\": 1390, \"epoch\": 7}, {\"training_acc\": 0.890625, \"training_loss\": 338.1190185546875, \"iteration\": 1391, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 233.30267333984375, \"iteration\": 1392, \"epoch\": 7}, {\"training_acc\": 0.8515625, \"training_loss\": 243.67282104492188, \"iteration\": 1393, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 253.41946411132812, \"iteration\": 1394, \"epoch\": 7}, {\"training_acc\": 0.8984375, \"training_loss\": 298.35321044921875, \"iteration\": 1395, \"epoch\": 7}, {\"training_acc\": 0.8515625, \"training_loss\": 256.4635925292969, \"iteration\": 1396, \"epoch\": 7}, {\"training_acc\": 0.8515625, \"training_loss\": 259.9716491699219, \"iteration\": 1397, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 287.39501953125, \"iteration\": 1398, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 302.6014404296875, \"iteration\": 1399, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 315.9307861328125, \"iteration\": 1400, \"epoch\": 7}, {\"training_acc\": 0.9453125, \"training_loss\": 278.55474853515625, \"iteration\": 1401, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 225.23202514648438, \"iteration\": 1402, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 232.5155487060547, \"iteration\": 1403, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 297.18389892578125, \"iteration\": 1404, \"epoch\": 7}, {\"training_acc\": 0.890625, \"training_loss\": 254.85409545898438, \"iteration\": 1405, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 266.6676025390625, \"iteration\": 1406, \"epoch\": 7}, {\"training_acc\": 0.8046875, \"training_loss\": 290.4407043457031, \"iteration\": 1407, \"epoch\": 7}, {\"training_acc\": 0.90625, \"training_loss\": 291.033447265625, \"iteration\": 1408, \"epoch\": 7}, {\"training_acc\": 0.8515625, \"training_loss\": 303.1717224121094, \"iteration\": 1409, \"epoch\": 7}, {\"training_acc\": 0.8359375, \"training_loss\": 323.13861083984375, \"iteration\": 1410, \"epoch\": 7}, {\"training_acc\": 0.7734375, \"training_loss\": 220.4236602783203, \"iteration\": 1411, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 296.22906494140625, \"iteration\": 1412, \"epoch\": 7}, {\"training_acc\": 0.875, \"training_loss\": 225.18666076660156, \"iteration\": 1413, \"epoch\": 7}, {\"training_acc\": 0.8359375, \"training_loss\": 299.4209899902344, \"iteration\": 1414, \"epoch\": 7}, {\"training_acc\": 0.8984375, \"training_loss\": 315.2923583984375, \"iteration\": 1415, \"epoch\": 7}, {\"training_acc\": 0.828125, \"training_loss\": 210.5100860595703, \"iteration\": 1416, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 268.9307556152344, \"iteration\": 1417, \"epoch\": 7}, {\"training_acc\": 0.84375, \"training_loss\": 221.2553253173828, \"iteration\": 1418, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 263.96734619140625, \"iteration\": 1419, \"epoch\": 7}, {\"training_acc\": 0.8515625, \"training_loss\": 262.59417724609375, \"iteration\": 1420, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 295.0389404296875, \"iteration\": 1421, \"epoch\": 7}, {\"training_acc\": 0.8515625, \"training_loss\": 288.9935607910156, \"iteration\": 1422, \"epoch\": 7}, {\"training_acc\": 0.8359375, \"training_loss\": 248.09707641601562, \"iteration\": 1423, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 290.2913818359375, \"iteration\": 1424, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 216.44049072265625, \"iteration\": 1425, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 302.7977600097656, \"iteration\": 1426, \"epoch\": 7}, {\"training_acc\": 0.7890625, \"training_loss\": 259.7865905761719, \"iteration\": 1427, \"epoch\": 7}, {\"training_acc\": 0.84375, \"training_loss\": 260.0451354980469, \"iteration\": 1428, \"epoch\": 7}, {\"training_acc\": 0.796875, \"training_loss\": 287.24371337890625, \"iteration\": 1429, \"epoch\": 7}, {\"training_acc\": 0.8359375, \"training_loss\": 261.7852783203125, \"iteration\": 1430, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 245.34539794921875, \"iteration\": 1431, \"epoch\": 7}, {\"training_acc\": 0.84375, \"training_loss\": 215.1140899658203, \"iteration\": 1432, \"epoch\": 7}, {\"training_acc\": 0.90625, \"training_loss\": 243.69866943359375, \"iteration\": 1433, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 244.02230834960938, \"iteration\": 1434, \"epoch\": 7}, {\"training_acc\": 0.9296875, \"training_loss\": 267.7161865234375, \"iteration\": 1435, \"epoch\": 7}, {\"training_acc\": 0.9375, \"training_loss\": 275.0794677734375, \"iteration\": 1436, \"epoch\": 7}, {\"training_acc\": 0.90625, \"training_loss\": 248.82464599609375, \"iteration\": 1437, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 225.27809143066406, \"iteration\": 1438, \"epoch\": 7}, {\"training_acc\": 0.8515625, \"training_loss\": 265.35260009765625, \"iteration\": 1439, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 249.73439025878906, \"iteration\": 1440, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 300.5381164550781, \"iteration\": 1441, \"epoch\": 7}, {\"training_acc\": 0.828125, \"training_loss\": 281.31121826171875, \"iteration\": 1442, \"epoch\": 7}, {\"training_acc\": 0.8046875, \"training_loss\": 274.35089111328125, \"iteration\": 1443, \"epoch\": 7}, {\"training_acc\": 0.875, \"training_loss\": 275.7445068359375, \"iteration\": 1444, \"epoch\": 7}, {\"training_acc\": 0.84375, \"training_loss\": 220.17405700683594, \"iteration\": 1445, \"epoch\": 7}, {\"training_acc\": 0.8046875, \"training_loss\": 227.1510009765625, \"iteration\": 1446, \"epoch\": 7}, {\"training_acc\": 0.8984375, \"training_loss\": 290.82794189453125, \"iteration\": 1447, \"epoch\": 7}, {\"training_acc\": 0.875, \"training_loss\": 285.68310546875, \"iteration\": 1448, \"epoch\": 7}, {\"training_acc\": 0.921875, \"training_loss\": 281.32904052734375, \"iteration\": 1449, \"epoch\": 7}, {\"training_acc\": 0.9375, \"training_loss\": 284.8633728027344, \"iteration\": 1450, \"epoch\": 7}, {\"training_acc\": 0.890625, \"training_loss\": 261.41876220703125, \"iteration\": 1451, \"epoch\": 7}, {\"training_acc\": 0.9453125, \"training_loss\": 283.2222900390625, \"iteration\": 1452, \"epoch\": 7}, {\"training_acc\": 0.9453125, \"training_loss\": 232.78045654296875, \"iteration\": 1453, \"epoch\": 7}, {\"training_acc\": 0.921875, \"training_loss\": 283.7581787109375, \"iteration\": 1454, \"epoch\": 7}, {\"training_acc\": 0.953125, \"training_loss\": 279.4464416503906, \"iteration\": 1455, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 253.60147094726562, \"iteration\": 1456, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 291.42315673828125, \"iteration\": 1457, \"epoch\": 7}, {\"training_acc\": 0.84375, \"training_loss\": 286.6005859375, \"iteration\": 1458, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 271.0012512207031, \"iteration\": 1459, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 280.4769592285156, \"iteration\": 1460, \"epoch\": 7}, {\"training_acc\": 0.8359375, \"training_loss\": 278.2657470703125, \"iteration\": 1461, \"epoch\": 7}, {\"training_acc\": 0.7890625, \"training_loss\": 252.77830505371094, \"iteration\": 1462, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 268.89935302734375, \"iteration\": 1463, \"epoch\": 7}, {\"training_acc\": 0.90625, \"training_loss\": 255.84951782226562, \"iteration\": 1464, \"epoch\": 7}, {\"training_acc\": 0.875, \"training_loss\": 296.03997802734375, \"iteration\": 1465, \"epoch\": 7}, {\"training_acc\": 0.828125, \"training_loss\": 247.0677490234375, \"iteration\": 1466, \"epoch\": 7}, {\"training_acc\": 0.9375, \"training_loss\": 295.6063232421875, \"iteration\": 1467, \"epoch\": 7}, {\"training_acc\": 0.9453125, \"training_loss\": 241.78485107421875, \"iteration\": 1468, \"epoch\": 7}, {\"training_acc\": 0.90625, \"training_loss\": 266.78533935546875, \"iteration\": 1469, \"epoch\": 7}, {\"training_acc\": 0.890625, \"training_loss\": 285.30926513671875, \"iteration\": 1470, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 313.47918701171875, \"iteration\": 1471, \"epoch\": 7}, {\"training_acc\": 0.875, \"training_loss\": 261.7564392089844, \"iteration\": 1472, \"epoch\": 7}, {\"training_acc\": 0.9375, \"training_loss\": 233.30557250976562, \"iteration\": 1473, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 246.51702880859375, \"iteration\": 1474, \"epoch\": 7}, {\"training_acc\": 0.90625, \"training_loss\": 242.83628845214844, \"iteration\": 1475, \"epoch\": 7}, {\"training_acc\": 0.890625, \"training_loss\": 265.77642822265625, \"iteration\": 1476, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 278.1943664550781, \"iteration\": 1477, \"epoch\": 7}, {\"training_acc\": 0.78125, \"training_loss\": 268.8948974609375, \"iteration\": 1478, \"epoch\": 7}, {\"training_acc\": 0.7734375, \"training_loss\": 241.571044921875, \"iteration\": 1479, \"epoch\": 7}, {\"training_acc\": 0.7734375, \"training_loss\": 219.68380737304688, \"iteration\": 1480, \"epoch\": 7}, {\"training_acc\": 0.7890625, \"training_loss\": 251.8465576171875, \"iteration\": 1481, \"epoch\": 7}, {\"training_acc\": 0.84375, \"training_loss\": 264.2367248535156, \"iteration\": 1482, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 311.7127380371094, \"iteration\": 1483, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 267.01611328125, \"iteration\": 1484, \"epoch\": 7}, {\"training_acc\": 0.875, \"training_loss\": 246.34524536132812, \"iteration\": 1485, \"epoch\": 7}, {\"training_acc\": 0.8515625, \"training_loss\": 267.4325866699219, \"iteration\": 1486, \"epoch\": 7}, {\"training_acc\": 0.84375, \"training_loss\": 231.33255004882812, \"iteration\": 1487, \"epoch\": 7}, {\"training_acc\": 0.875, \"training_loss\": 296.47552490234375, \"iteration\": 1488, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 306.6426086425781, \"iteration\": 1489, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 271.838134765625, \"iteration\": 1490, \"epoch\": 7}, {\"training_acc\": 0.765625, \"training_loss\": 225.63424682617188, \"iteration\": 1491, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 295.3238525390625, \"iteration\": 1492, \"epoch\": 7}, {\"training_acc\": 0.7109375, \"training_loss\": 218.86834716796875, \"iteration\": 1493, \"epoch\": 7}, {\"training_acc\": 0.7421875, \"training_loss\": 287.82537841796875, \"iteration\": 1494, \"epoch\": 7}, {\"training_acc\": 0.7734375, \"training_loss\": 212.5458984375, \"iteration\": 1495, \"epoch\": 7}, {\"training_acc\": 0.7734375, \"training_loss\": 221.3156280517578, \"iteration\": 1496, \"epoch\": 7}, {\"training_acc\": 0.796875, \"training_loss\": 235.8800506591797, \"iteration\": 1497, \"epoch\": 7}, {\"training_acc\": 0.8125, \"training_loss\": 329.78704833984375, \"iteration\": 1498, \"epoch\": 7}, {\"training_acc\": 0.7265625, \"training_loss\": 182.6650390625, \"iteration\": 1499, \"epoch\": 7}, {\"training_acc\": 0.84375, \"training_loss\": 267.2552795410156, \"iteration\": 1500, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 291.1536865234375, \"iteration\": 1501, \"epoch\": 7}, {\"training_acc\": 0.7421875, \"training_loss\": 244.29876708984375, \"iteration\": 1502, \"epoch\": 7}, {\"training_acc\": 0.8125, \"training_loss\": 290.9893493652344, \"iteration\": 1503, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 314.50677490234375, \"iteration\": 1504, \"epoch\": 7}, {\"training_acc\": 0.7890625, \"training_loss\": 254.03070068359375, \"iteration\": 1505, \"epoch\": 7}, {\"training_acc\": 0.7734375, \"training_loss\": 278.267333984375, \"iteration\": 1506, \"epoch\": 7}, {\"training_acc\": 0.8046875, \"training_loss\": 296.91339111328125, \"iteration\": 1507, \"epoch\": 7}, {\"training_acc\": 0.7890625, \"training_loss\": 293.7884826660156, \"iteration\": 1508, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 320.78302001953125, \"iteration\": 1509, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 303.5242004394531, \"iteration\": 1510, \"epoch\": 7}, {\"training_acc\": 0.8125, \"training_loss\": 271.569580078125, \"iteration\": 1511, \"epoch\": 7}, {\"training_acc\": 0.8125, \"training_loss\": 318.1693115234375, \"iteration\": 1512, \"epoch\": 7}, {\"training_acc\": 0.828125, \"training_loss\": 233.9815216064453, \"iteration\": 1513, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 195.50914001464844, \"iteration\": 1514, \"epoch\": 7}, {\"training_acc\": 0.796875, \"training_loss\": 276.869384765625, \"iteration\": 1515, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 291.69696044921875, \"iteration\": 1516, \"epoch\": 7}, {\"training_acc\": 0.7734375, \"training_loss\": 309.01251220703125, \"iteration\": 1517, \"epoch\": 7}, {\"training_acc\": 0.8046875, \"training_loss\": 269.55517578125, \"iteration\": 1518, \"epoch\": 7}, {\"training_acc\": 0.7890625, \"training_loss\": 234.5925750732422, \"iteration\": 1519, \"epoch\": 7}, {\"training_acc\": 0.7265625, \"training_loss\": 281.1260986328125, \"iteration\": 1520, \"epoch\": 7}, {\"training_acc\": 0.765625, \"training_loss\": 255.74156188964844, \"iteration\": 1521, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 271.9113464355469, \"iteration\": 1522, \"epoch\": 7}, {\"training_acc\": 0.7734375, \"training_loss\": 252.51441955566406, \"iteration\": 1523, \"epoch\": 7}, {\"training_acc\": 0.8125, \"training_loss\": 257.2264404296875, \"iteration\": 1524, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 235.16278076171875, \"iteration\": 1525, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 307.2939758300781, \"iteration\": 1526, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 248.1104278564453, \"iteration\": 1527, \"epoch\": 7}, {\"training_acc\": 0.8125, \"training_loss\": 221.34332275390625, \"iteration\": 1528, \"epoch\": 7}, {\"training_acc\": 0.8046875, \"training_loss\": 230.53359985351562, \"iteration\": 1529, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 296.828857421875, \"iteration\": 1530, \"epoch\": 7}, {\"training_acc\": 0.7734375, \"training_loss\": 232.41075134277344, \"iteration\": 1531, \"epoch\": 7}, {\"training_acc\": 0.8984375, \"training_loss\": 266.1610107421875, \"iteration\": 1532, \"epoch\": 7}, {\"training_acc\": 0.859375, \"training_loss\": 283.04278564453125, \"iteration\": 1533, \"epoch\": 7}, {\"training_acc\": 0.765625, \"training_loss\": 262.52557373046875, \"iteration\": 1534, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 272.3476257324219, \"iteration\": 1535, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 255.08193969726562, \"iteration\": 1536, \"epoch\": 7}, {\"training_acc\": 0.8359375, \"training_loss\": 298.2734375, \"iteration\": 1537, \"epoch\": 7}, {\"training_acc\": 0.8359375, \"training_loss\": 290.347900390625, \"iteration\": 1538, \"epoch\": 7}, {\"training_acc\": 0.78125, \"training_loss\": 212.80718994140625, \"iteration\": 1539, \"epoch\": 7}, {\"training_acc\": 0.8125, \"training_loss\": 270.56500244140625, \"iteration\": 1540, \"epoch\": 7}, {\"training_acc\": 0.8671875, \"training_loss\": 270.2115783691406, \"iteration\": 1541, \"epoch\": 7}, {\"training_acc\": 0.8359375, \"training_loss\": 198.97833251953125, \"iteration\": 1542, \"epoch\": 7}, {\"training_acc\": 0.9375, \"training_loss\": 306.5125427246094, \"iteration\": 1543, \"epoch\": 7}, {\"training_acc\": 0.9140625, \"training_loss\": 329.74853515625, \"iteration\": 1544, \"epoch\": 7}, {\"training_acc\": 0.8828125, \"training_loss\": 249.60118103027344, \"iteration\": 1545, \"epoch\": 7}, {\"training_acc\": 0.8203125, \"training_loss\": 267.39654541015625, \"iteration\": 1546, \"epoch\": 7}, {\"training_acc\": 0.90625, \"training_loss\": 275.2806396484375, \"iteration\": 1547, \"epoch\": 7}, {\"training_acc\": 0.875, \"training_loss\": 211.36428833007812, \"iteration\": 1548, \"epoch\": 7}, {\"training_acc\": 0.8359375, \"training_loss\": 270.3457946777344, \"iteration\": 1549, \"epoch\": 7}, {\"training_acc\": 0.875, \"training_loss\": 333.49407958984375, \"iteration\": 1550, \"epoch\": 7}, {\"training_acc\": 0.8046875, \"training_loss\": 266.93585205078125, \"iteration\": 1551, \"epoch\": 7}, {\"training_acc\": 0.71875, \"training_loss\": 254.90408325195312, \"iteration\": 1552, \"epoch\": 7}, {\"training_acc\": 0.7421875, \"training_loss\": 239.4803466796875, \"iteration\": 1553, \"epoch\": 7}, {\"training_acc\": 0.8367346938775511, \"training_loss\": 77.59031677246094, \"iteration\": 1554, \"epoch\": 7}, {\"training_acc\": 0.8046875, \"training_loss\": 268.473388671875, \"iteration\": 1555, \"epoch\": 8}, {\"training_acc\": 0.78125, \"training_loss\": 232.14846801757812, \"iteration\": 1556, \"epoch\": 8}, {\"training_acc\": 0.7265625, \"training_loss\": 197.8432159423828, \"iteration\": 1557, \"epoch\": 8}, {\"training_acc\": 0.796875, \"training_loss\": 233.42401123046875, \"iteration\": 1558, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 300.606201171875, \"iteration\": 1559, \"epoch\": 8}, {\"training_acc\": 0.8515625, \"training_loss\": 280.095703125, \"iteration\": 1560, \"epoch\": 8}, {\"training_acc\": 0.828125, \"training_loss\": 235.3212890625, \"iteration\": 1561, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 219.49293518066406, \"iteration\": 1562, \"epoch\": 8}, {\"training_acc\": 0.8515625, \"training_loss\": 301.1633605957031, \"iteration\": 1563, \"epoch\": 8}, {\"training_acc\": 0.8125, \"training_loss\": 240.83334350585938, \"iteration\": 1564, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 227.08172607421875, \"iteration\": 1565, \"epoch\": 8}, {\"training_acc\": 0.7890625, \"training_loss\": 269.8250732421875, \"iteration\": 1566, \"epoch\": 8}, {\"training_acc\": 0.7734375, \"training_loss\": 242.3880615234375, \"iteration\": 1567, \"epoch\": 8}, {\"training_acc\": 0.8515625, \"training_loss\": 263.3282775878906, \"iteration\": 1568, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 258.3714599609375, \"iteration\": 1569, \"epoch\": 8}, {\"training_acc\": 0.8515625, \"training_loss\": 249.01678466796875, \"iteration\": 1570, \"epoch\": 8}, {\"training_acc\": 0.796875, \"training_loss\": 266.6837158203125, \"iteration\": 1571, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 270.20294189453125, \"iteration\": 1572, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 279.81195068359375, \"iteration\": 1573, \"epoch\": 8}, {\"training_acc\": 0.859375, \"training_loss\": 269.2086181640625, \"iteration\": 1574, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 269.65167236328125, \"iteration\": 1575, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 290.918701171875, \"iteration\": 1576, \"epoch\": 8}, {\"training_acc\": 0.890625, \"training_loss\": 307.5538635253906, \"iteration\": 1577, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 333.6976013183594, \"iteration\": 1578, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 251.75267028808594, \"iteration\": 1579, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 249.6745147705078, \"iteration\": 1580, \"epoch\": 8}, {\"training_acc\": 0.90625, \"training_loss\": 279.2803955078125, \"iteration\": 1581, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 250.8792724609375, \"iteration\": 1582, \"epoch\": 8}, {\"training_acc\": 0.875, \"training_loss\": 242.67800903320312, \"iteration\": 1583, \"epoch\": 8}, {\"training_acc\": 0.90625, \"training_loss\": 280.30535888671875, \"iteration\": 1584, \"epoch\": 8}, {\"training_acc\": 0.921875, \"training_loss\": 256.7413024902344, \"iteration\": 1585, \"epoch\": 8}, {\"training_acc\": 0.953125, \"training_loss\": 273.96820068359375, \"iteration\": 1586, \"epoch\": 8}, {\"training_acc\": 0.921875, \"training_loss\": 221.70497131347656, \"iteration\": 1587, \"epoch\": 8}, {\"training_acc\": 0.9453125, \"training_loss\": 299.4375, \"iteration\": 1588, \"epoch\": 8}, {\"training_acc\": 0.859375, \"training_loss\": 241.97662353515625, \"iteration\": 1589, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 256.4809265136719, \"iteration\": 1590, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 274.2866516113281, \"iteration\": 1591, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 192.4686279296875, \"iteration\": 1592, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 261.27386474609375, \"iteration\": 1593, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 261.01788330078125, \"iteration\": 1594, \"epoch\": 8}, {\"training_acc\": 0.90625, \"training_loss\": 296.63641357421875, \"iteration\": 1595, \"epoch\": 8}, {\"training_acc\": 0.90625, \"training_loss\": 305.97869873046875, \"iteration\": 1596, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 285.64093017578125, \"iteration\": 1597, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 235.05804443359375, \"iteration\": 1598, \"epoch\": 8}, {\"training_acc\": 0.953125, \"training_loss\": 259.7276916503906, \"iteration\": 1599, \"epoch\": 8}, {\"training_acc\": 0.9375, \"training_loss\": 309.7930908203125, \"iteration\": 1600, \"epoch\": 8}, {\"training_acc\": 0.921875, \"training_loss\": 229.39390563964844, \"iteration\": 1601, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 282.1969299316406, \"iteration\": 1602, \"epoch\": 8}, {\"training_acc\": 0.9453125, \"training_loss\": 270.06103515625, \"iteration\": 1603, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 286.3778991699219, \"iteration\": 1604, \"epoch\": 8}, {\"training_acc\": 0.875, \"training_loss\": 258.889892578125, \"iteration\": 1605, \"epoch\": 8}, {\"training_acc\": 0.859375, \"training_loss\": 271.861083984375, \"iteration\": 1606, \"epoch\": 8}, {\"training_acc\": 0.7890625, \"training_loss\": 311.42706298828125, \"iteration\": 1607, \"epoch\": 8}, {\"training_acc\": 0.78125, \"training_loss\": 261.9206848144531, \"iteration\": 1608, \"epoch\": 8}, {\"training_acc\": 0.78125, \"training_loss\": 315.9510192871094, \"iteration\": 1609, \"epoch\": 8}, {\"training_acc\": 0.7734375, \"training_loss\": 289.3829345703125, \"iteration\": 1610, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 301.350830078125, \"iteration\": 1611, \"epoch\": 8}, {\"training_acc\": 0.7734375, \"training_loss\": 233.23135375976562, \"iteration\": 1612, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 272.2950744628906, \"iteration\": 1613, \"epoch\": 8}, {\"training_acc\": 0.8828125, \"training_loss\": 273.00628662109375, \"iteration\": 1614, \"epoch\": 8}, {\"training_acc\": 0.9296875, \"training_loss\": 268.25726318359375, \"iteration\": 1615, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 270.45166015625, \"iteration\": 1616, \"epoch\": 8}, {\"training_acc\": 0.90625, \"training_loss\": 234.55372619628906, \"iteration\": 1617, \"epoch\": 8}, {\"training_acc\": 0.90625, \"training_loss\": 227.91836547851562, \"iteration\": 1618, \"epoch\": 8}, {\"training_acc\": 0.9453125, \"training_loss\": 287.5972900390625, \"iteration\": 1619, \"epoch\": 8}, {\"training_acc\": 0.9296875, \"training_loss\": 329.70257568359375, \"iteration\": 1620, \"epoch\": 8}, {\"training_acc\": 0.921875, \"training_loss\": 258.40374755859375, \"iteration\": 1621, \"epoch\": 8}, {\"training_acc\": 0.8515625, \"training_loss\": 218.4322509765625, \"iteration\": 1622, \"epoch\": 8}, {\"training_acc\": 0.8046875, \"training_loss\": 241.66995239257812, \"iteration\": 1623, \"epoch\": 8}, {\"training_acc\": 0.8203125, \"training_loss\": 275.92266845703125, \"iteration\": 1624, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 276.052490234375, \"iteration\": 1625, \"epoch\": 8}, {\"training_acc\": 0.8125, \"training_loss\": 259.7281799316406, \"iteration\": 1626, \"epoch\": 8}, {\"training_acc\": 0.8203125, \"training_loss\": 261.873291015625, \"iteration\": 1627, \"epoch\": 8}, {\"training_acc\": 0.8203125, \"training_loss\": 298.5408935546875, \"iteration\": 1628, \"epoch\": 8}, {\"training_acc\": 0.7734375, \"training_loss\": 244.00128173828125, \"iteration\": 1629, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 251.2303466796875, \"iteration\": 1630, \"epoch\": 8}, {\"training_acc\": 0.90625, \"training_loss\": 316.66650390625, \"iteration\": 1631, \"epoch\": 8}, {\"training_acc\": 0.890625, \"training_loss\": 278.72052001953125, \"iteration\": 1632, \"epoch\": 8}, {\"training_acc\": 0.859375, \"training_loss\": 264.47119140625, \"iteration\": 1633, \"epoch\": 8}, {\"training_acc\": 0.90625, \"training_loss\": 239.21771240234375, \"iteration\": 1634, \"epoch\": 8}, {\"training_acc\": 0.921875, \"training_loss\": 263.91900634765625, \"iteration\": 1635, \"epoch\": 8}, {\"training_acc\": 0.875, \"training_loss\": 247.59698486328125, \"iteration\": 1636, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 223.35980224609375, \"iteration\": 1637, \"epoch\": 8}, {\"training_acc\": 0.9375, \"training_loss\": 237.80584716796875, \"iteration\": 1638, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 287.68353271484375, \"iteration\": 1639, \"epoch\": 8}, {\"training_acc\": 0.890625, \"training_loss\": 245.43072509765625, \"iteration\": 1640, \"epoch\": 8}, {\"training_acc\": 0.8515625, \"training_loss\": 283.20684814453125, \"iteration\": 1641, \"epoch\": 8}, {\"training_acc\": 0.8828125, \"training_loss\": 289.1032409667969, \"iteration\": 1642, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 262.4375915527344, \"iteration\": 1643, \"epoch\": 8}, {\"training_acc\": 0.828125, \"training_loss\": 251.08740234375, \"iteration\": 1644, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 255.0675506591797, \"iteration\": 1645, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 235.6503143310547, \"iteration\": 1646, \"epoch\": 8}, {\"training_acc\": 0.875, \"training_loss\": 220.188720703125, \"iteration\": 1647, \"epoch\": 8}, {\"training_acc\": 0.9453125, \"training_loss\": 306.1177673339844, \"iteration\": 1648, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 232.65533447265625, \"iteration\": 1649, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 250.10867309570312, \"iteration\": 1650, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 293.5249328613281, \"iteration\": 1651, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 291.8264465332031, \"iteration\": 1652, \"epoch\": 8}, {\"training_acc\": 0.875, \"training_loss\": 270.65802001953125, \"iteration\": 1653, \"epoch\": 8}, {\"training_acc\": 0.8828125, \"training_loss\": 240.06906127929688, \"iteration\": 1654, \"epoch\": 8}, {\"training_acc\": 0.9453125, \"training_loss\": 262.9171447753906, \"iteration\": 1655, \"epoch\": 8}, {\"training_acc\": 0.9609375, \"training_loss\": 253.49246215820312, \"iteration\": 1656, \"epoch\": 8}, {\"training_acc\": 0.890625, \"training_loss\": 297.64898681640625, \"iteration\": 1657, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 291.09228515625, \"iteration\": 1658, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 315.5706787109375, \"iteration\": 1659, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 249.56195068359375, \"iteration\": 1660, \"epoch\": 8}, {\"training_acc\": 0.890625, \"training_loss\": 310.2342224121094, \"iteration\": 1661, \"epoch\": 8}, {\"training_acc\": 0.859375, \"training_loss\": 291.85296630859375, \"iteration\": 1662, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 264.948974609375, \"iteration\": 1663, \"epoch\": 8}, {\"training_acc\": 0.8515625, \"training_loss\": 250.62197875976562, \"iteration\": 1664, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 270.5863037109375, \"iteration\": 1665, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 275.0666809082031, \"iteration\": 1666, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 276.36260986328125, \"iteration\": 1667, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 279.70452880859375, \"iteration\": 1668, \"epoch\": 8}, {\"training_acc\": 0.828125, \"training_loss\": 243.13418579101562, \"iteration\": 1669, \"epoch\": 8}, {\"training_acc\": 0.875, \"training_loss\": 328.53936767578125, \"iteration\": 1670, \"epoch\": 8}, {\"training_acc\": 0.828125, \"training_loss\": 250.5040740966797, \"iteration\": 1671, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 263.9129333496094, \"iteration\": 1672, \"epoch\": 8}, {\"training_acc\": 0.828125, \"training_loss\": 251.90921020507812, \"iteration\": 1673, \"epoch\": 8}, {\"training_acc\": 0.890625, \"training_loss\": 301.2599792480469, \"iteration\": 1674, \"epoch\": 8}, {\"training_acc\": 0.875, \"training_loss\": 340.97503662109375, \"iteration\": 1675, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 280.5406188964844, \"iteration\": 1676, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 250.12496948242188, \"iteration\": 1677, \"epoch\": 8}, {\"training_acc\": 0.875, \"training_loss\": 270.4458312988281, \"iteration\": 1678, \"epoch\": 8}, {\"training_acc\": 0.9375, \"training_loss\": 263.31561279296875, \"iteration\": 1679, \"epoch\": 8}, {\"training_acc\": 0.8828125, \"training_loss\": 294.4261474609375, \"iteration\": 1680, \"epoch\": 8}, {\"training_acc\": 0.90625, \"training_loss\": 340.9327392578125, \"iteration\": 1681, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 307.64141845703125, \"iteration\": 1682, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 224.98052978515625, \"iteration\": 1683, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 248.42645263671875, \"iteration\": 1684, \"epoch\": 8}, {\"training_acc\": 0.890625, \"training_loss\": 284.38299560546875, \"iteration\": 1685, \"epoch\": 8}, {\"training_acc\": 0.9296875, \"training_loss\": 275.28094482421875, \"iteration\": 1686, \"epoch\": 8}, {\"training_acc\": 0.90625, \"training_loss\": 227.441650390625, \"iteration\": 1687, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 244.89990234375, \"iteration\": 1688, \"epoch\": 8}, {\"training_acc\": 0.8828125, \"training_loss\": 237.61083984375, \"iteration\": 1689, \"epoch\": 8}, {\"training_acc\": 0.953125, \"training_loss\": 285.18414306640625, \"iteration\": 1690, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 232.9518280029297, \"iteration\": 1691, \"epoch\": 8}, {\"training_acc\": 0.8828125, \"training_loss\": 250.54055786132812, \"iteration\": 1692, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 304.5321044921875, \"iteration\": 1693, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 277.35699462890625, \"iteration\": 1694, \"epoch\": 8}, {\"training_acc\": 0.828125, \"training_loss\": 290.40521240234375, \"iteration\": 1695, \"epoch\": 8}, {\"training_acc\": 0.8046875, \"training_loss\": 266.9617919921875, \"iteration\": 1696, \"epoch\": 8}, {\"training_acc\": 0.7109375, \"training_loss\": 218.89913940429688, \"iteration\": 1697, \"epoch\": 8}, {\"training_acc\": 0.8203125, \"training_loss\": 330.713623046875, \"iteration\": 1698, \"epoch\": 8}, {\"training_acc\": 0.78125, \"training_loss\": 264.17706298828125, \"iteration\": 1699, \"epoch\": 8}, {\"training_acc\": 0.7890625, \"training_loss\": 301.72509765625, \"iteration\": 1700, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 267.264892578125, \"iteration\": 1701, \"epoch\": 8}, {\"training_acc\": 0.8125, \"training_loss\": 230.77621459960938, \"iteration\": 1702, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 264.32611083984375, \"iteration\": 1703, \"epoch\": 8}, {\"training_acc\": 0.9453125, \"training_loss\": 268.8765869140625, \"iteration\": 1704, \"epoch\": 8}, {\"training_acc\": 0.9375, \"training_loss\": 264.0916748046875, \"iteration\": 1705, \"epoch\": 8}, {\"training_acc\": 0.9609375, \"training_loss\": 272.7734375, \"iteration\": 1706, \"epoch\": 8}, {\"training_acc\": 0.9375, \"training_loss\": 263.8623046875, \"iteration\": 1707, \"epoch\": 8}, {\"training_acc\": 0.875, \"training_loss\": 264.4197692871094, \"iteration\": 1708, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 250.5772247314453, \"iteration\": 1709, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 278.23626708984375, \"iteration\": 1710, \"epoch\": 8}, {\"training_acc\": 0.796875, \"training_loss\": 265.4080810546875, \"iteration\": 1711, \"epoch\": 8}, {\"training_acc\": 0.8125, \"training_loss\": 246.03086853027344, \"iteration\": 1712, \"epoch\": 8}, {\"training_acc\": 0.828125, \"training_loss\": 327.70159912109375, \"iteration\": 1713, \"epoch\": 8}, {\"training_acc\": 0.828125, \"training_loss\": 271.81536865234375, \"iteration\": 1714, \"epoch\": 8}, {\"training_acc\": 0.7734375, \"training_loss\": 266.34613037109375, \"iteration\": 1715, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 246.42213439941406, \"iteration\": 1716, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 248.43606567382812, \"iteration\": 1717, \"epoch\": 8}, {\"training_acc\": 0.8203125, \"training_loss\": 245.93710327148438, \"iteration\": 1718, \"epoch\": 8}, {\"training_acc\": 0.921875, \"training_loss\": 293.326416015625, \"iteration\": 1719, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 292.5648193359375, \"iteration\": 1720, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 247.35281372070312, \"iteration\": 1721, \"epoch\": 8}, {\"training_acc\": 0.859375, \"training_loss\": 259.7538146972656, \"iteration\": 1722, \"epoch\": 8}, {\"training_acc\": 0.8046875, \"training_loss\": 251.21133422851562, \"iteration\": 1723, \"epoch\": 8}, {\"training_acc\": 0.8515625, \"training_loss\": 284.5477600097656, \"iteration\": 1724, \"epoch\": 8}, {\"training_acc\": 0.7734375, \"training_loss\": 257.5624694824219, \"iteration\": 1725, \"epoch\": 8}, {\"training_acc\": 0.828125, \"training_loss\": 275.5360107421875, \"iteration\": 1726, \"epoch\": 8}, {\"training_acc\": 0.796875, \"training_loss\": 303.7545471191406, \"iteration\": 1727, \"epoch\": 8}, {\"training_acc\": 0.8203125, \"training_loss\": 297.1383056640625, \"iteration\": 1728, \"epoch\": 8}, {\"training_acc\": 0.7734375, \"training_loss\": 227.67544555664062, \"iteration\": 1729, \"epoch\": 8}, {\"training_acc\": 0.765625, \"training_loss\": 260.0697326660156, \"iteration\": 1730, \"epoch\": 8}, {\"training_acc\": 0.7734375, \"training_loss\": 247.28553771972656, \"iteration\": 1731, \"epoch\": 8}, {\"training_acc\": 0.8125, \"training_loss\": 251.81597900390625, \"iteration\": 1732, \"epoch\": 8}, {\"training_acc\": 0.875, \"training_loss\": 276.7837219238281, \"iteration\": 1733, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 256.44525146484375, \"iteration\": 1734, \"epoch\": 8}, {\"training_acc\": 0.8203125, \"training_loss\": 216.80746459960938, \"iteration\": 1735, \"epoch\": 8}, {\"training_acc\": 0.859375, \"training_loss\": 240.5143280029297, \"iteration\": 1736, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 205.9962158203125, \"iteration\": 1737, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 258.03936767578125, \"iteration\": 1738, \"epoch\": 8}, {\"training_acc\": 0.9296875, \"training_loss\": 278.6432800292969, \"iteration\": 1739, \"epoch\": 8}, {\"training_acc\": 0.90625, \"training_loss\": 277.35833740234375, \"iteration\": 1740, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 234.23744201660156, \"iteration\": 1741, \"epoch\": 8}, {\"training_acc\": 0.859375, \"training_loss\": 261.73162841796875, \"iteration\": 1742, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 282.4619140625, \"iteration\": 1743, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 316.1435546875, \"iteration\": 1744, \"epoch\": 8}, {\"training_acc\": 0.78125, \"training_loss\": 240.58355712890625, \"iteration\": 1745, \"epoch\": 8}, {\"training_acc\": 0.8203125, \"training_loss\": 241.53628540039062, \"iteration\": 1746, \"epoch\": 8}, {\"training_acc\": 0.796875, \"training_loss\": 293.64923095703125, \"iteration\": 1747, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 303.12628173828125, \"iteration\": 1748, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 272.5652160644531, \"iteration\": 1749, \"epoch\": 8}, {\"training_acc\": 0.7890625, \"training_loss\": 289.84429931640625, \"iteration\": 1750, \"epoch\": 8}, {\"training_acc\": 0.8046875, \"training_loss\": 261.2625427246094, \"iteration\": 1751, \"epoch\": 8}, {\"training_acc\": 0.859375, \"training_loss\": 293.7042236328125, \"iteration\": 1752, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 238.60791015625, \"iteration\": 1753, \"epoch\": 8}, {\"training_acc\": 0.9375, \"training_loss\": 234.15689086914062, \"iteration\": 1754, \"epoch\": 8}, {\"training_acc\": 0.921875, \"training_loss\": 269.0540771484375, \"iteration\": 1755, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 244.53890991210938, \"iteration\": 1756, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 205.91555786132812, \"iteration\": 1757, \"epoch\": 8}, {\"training_acc\": 0.875, \"training_loss\": 254.68846130371094, \"iteration\": 1758, \"epoch\": 8}, {\"training_acc\": 0.9453125, \"training_loss\": 259.61566162109375, \"iteration\": 1759, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 272.04595947265625, \"iteration\": 1760, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 286.9410400390625, \"iteration\": 1761, \"epoch\": 8}, {\"training_acc\": 0.8984375, \"training_loss\": 263.99884033203125, \"iteration\": 1762, \"epoch\": 8}, {\"training_acc\": 0.8046875, \"training_loss\": 283.27679443359375, \"iteration\": 1763, \"epoch\": 8}, {\"training_acc\": 0.7734375, \"training_loss\": 268.7083435058594, \"iteration\": 1764, \"epoch\": 8}, {\"training_acc\": 0.8125, \"training_loss\": 236.2240753173828, \"iteration\": 1765, \"epoch\": 8}, {\"training_acc\": 0.8125, \"training_loss\": 296.16571044921875, \"iteration\": 1766, \"epoch\": 8}, {\"training_acc\": 0.8125, \"training_loss\": 273.395263671875, \"iteration\": 1767, \"epoch\": 8}, {\"training_acc\": 0.8359375, \"training_loss\": 247.22885131835938, \"iteration\": 1768, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 276.77520751953125, \"iteration\": 1769, \"epoch\": 8}, {\"training_acc\": 0.8828125, \"training_loss\": 235.7589569091797, \"iteration\": 1770, \"epoch\": 8}, {\"training_acc\": 0.8828125, \"training_loss\": 256.5035400390625, \"iteration\": 1771, \"epoch\": 8}, {\"training_acc\": 0.9140625, \"training_loss\": 265.81170654296875, \"iteration\": 1772, \"epoch\": 8}, {\"training_acc\": 0.890625, \"training_loss\": 317.40069580078125, \"iteration\": 1773, \"epoch\": 8}, {\"training_acc\": 0.8671875, \"training_loss\": 292.3108215332031, \"iteration\": 1774, \"epoch\": 8}, {\"training_acc\": 0.75, \"training_loss\": 232.87921142578125, \"iteration\": 1775, \"epoch\": 8}, {\"training_acc\": 0.7959183673469388, \"training_loss\": 60.79483413696289, \"iteration\": 1776, \"epoch\": 8}, {\"training_acc\": 0.84375, \"training_loss\": 300.68865966796875, \"iteration\": 1777, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 258.75439453125, \"iteration\": 1778, \"epoch\": 9}, {\"training_acc\": 0.7421875, \"training_loss\": 255.529541015625, \"iteration\": 1779, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 266.4523620605469, \"iteration\": 1780, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 245.44459533691406, \"iteration\": 1781, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 283.68804931640625, \"iteration\": 1782, \"epoch\": 9}, {\"training_acc\": 0.8671875, \"training_loss\": 249.89810180664062, \"iteration\": 1783, \"epoch\": 9}, {\"training_acc\": 0.875, \"training_loss\": 255.50506591796875, \"iteration\": 1784, \"epoch\": 9}, {\"training_acc\": 0.921875, \"training_loss\": 292.6968688964844, \"iteration\": 1785, \"epoch\": 9}, {\"training_acc\": 0.9140625, \"training_loss\": 341.20526123046875, \"iteration\": 1786, \"epoch\": 9}, {\"training_acc\": 0.890625, \"training_loss\": 274.747802734375, \"iteration\": 1787, \"epoch\": 9}, {\"training_acc\": 0.9375, \"training_loss\": 258.84710693359375, \"iteration\": 1788, \"epoch\": 9}, {\"training_acc\": 0.8984375, \"training_loss\": 233.80917358398438, \"iteration\": 1789, \"epoch\": 9}, {\"training_acc\": 0.8828125, \"training_loss\": 253.82504272460938, \"iteration\": 1790, \"epoch\": 9}, {\"training_acc\": 0.90625, \"training_loss\": 336.95599365234375, \"iteration\": 1791, \"epoch\": 9}, {\"training_acc\": 0.8984375, \"training_loss\": 270.14324951171875, \"iteration\": 1792, \"epoch\": 9}, {\"training_acc\": 0.8671875, \"training_loss\": 275.355712890625, \"iteration\": 1793, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 238.73870849609375, \"iteration\": 1794, \"epoch\": 9}, {\"training_acc\": 0.796875, \"training_loss\": 255.00877380371094, \"iteration\": 1795, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 260.3595275878906, \"iteration\": 1796, \"epoch\": 9}, {\"training_acc\": 0.7734375, \"training_loss\": 259.93817138671875, \"iteration\": 1797, \"epoch\": 9}, {\"training_acc\": 0.796875, \"training_loss\": 278.61865234375, \"iteration\": 1798, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 296.21881103515625, \"iteration\": 1799, \"epoch\": 9}, {\"training_acc\": 0.875, \"training_loss\": 279.6256408691406, \"iteration\": 1800, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 228.4473876953125, \"iteration\": 1801, \"epoch\": 9}, {\"training_acc\": 0.9375, \"training_loss\": 293.90533447265625, \"iteration\": 1802, \"epoch\": 9}, {\"training_acc\": 0.8984375, \"training_loss\": 319.0783386230469, \"iteration\": 1803, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 303.85723876953125, \"iteration\": 1804, \"epoch\": 9}, {\"training_acc\": 0.875, \"training_loss\": 295.8016052246094, \"iteration\": 1805, \"epoch\": 9}, {\"training_acc\": 0.90625, \"training_loss\": 288.8044738769531, \"iteration\": 1806, \"epoch\": 9}, {\"training_acc\": 0.90625, \"training_loss\": 299.56732177734375, \"iteration\": 1807, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 188.713134765625, \"iteration\": 1808, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 304.3500671386719, \"iteration\": 1809, \"epoch\": 9}, {\"training_acc\": 0.796875, \"training_loss\": 230.10145568847656, \"iteration\": 1810, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 266.1294860839844, \"iteration\": 1811, \"epoch\": 9}, {\"training_acc\": 0.796875, \"training_loss\": 224.79293823242188, \"iteration\": 1812, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 255.19696044921875, \"iteration\": 1813, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 254.0831298828125, \"iteration\": 1814, \"epoch\": 9}, {\"training_acc\": 0.875, \"training_loss\": 279.3960266113281, \"iteration\": 1815, \"epoch\": 9}, {\"training_acc\": 0.8671875, \"training_loss\": 275.93536376953125, \"iteration\": 1816, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 279.6318054199219, \"iteration\": 1817, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 240.7664337158203, \"iteration\": 1818, \"epoch\": 9}, {\"training_acc\": 0.8828125, \"training_loss\": 273.4927978515625, \"iteration\": 1819, \"epoch\": 9}, {\"training_acc\": 0.8671875, \"training_loss\": 249.44236755371094, \"iteration\": 1820, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 267.5323181152344, \"iteration\": 1821, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 279.3905029296875, \"iteration\": 1822, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 238.45254516601562, \"iteration\": 1823, \"epoch\": 9}, {\"training_acc\": 0.8203125, \"training_loss\": 268.3200378417969, \"iteration\": 1824, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 278.85748291015625, \"iteration\": 1825, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 270.35821533203125, \"iteration\": 1826, \"epoch\": 9}, {\"training_acc\": 0.8828125, \"training_loss\": 290.0802001953125, \"iteration\": 1827, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 243.57687377929688, \"iteration\": 1828, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 277.96875, \"iteration\": 1829, \"epoch\": 9}, {\"training_acc\": 0.7890625, \"training_loss\": 219.24169921875, \"iteration\": 1830, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 317.110107421875, \"iteration\": 1831, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 217.13885498046875, \"iteration\": 1832, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 233.75869750976562, \"iteration\": 1833, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 239.41763305664062, \"iteration\": 1834, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 300.5769958496094, \"iteration\": 1835, \"epoch\": 9}, {\"training_acc\": 0.890625, \"training_loss\": 217.02426147460938, \"iteration\": 1836, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 229.61077880859375, \"iteration\": 1837, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 293.25628662109375, \"iteration\": 1838, \"epoch\": 9}, {\"training_acc\": 0.8203125, \"training_loss\": 202.68084716796875, \"iteration\": 1839, \"epoch\": 9}, {\"training_acc\": 0.875, \"training_loss\": 260.215087890625, \"iteration\": 1840, \"epoch\": 9}, {\"training_acc\": 0.875, \"training_loss\": 291.79583740234375, \"iteration\": 1841, \"epoch\": 9}, {\"training_acc\": 0.8671875, \"training_loss\": 229.55831909179688, \"iteration\": 1842, \"epoch\": 9}, {\"training_acc\": 0.9140625, \"training_loss\": 268.125732421875, \"iteration\": 1843, \"epoch\": 9}, {\"training_acc\": 0.8828125, \"training_loss\": 276.06549072265625, \"iteration\": 1844, \"epoch\": 9}, {\"training_acc\": 0.875, \"training_loss\": 285.7349853515625, \"iteration\": 1845, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 248.17645263671875, \"iteration\": 1846, \"epoch\": 9}, {\"training_acc\": 0.8671875, \"training_loss\": 270.9869079589844, \"iteration\": 1847, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 288.3485107421875, \"iteration\": 1848, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 271.39599609375, \"iteration\": 1849, \"epoch\": 9}, {\"training_acc\": 0.78125, \"training_loss\": 243.64857482910156, \"iteration\": 1850, \"epoch\": 9}, {\"training_acc\": 0.7421875, \"training_loss\": 219.6994171142578, \"iteration\": 1851, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 257.4883117675781, \"iteration\": 1852, \"epoch\": 9}, {\"training_acc\": 0.7421875, \"training_loss\": 245.591796875, \"iteration\": 1853, \"epoch\": 9}, {\"training_acc\": 0.7734375, \"training_loss\": 253.49868774414062, \"iteration\": 1854, \"epoch\": 9}, {\"training_acc\": 0.8203125, \"training_loss\": 234.35830688476562, \"iteration\": 1855, \"epoch\": 9}, {\"training_acc\": 0.7265625, \"training_loss\": 248.94985961914062, \"iteration\": 1856, \"epoch\": 9}, {\"training_acc\": 0.7421875, \"training_loss\": 276.032958984375, \"iteration\": 1857, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 253.89984130859375, \"iteration\": 1858, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 255.9700927734375, \"iteration\": 1859, \"epoch\": 9}, {\"training_acc\": 0.875, \"training_loss\": 275.4086608886719, \"iteration\": 1860, \"epoch\": 9}, {\"training_acc\": 0.890625, \"training_loss\": 311.12799072265625, \"iteration\": 1861, \"epoch\": 9}, {\"training_acc\": 0.8984375, \"training_loss\": 250.70948791503906, \"iteration\": 1862, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 284.4635009765625, \"iteration\": 1863, \"epoch\": 9}, {\"training_acc\": 0.9296875, \"training_loss\": 299.8982238769531, \"iteration\": 1864, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 282.4880676269531, \"iteration\": 1865, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 199.542236328125, \"iteration\": 1866, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 200.775634765625, \"iteration\": 1867, \"epoch\": 9}, {\"training_acc\": 0.8203125, \"training_loss\": 269.73388671875, \"iteration\": 1868, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 206.25250244140625, \"iteration\": 1869, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 279.9478454589844, \"iteration\": 1870, \"epoch\": 9}, {\"training_acc\": 0.9375, \"training_loss\": 306.17657470703125, \"iteration\": 1871, \"epoch\": 9}, {\"training_acc\": 0.921875, \"training_loss\": 273.01898193359375, \"iteration\": 1872, \"epoch\": 9}, {\"training_acc\": 0.8828125, \"training_loss\": 248.74705505371094, \"iteration\": 1873, \"epoch\": 9}, {\"training_acc\": 0.890625, \"training_loss\": 249.0192108154297, \"iteration\": 1874, \"epoch\": 9}, {\"training_acc\": 0.90625, \"training_loss\": 220.84320068359375, \"iteration\": 1875, \"epoch\": 9}, {\"training_acc\": 0.90625, \"training_loss\": 296.34967041015625, \"iteration\": 1876, \"epoch\": 9}, {\"training_acc\": 0.890625, \"training_loss\": 280.15106201171875, \"iteration\": 1877, \"epoch\": 9}, {\"training_acc\": 0.9453125, \"training_loss\": 289.3782958984375, \"iteration\": 1878, \"epoch\": 9}, {\"training_acc\": 0.8984375, \"training_loss\": 294.19549560546875, \"iteration\": 1879, \"epoch\": 9}, {\"training_acc\": 0.8671875, \"training_loss\": 249.05465698242188, \"iteration\": 1880, \"epoch\": 9}, {\"training_acc\": 0.890625, \"training_loss\": 294.81597900390625, \"iteration\": 1881, \"epoch\": 9}, {\"training_acc\": 0.921875, \"training_loss\": 259.3822021484375, \"iteration\": 1882, \"epoch\": 9}, {\"training_acc\": 0.8984375, \"training_loss\": 244.46749877929688, \"iteration\": 1883, \"epoch\": 9}, {\"training_acc\": 0.8671875, \"training_loss\": 221.6251678466797, \"iteration\": 1884, \"epoch\": 9}, {\"training_acc\": 0.921875, \"training_loss\": 293.939208984375, \"iteration\": 1885, \"epoch\": 9}, {\"training_acc\": 0.875, \"training_loss\": 258.98712158203125, \"iteration\": 1886, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 288.1704406738281, \"iteration\": 1887, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 279.4428405761719, \"iteration\": 1888, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 286.37091064453125, \"iteration\": 1889, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 290.0440368652344, \"iteration\": 1890, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 275.1404724121094, \"iteration\": 1891, \"epoch\": 9}, {\"training_acc\": 0.765625, \"training_loss\": 273.262939453125, \"iteration\": 1892, \"epoch\": 9}, {\"training_acc\": 0.78125, \"training_loss\": 263.9707946777344, \"iteration\": 1893, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 282.7442626953125, \"iteration\": 1894, \"epoch\": 9}, {\"training_acc\": 0.890625, \"training_loss\": 306.7193603515625, \"iteration\": 1895, \"epoch\": 9}, {\"training_acc\": 0.875, \"training_loss\": 265.2774658203125, \"iteration\": 1896, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 242.21849060058594, \"iteration\": 1897, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 317.89093017578125, \"iteration\": 1898, \"epoch\": 9}, {\"training_acc\": 0.90625, \"training_loss\": 303.23785400390625, \"iteration\": 1899, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 274.84979248046875, \"iteration\": 1900, \"epoch\": 9}, {\"training_acc\": 0.90625, \"training_loss\": 316.207763671875, \"iteration\": 1901, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 240.12493896484375, \"iteration\": 1902, \"epoch\": 9}, {\"training_acc\": 0.8828125, \"training_loss\": 293.97760009765625, \"iteration\": 1903, \"epoch\": 9}, {\"training_acc\": 0.71875, \"training_loss\": 207.1297607421875, \"iteration\": 1904, \"epoch\": 9}, {\"training_acc\": 0.765625, \"training_loss\": 269.5186767578125, \"iteration\": 1905, \"epoch\": 9}, {\"training_acc\": 0.7421875, \"training_loss\": 236.6020050048828, \"iteration\": 1906, \"epoch\": 9}, {\"training_acc\": 0.7890625, \"training_loss\": 304.5820617675781, \"iteration\": 1907, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 288.5167541503906, \"iteration\": 1908, \"epoch\": 9}, {\"training_acc\": 0.7890625, \"training_loss\": 263.9176025390625, \"iteration\": 1909, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 267.215087890625, \"iteration\": 1910, \"epoch\": 9}, {\"training_acc\": 0.75, \"training_loss\": 259.5575256347656, \"iteration\": 1911, \"epoch\": 9}, {\"training_acc\": 0.7578125, \"training_loss\": 221.3498077392578, \"iteration\": 1912, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 308.5297546386719, \"iteration\": 1913, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 234.8733367919922, \"iteration\": 1914, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 265.3468017578125, \"iteration\": 1915, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 248.99508666992188, \"iteration\": 1916, \"epoch\": 9}, {\"training_acc\": 0.890625, \"training_loss\": 264.46600341796875, \"iteration\": 1917, \"epoch\": 9}, {\"training_acc\": 0.78125, \"training_loss\": 244.25125122070312, \"iteration\": 1918, \"epoch\": 9}, {\"training_acc\": 0.7734375, \"training_loss\": 245.13070678710938, \"iteration\": 1919, \"epoch\": 9}, {\"training_acc\": 0.75, \"training_loss\": 223.1745147705078, \"iteration\": 1920, \"epoch\": 9}, {\"training_acc\": 0.7421875, \"training_loss\": 288.91815185546875, \"iteration\": 1921, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 248.4925079345703, \"iteration\": 1922, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 219.19244384765625, \"iteration\": 1923, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 255.83792114257812, \"iteration\": 1924, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 239.58001708984375, \"iteration\": 1925, \"epoch\": 9}, {\"training_acc\": 0.796875, \"training_loss\": 273.49371337890625, \"iteration\": 1926, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 284.3691101074219, \"iteration\": 1927, \"epoch\": 9}, {\"training_acc\": 0.734375, \"training_loss\": 223.823486328125, \"iteration\": 1928, \"epoch\": 9}, {\"training_acc\": 0.8671875, \"training_loss\": 297.84271240234375, \"iteration\": 1929, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 249.39013671875, \"iteration\": 1930, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 287.8509521484375, \"iteration\": 1931, \"epoch\": 9}, {\"training_acc\": 0.921875, \"training_loss\": 233.55677795410156, \"iteration\": 1932, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 213.31890869140625, \"iteration\": 1933, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 237.81051635742188, \"iteration\": 1934, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 316.59527587890625, \"iteration\": 1935, \"epoch\": 9}, {\"training_acc\": 0.75, \"training_loss\": 243.24417114257812, \"iteration\": 1936, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 227.67398071289062, \"iteration\": 1937, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 219.24871826171875, \"iteration\": 1938, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 262.0516357421875, \"iteration\": 1939, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 295.76373291015625, \"iteration\": 1940, \"epoch\": 9}, {\"training_acc\": 0.8515625, \"training_loss\": 307.07177734375, \"iteration\": 1941, \"epoch\": 9}, {\"training_acc\": 0.8828125, \"training_loss\": 300.3727111816406, \"iteration\": 1942, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 277.31658935546875, \"iteration\": 1943, \"epoch\": 9}, {\"training_acc\": 0.7890625, \"training_loss\": 276.4640197753906, \"iteration\": 1944, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 228.5650177001953, \"iteration\": 1945, \"epoch\": 9}, {\"training_acc\": 0.7421875, \"training_loss\": 231.27740478515625, \"iteration\": 1946, \"epoch\": 9}, {\"training_acc\": 0.7890625, \"training_loss\": 226.94859313964844, \"iteration\": 1947, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 258.60870361328125, \"iteration\": 1948, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 292.7352294921875, \"iteration\": 1949, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 289.32135009765625, \"iteration\": 1950, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 244.15109252929688, \"iteration\": 1951, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 277.96246337890625, \"iteration\": 1952, \"epoch\": 9}, {\"training_acc\": 0.8359375, \"training_loss\": 294.298583984375, \"iteration\": 1953, \"epoch\": 9}, {\"training_acc\": 0.8828125, \"training_loss\": 276.35650634765625, \"iteration\": 1954, \"epoch\": 9}, {\"training_acc\": 0.8828125, \"training_loss\": 297.6015625, \"iteration\": 1955, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 301.01959228515625, \"iteration\": 1956, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 238.64208984375, \"iteration\": 1957, \"epoch\": 9}, {\"training_acc\": 0.859375, \"training_loss\": 260.438720703125, \"iteration\": 1958, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 277.04888916015625, \"iteration\": 1959, \"epoch\": 9}, {\"training_acc\": 0.796875, \"training_loss\": 283.96514892578125, \"iteration\": 1960, \"epoch\": 9}, {\"training_acc\": 0.78125, \"training_loss\": 239.5015106201172, \"iteration\": 1961, \"epoch\": 9}, {\"training_acc\": 0.8203125, \"training_loss\": 240.26580810546875, \"iteration\": 1962, \"epoch\": 9}, {\"training_acc\": 0.78125, \"training_loss\": 295.6727294921875, \"iteration\": 1963, \"epoch\": 9}, {\"training_acc\": 0.7890625, \"training_loss\": 287.940673828125, \"iteration\": 1964, \"epoch\": 9}, {\"training_acc\": 0.796875, \"training_loss\": 297.18505859375, \"iteration\": 1965, \"epoch\": 9}, {\"training_acc\": 0.7421875, \"training_loss\": 259.8951416015625, \"iteration\": 1966, \"epoch\": 9}, {\"training_acc\": 0.765625, \"training_loss\": 242.40737915039062, \"iteration\": 1967, \"epoch\": 9}, {\"training_acc\": 0.7109375, \"training_loss\": 248.1906280517578, \"iteration\": 1968, \"epoch\": 9}, {\"training_acc\": 0.796875, \"training_loss\": 307.4356689453125, \"iteration\": 1969, \"epoch\": 9}, {\"training_acc\": 0.8203125, \"training_loss\": 289.485595703125, \"iteration\": 1970, \"epoch\": 9}, {\"training_acc\": 0.765625, \"training_loss\": 228.64923095703125, \"iteration\": 1971, \"epoch\": 9}, {\"training_acc\": 0.78125, \"training_loss\": 286.1999816894531, \"iteration\": 1972, \"epoch\": 9}, {\"training_acc\": 0.65625, \"training_loss\": 218.01089477539062, \"iteration\": 1973, \"epoch\": 9}, {\"training_acc\": 0.7265625, \"training_loss\": 263.12347412109375, \"iteration\": 1974, \"epoch\": 9}, {\"training_acc\": 0.7734375, \"training_loss\": 245.6947479248047, \"iteration\": 1975, \"epoch\": 9}, {\"training_acc\": 0.7734375, \"training_loss\": 259.3876647949219, \"iteration\": 1976, \"epoch\": 9}, {\"training_acc\": 0.75, \"training_loss\": 248.9047088623047, \"iteration\": 1977, \"epoch\": 9}, {\"training_acc\": 0.7578125, \"training_loss\": 256.77667236328125, \"iteration\": 1978, \"epoch\": 9}, {\"training_acc\": 0.7578125, \"training_loss\": 261.63446044921875, \"iteration\": 1979, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 311.78302001953125, \"iteration\": 1980, \"epoch\": 9}, {\"training_acc\": 0.8671875, \"training_loss\": 290.5166931152344, \"iteration\": 1981, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 262.2076721191406, \"iteration\": 1982, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 247.62222290039062, \"iteration\": 1983, \"epoch\": 9}, {\"training_acc\": 0.8984375, \"training_loss\": 288.25616455078125, \"iteration\": 1984, \"epoch\": 9}, {\"training_acc\": 0.8046875, \"training_loss\": 292.34417724609375, \"iteration\": 1985, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 276.54034423828125, \"iteration\": 1986, \"epoch\": 9}, {\"training_acc\": 0.828125, \"training_loss\": 285.33184814453125, \"iteration\": 1987, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 279.2357177734375, \"iteration\": 1988, \"epoch\": 9}, {\"training_acc\": 0.7421875, \"training_loss\": 268.40777587890625, \"iteration\": 1989, \"epoch\": 9}, {\"training_acc\": 0.84375, \"training_loss\": 264.9288635253906, \"iteration\": 1990, \"epoch\": 9}, {\"training_acc\": 0.8125, \"training_loss\": 271.2813415527344, \"iteration\": 1991, \"epoch\": 9}, {\"training_acc\": 0.796875, \"training_loss\": 314.26043701171875, \"iteration\": 1992, \"epoch\": 9}, {\"training_acc\": 0.7421875, \"training_loss\": 237.31137084960938, \"iteration\": 1993, \"epoch\": 9}, {\"training_acc\": 0.75, \"training_loss\": 262.485107421875, \"iteration\": 1994, \"epoch\": 9}, {\"training_acc\": 0.796875, \"training_loss\": 261.641845703125, \"iteration\": 1995, \"epoch\": 9}, {\"training_acc\": 0.8203125, \"training_loss\": 303.635009765625, \"iteration\": 1996, \"epoch\": 9}, {\"training_acc\": 0.90625, \"training_loss\": 283.20452880859375, \"iteration\": 1997, \"epoch\": 9}, {\"training_acc\": 0.7755102040816326, \"training_loss\": 87.51658630371094, \"iteration\": 1998, \"epoch\": 9}, {\"training_acc\": 0.890625, \"training_loss\": 299.40155029296875, \"iteration\": 1999, \"epoch\": 10}, {\"training_acc\": 0.8984375, \"training_loss\": 265.11614990234375, \"iteration\": 2000, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 264.1672058105469, \"iteration\": 2001, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 258.84228515625, \"iteration\": 2002, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 253.47059631347656, \"iteration\": 2003, \"epoch\": 10}, {\"training_acc\": 0.8984375, \"training_loss\": 258.3819580078125, \"iteration\": 2004, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 281.0070495605469, \"iteration\": 2005, \"epoch\": 10}, {\"training_acc\": 0.890625, \"training_loss\": 294.2764892578125, \"iteration\": 2006, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 237.6970977783203, \"iteration\": 2007, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 240.13308715820312, \"iteration\": 2008, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 260.11871337890625, \"iteration\": 2009, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 228.92578125, \"iteration\": 2010, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 270.5181884765625, \"iteration\": 2011, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 248.2710723876953, \"iteration\": 2012, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 282.9895324707031, \"iteration\": 2013, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 246.47894287109375, \"iteration\": 2014, \"epoch\": 10}, {\"training_acc\": 0.8125, \"training_loss\": 245.44586181640625, \"iteration\": 2015, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 244.46092224121094, \"iteration\": 2016, \"epoch\": 10}, {\"training_acc\": 0.78125, \"training_loss\": 285.32354736328125, \"iteration\": 2017, \"epoch\": 10}, {\"training_acc\": 0.8125, \"training_loss\": 297.41510009765625, \"iteration\": 2018, \"epoch\": 10}, {\"training_acc\": 0.7890625, \"training_loss\": 263.8204650878906, \"iteration\": 2019, \"epoch\": 10}, {\"training_acc\": 0.78125, \"training_loss\": 246.56219482421875, \"iteration\": 2020, \"epoch\": 10}, {\"training_acc\": 0.7421875, \"training_loss\": 258.8398742675781, \"iteration\": 2021, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 284.92041015625, \"iteration\": 2022, \"epoch\": 10}, {\"training_acc\": 0.8125, \"training_loss\": 291.78863525390625, \"iteration\": 2023, \"epoch\": 10}, {\"training_acc\": 0.765625, \"training_loss\": 276.747802734375, \"iteration\": 2024, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 290.85003662109375, \"iteration\": 2025, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 284.1141052246094, \"iteration\": 2026, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 265.996337890625, \"iteration\": 2027, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 281.6860656738281, \"iteration\": 2028, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 274.50689697265625, \"iteration\": 2029, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 254.57952880859375, \"iteration\": 2030, \"epoch\": 10}, {\"training_acc\": 0.8046875, \"training_loss\": 233.5022735595703, \"iteration\": 2031, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 279.338134765625, \"iteration\": 2032, \"epoch\": 10}, {\"training_acc\": 0.8125, \"training_loss\": 224.5674285888672, \"iteration\": 2033, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 315.10418701171875, \"iteration\": 2034, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 248.88088989257812, \"iteration\": 2035, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 275.16534423828125, \"iteration\": 2036, \"epoch\": 10}, {\"training_acc\": 0.78125, \"training_loss\": 291.62548828125, \"iteration\": 2037, \"epoch\": 10}, {\"training_acc\": 0.7734375, \"training_loss\": 229.17535400390625, \"iteration\": 2038, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 309.5750427246094, \"iteration\": 2039, \"epoch\": 10}, {\"training_acc\": 0.7265625, \"training_loss\": 275.91705322265625, \"iteration\": 2040, \"epoch\": 10}, {\"training_acc\": 0.78125, \"training_loss\": 264.47607421875, \"iteration\": 2041, \"epoch\": 10}, {\"training_acc\": 0.765625, \"training_loss\": 238.77984619140625, \"iteration\": 2042, \"epoch\": 10}, {\"training_acc\": 0.8984375, \"training_loss\": 284.191162109375, \"iteration\": 2043, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 331.8004150390625, \"iteration\": 2044, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 232.27069091796875, \"iteration\": 2045, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 227.51467895507812, \"iteration\": 2046, \"epoch\": 10}, {\"training_acc\": 0.890625, \"training_loss\": 257.71002197265625, \"iteration\": 2047, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 252.88417053222656, \"iteration\": 2048, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 209.75204467773438, \"iteration\": 2049, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 264.2847595214844, \"iteration\": 2050, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 213.89199829101562, \"iteration\": 2051, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 243.0721893310547, \"iteration\": 2052, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 242.79978942871094, \"iteration\": 2053, \"epoch\": 10}, {\"training_acc\": 0.8125, \"training_loss\": 262.947265625, \"iteration\": 2054, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 236.64036560058594, \"iteration\": 2055, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 253.3619384765625, \"iteration\": 2056, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 233.4583740234375, \"iteration\": 2057, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 256.5952453613281, \"iteration\": 2058, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 304.82781982421875, \"iteration\": 2059, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 278.5924072265625, \"iteration\": 2060, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 224.96041870117188, \"iteration\": 2061, \"epoch\": 10}, {\"training_acc\": 0.7734375, \"training_loss\": 274.75384521484375, \"iteration\": 2062, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 275.59393310546875, \"iteration\": 2063, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 294.11114501953125, \"iteration\": 2064, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 213.56381225585938, \"iteration\": 2065, \"epoch\": 10}, {\"training_acc\": 0.8984375, \"training_loss\": 243.313720703125, \"iteration\": 2066, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 228.76405334472656, \"iteration\": 2067, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 279.87738037109375, \"iteration\": 2068, \"epoch\": 10}, {\"training_acc\": 0.9375, \"training_loss\": 310.7642822265625, \"iteration\": 2069, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 261.20684814453125, \"iteration\": 2070, \"epoch\": 10}, {\"training_acc\": 0.8984375, \"training_loss\": 297.5580749511719, \"iteration\": 2071, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 288.0828857421875, \"iteration\": 2072, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 231.79531860351562, \"iteration\": 2073, \"epoch\": 10}, {\"training_acc\": 0.890625, \"training_loss\": 293.3559265136719, \"iteration\": 2074, \"epoch\": 10}, {\"training_acc\": 0.90625, \"training_loss\": 290.48046875, \"iteration\": 2075, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 229.78854370117188, \"iteration\": 2076, \"epoch\": 10}, {\"training_acc\": 0.8125, \"training_loss\": 267.5929260253906, \"iteration\": 2077, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 300.464111328125, \"iteration\": 2078, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 257.5927429199219, \"iteration\": 2079, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 239.22726440429688, \"iteration\": 2080, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 244.99266052246094, \"iteration\": 2081, \"epoch\": 10}, {\"training_acc\": 0.890625, \"training_loss\": 264.965576171875, \"iteration\": 2082, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 241.38629150390625, \"iteration\": 2083, \"epoch\": 10}, {\"training_acc\": 0.78125, \"training_loss\": 287.9559631347656, \"iteration\": 2084, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 188.3474578857422, \"iteration\": 2085, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 288.0008544921875, \"iteration\": 2086, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 248.50778198242188, \"iteration\": 2087, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 293.3980712890625, \"iteration\": 2088, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 242.34976196289062, \"iteration\": 2089, \"epoch\": 10}, {\"training_acc\": 0.7734375, \"training_loss\": 291.5917053222656, \"iteration\": 2090, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 280.60162353515625, \"iteration\": 2091, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 271.85528564453125, \"iteration\": 2092, \"epoch\": 10}, {\"training_acc\": 0.890625, \"training_loss\": 258.6406555175781, \"iteration\": 2093, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 223.46719360351562, \"iteration\": 2094, \"epoch\": 10}, {\"training_acc\": 0.9296875, \"training_loss\": 267.7122497558594, \"iteration\": 2095, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 253.6423797607422, \"iteration\": 2096, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 309.4486389160156, \"iteration\": 2097, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 261.70330810546875, \"iteration\": 2098, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 284.8260192871094, \"iteration\": 2099, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 292.89666748046875, \"iteration\": 2100, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 285.8564453125, \"iteration\": 2101, \"epoch\": 10}, {\"training_acc\": 0.9140625, \"training_loss\": 279.9079895019531, \"iteration\": 2102, \"epoch\": 10}, {\"training_acc\": 0.90625, \"training_loss\": 257.8082275390625, \"iteration\": 2103, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 241.3363494873047, \"iteration\": 2104, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 237.45822143554688, \"iteration\": 2105, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 265.4695739746094, \"iteration\": 2106, \"epoch\": 10}, {\"training_acc\": 0.765625, \"training_loss\": 235.42547607421875, \"iteration\": 2107, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 291.8302001953125, \"iteration\": 2108, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 286.2415771484375, \"iteration\": 2109, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 304.85198974609375, \"iteration\": 2110, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 246.21392822265625, \"iteration\": 2111, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 282.9866638183594, \"iteration\": 2112, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 286.1501770019531, \"iteration\": 2113, \"epoch\": 10}, {\"training_acc\": 0.921875, \"training_loss\": 283.53253173828125, \"iteration\": 2114, \"epoch\": 10}, {\"training_acc\": 0.890625, \"training_loss\": 308.7979736328125, \"iteration\": 2115, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 318.013671875, \"iteration\": 2116, \"epoch\": 10}, {\"training_acc\": 0.796875, \"training_loss\": 246.0157012939453, \"iteration\": 2117, \"epoch\": 10}, {\"training_acc\": 0.890625, \"training_loss\": 306.1788330078125, \"iteration\": 2118, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 278.4807434082031, \"iteration\": 2119, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 255.39260864257812, \"iteration\": 2120, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 244.73623657226562, \"iteration\": 2121, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 228.1370849609375, \"iteration\": 2122, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 311.85211181640625, \"iteration\": 2123, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 233.30355834960938, \"iteration\": 2124, \"epoch\": 10}, {\"training_acc\": 0.8125, \"training_loss\": 269.2231140136719, \"iteration\": 2125, \"epoch\": 10}, {\"training_acc\": 0.765625, \"training_loss\": 227.65716552734375, \"iteration\": 2126, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 303.12286376953125, \"iteration\": 2127, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 255.77386474609375, \"iteration\": 2128, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 314.38134765625, \"iteration\": 2129, \"epoch\": 10}, {\"training_acc\": 0.8046875, \"training_loss\": 292.01739501953125, \"iteration\": 2130, \"epoch\": 10}, {\"training_acc\": 0.7890625, \"training_loss\": 278.23870849609375, \"iteration\": 2131, \"epoch\": 10}, {\"training_acc\": 0.765625, \"training_loss\": 213.08885192871094, \"iteration\": 2132, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 219.75457763671875, \"iteration\": 2133, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 260.02850341796875, \"iteration\": 2134, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 251.73098754882812, \"iteration\": 2135, \"epoch\": 10}, {\"training_acc\": 0.8984375, \"training_loss\": 276.97216796875, \"iteration\": 2136, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 279.696533203125, \"iteration\": 2137, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 279.82928466796875, \"iteration\": 2138, \"epoch\": 10}, {\"training_acc\": 0.8984375, \"training_loss\": 281.44537353515625, \"iteration\": 2139, \"epoch\": 10}, {\"training_acc\": 0.7890625, \"training_loss\": 255.49560546875, \"iteration\": 2140, \"epoch\": 10}, {\"training_acc\": 0.8046875, \"training_loss\": 239.99143981933594, \"iteration\": 2141, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 264.2158203125, \"iteration\": 2142, \"epoch\": 10}, {\"training_acc\": 0.796875, \"training_loss\": 295.88568115234375, \"iteration\": 2143, \"epoch\": 10}, {\"training_acc\": 0.8125, \"training_loss\": 261.84661865234375, \"iteration\": 2144, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 350.51446533203125, \"iteration\": 2145, \"epoch\": 10}, {\"training_acc\": 0.78125, \"training_loss\": 233.11143493652344, \"iteration\": 2146, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 257.9696350097656, \"iteration\": 2147, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 271.202392578125, \"iteration\": 2148, \"epoch\": 10}, {\"training_acc\": 0.90625, \"training_loss\": 306.4974060058594, \"iteration\": 2149, \"epoch\": 10}, {\"training_acc\": 0.8984375, \"training_loss\": 277.4854431152344, \"iteration\": 2150, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 242.98748779296875, \"iteration\": 2151, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 245.84231567382812, \"iteration\": 2152, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 284.9248962402344, \"iteration\": 2153, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 243.05172729492188, \"iteration\": 2154, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 262.9698181152344, \"iteration\": 2155, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 254.10401916503906, \"iteration\": 2156, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 290.5366516113281, \"iteration\": 2157, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 303.2002258300781, \"iteration\": 2158, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 287.0205993652344, \"iteration\": 2159, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 287.62286376953125, \"iteration\": 2160, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 257.62744140625, \"iteration\": 2161, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 266.46246337890625, \"iteration\": 2162, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 228.51998901367188, \"iteration\": 2163, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 225.1306915283203, \"iteration\": 2164, \"epoch\": 10}, {\"training_acc\": 0.8828125, \"training_loss\": 238.33364868164062, \"iteration\": 2165, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 250.70388793945312, \"iteration\": 2166, \"epoch\": 10}, {\"training_acc\": 0.7890625, \"training_loss\": 278.5614318847656, \"iteration\": 2167, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 253.39356994628906, \"iteration\": 2168, \"epoch\": 10}, {\"training_acc\": 0.7890625, \"training_loss\": 280.0556640625, \"iteration\": 2169, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 266.3485107421875, \"iteration\": 2170, \"epoch\": 10}, {\"training_acc\": 0.765625, \"training_loss\": 253.76632690429688, \"iteration\": 2171, \"epoch\": 10}, {\"training_acc\": 0.71875, \"training_loss\": 245.33160400390625, \"iteration\": 2172, \"epoch\": 10}, {\"training_acc\": 0.7890625, \"training_loss\": 233.58615112304688, \"iteration\": 2173, \"epoch\": 10}, {\"training_acc\": 0.7890625, \"training_loss\": 243.3700408935547, \"iteration\": 2174, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 259.4566955566406, \"iteration\": 2175, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 273.96759033203125, \"iteration\": 2176, \"epoch\": 10}, {\"training_acc\": 0.859375, \"training_loss\": 286.1548156738281, \"iteration\": 2177, \"epoch\": 10}, {\"training_acc\": 0.8046875, \"training_loss\": 250.45220947265625, \"iteration\": 2178, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 279.77154541015625, \"iteration\": 2179, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 315.9609375, \"iteration\": 2180, \"epoch\": 10}, {\"training_acc\": 0.8046875, \"training_loss\": 229.48153686523438, \"iteration\": 2181, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 240.9326171875, \"iteration\": 2182, \"epoch\": 10}, {\"training_acc\": 0.90625, \"training_loss\": 283.91864013671875, \"iteration\": 2183, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 256.2410583496094, \"iteration\": 2184, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 268.7073974609375, \"iteration\": 2185, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 285.6634521484375, \"iteration\": 2186, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 234.93035888671875, \"iteration\": 2187, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 279.3289794921875, \"iteration\": 2188, \"epoch\": 10}, {\"training_acc\": 0.7578125, \"training_loss\": 255.3058319091797, \"iteration\": 2189, \"epoch\": 10}, {\"training_acc\": 0.796875, \"training_loss\": 314.5249938964844, \"iteration\": 2190, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 296.9662170410156, \"iteration\": 2191, \"epoch\": 10}, {\"training_acc\": 0.8125, \"training_loss\": 312.6988830566406, \"iteration\": 2192, \"epoch\": 10}, {\"training_acc\": 0.78125, \"training_loss\": 213.39273071289062, \"iteration\": 2193, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 296.9521789550781, \"iteration\": 2194, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 274.47509765625, \"iteration\": 2195, \"epoch\": 10}, {\"training_acc\": 0.921875, \"training_loss\": 284.2154846191406, \"iteration\": 2196, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 257.2097473144531, \"iteration\": 2197, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 296.4877014160156, \"iteration\": 2198, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 248.5364990234375, \"iteration\": 2199, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 254.12924194335938, \"iteration\": 2200, \"epoch\": 10}, {\"training_acc\": 0.8515625, \"training_loss\": 217.76919555664062, \"iteration\": 2201, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 304.8817443847656, \"iteration\": 2202, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 279.0321044921875, \"iteration\": 2203, \"epoch\": 10}, {\"training_acc\": 0.8125, \"training_loss\": 241.12515258789062, \"iteration\": 2204, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 242.18377685546875, \"iteration\": 2205, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 270.1929931640625, \"iteration\": 2206, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 243.21859741210938, \"iteration\": 2207, \"epoch\": 10}, {\"training_acc\": 0.75, \"training_loss\": 220.387939453125, \"iteration\": 2208, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 272.44671630859375, \"iteration\": 2209, \"epoch\": 10}, {\"training_acc\": 0.8046875, \"training_loss\": 244.13912963867188, \"iteration\": 2210, \"epoch\": 10}, {\"training_acc\": 0.828125, \"training_loss\": 245.34762573242188, \"iteration\": 2211, \"epoch\": 10}, {\"training_acc\": 0.84375, \"training_loss\": 270.21722412109375, \"iteration\": 2212, \"epoch\": 10}, {\"training_acc\": 0.8359375, \"training_loss\": 280.1803283691406, \"iteration\": 2213, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 262.0640869140625, \"iteration\": 2214, \"epoch\": 10}, {\"training_acc\": 0.8203125, \"training_loss\": 229.15206909179688, \"iteration\": 2215, \"epoch\": 10}, {\"training_acc\": 0.8671875, \"training_loss\": 263.85986328125, \"iteration\": 2216, \"epoch\": 10}, {\"training_acc\": 0.875, \"training_loss\": 245.3511962890625, \"iteration\": 2217, \"epoch\": 10}, {\"training_acc\": 0.7890625, \"training_loss\": 235.3662109375, \"iteration\": 2218, \"epoch\": 10}, {\"training_acc\": 0.8125, \"training_loss\": 304.73272705078125, \"iteration\": 2219, \"epoch\": 10}, {\"training_acc\": 0.7959183673469388, \"training_loss\": 74.42796325683594, \"iteration\": 2220, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.HConcatChart(...)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_results(model_gru, gru_train_config, train_dataloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

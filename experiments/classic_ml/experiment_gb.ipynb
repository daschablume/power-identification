{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Classical models vs. Neural Networks on GB dataset\n",
    "\n",
    "We will compare the following models:\n",
    "- Neural Network, TF-IDF features, character token\n",
    "- Neural Network, TF-IDF features, word token\n",
    "- RNN, character features\n",
    "- RNN, word features\n",
    "- Linear SVC word, chars\n",
    "- Logistic regression words, chars\n",
    "- SGD classifier words, chars\n",
    "- Naive Bayes words, chars\n",
    "- XGBoost Words, chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Use: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import sys\n",
    "from pathlib import Path\n",
    "PARENT_DIR = Path.cwd().parent.parent.resolve()\n",
    "sys.path.append(str(PARENT_DIR))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from lib.data_processing import load_data, split_data, encode_data, PositionalEncoder\n",
    "from lib.models import NeuralNetwork, RNNClassifier, TrainConfig, save_model, load_model\n",
    "from lib.evaluation import evaluate, plot_results\n",
    "from lib.utils import check_cuda_memory\n",
    "from lib.logger import CustomLogger\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up device\n",
    "\n",
    "PREFERRED_DEVICE = 'cuda'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = PREFERRED_DEVICE\n",
    "    print(f\"CUDA available. Use: {DEVICE}\")\n",
    "    check_cuda_memory()\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print(f\"CUDA not available. Use: {DEVICE}\")\n",
    "\n",
    "# Set up dirs\n",
    "\n",
    "project_dir = Path.cwd().parent.parent\n",
    "models_dir = project_dir / ('models/gb')\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set up logger\n",
    "logger = CustomLogger(\"experiment_gb\", log_to_local=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 17:41:04,079:experiment_gb:INFO:Data size: 33257, % positive class: 56.39%\n"
     ]
    }
   ],
   "source": [
    "data = load_data(file_path_list=[project_dir / \"data/train/power/power-gb-train.tsv\"],text_head=\"text_en\")\n",
    "train_raw, test_raw = split_data(data, test_size=0.2, random_state=0)\n",
    "logger.info(f\"Data size: {len(data)}, % positive class: {sum(data.labels) / len(data) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare feature encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 17:41:13,318:experiment_gb:INFO:Prepare words_encoder...\n",
      "2024-09-27 17:41:18,745:experiment_gb:INFO:Prepare chars_encoder...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, max_features=50000, ngram_range=(3, 5),\n",
       "                sublinear_tf=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, max_features=50000, ngram_range=(3, 5),\n",
       "                sublinear_tf=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(analyzer='char', max_features=50000, ngram_range=(3, 5),\n",
       "                sublinear_tf=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "logger.info(\"Prepare words_encoder...\")\n",
    "words_encoder = TfidfVectorizer(max_features=50000)\n",
    "words_encoder.fit(train_raw.texts)\n",
    "\n",
    "logger.info(\"Prepare chars_encoder...\")\n",
    "chars_encoder = TfidfVectorizer(max_features=50000, analyzer=\"char\", ngram_range=(3,5), use_idf=True, sublinear_tf=True)\n",
    "chars_encoder.fit(train_raw.texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 17:44:27,861:experiment_gb:INFO:Prepare data...\n",
      "2024-09-27 17:44:42,568:experiment_gb:INFO:Train model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7087, Precision: 0.7254, Recall: 0.7701, F1: 0.7471, AUC: 0.7760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-60eb86b5377045c99e672116f0b7e4f7.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-60eb86b5377045c99e672116f0b7e4f7.vega-embed details,\n",
       "  #altair-viz-60eb86b5377045c99e672116f0b7e4f7.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-60eb86b5377045c99e672116f0b7e4f7\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-60eb86b5377045c99e672116f0b7e4f7\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-60eb86b5377045c99e672116f0b7e4f7\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-3200e6cae73cee1b7b6d8f8650b68cf2\"}, \"title\": \"model_nn_words - Training accuracy & Loss\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-3200e6cae73cee1b7b6d8f8650b68cf2\": [{\"training_acc\": 0.6328125, \"training_loss\": 0.6837177872657776, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 0.6941125392913818, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6832200288772583, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 0.6940221786499023, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6849960088729858, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6825844049453735, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.679219126701355, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 0.693629801273346, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6873566508293152, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 0.6933703422546387, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6828505992889404, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 0.6946320533752441, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 0.6904982328414917, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6925457715988159, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 0.692119836807251, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6803070902824402, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6840153336524963, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6712160110473633, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.683799147605896, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6830097436904907, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6772255301475525, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6615056991577148, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.675602376461029, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6786443591117859, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.673795759677887, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 0.6892477869987488, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6754521131515503, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6748008728027344, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 0.6844318509101868, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6844777464866638, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6738923788070679, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6647902727127075, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6702595949172974, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6553690433502197, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6543138027191162, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6578404903411865, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6631667613983154, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6473959684371948, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6354112029075623, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6550325751304626, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6438722014427185, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6371282935142517, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6498883962631226, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 0.6697431206703186, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6519497632980347, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6309738755226135, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.6158410906791687, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.6302422881126404, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.6178123950958252, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.6304617524147034, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6207873821258545, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.6002974510192871, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5789430737495422, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5976274609565735, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.6031991243362427, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6248663067817688, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5960060358047485, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5669068098068237, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5712525844573975, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5683321356773376, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6137403249740601, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.588111162185669, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.6181448101997375, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5750197768211365, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5487207174301147, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5831888914108276, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5843273401260376, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5745066404342651, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.560132622718811, \"iteration\": 69, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5640495419502258, \"iteration\": 70, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.553490936756134, \"iteration\": 71, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5174843668937683, \"iteration\": 72, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5813342928886414, \"iteration\": 73, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5343183279037476, \"iteration\": 74, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5170222520828247, \"iteration\": 75, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5732565522193909, \"iteration\": 76, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5082637071609497, \"iteration\": 77, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.538163423538208, \"iteration\": 78, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.5123302936553955, \"iteration\": 79, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5382758378982544, \"iteration\": 80, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.527461588382721, \"iteration\": 81, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.543683648109436, \"iteration\": 82, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4916269779205322, \"iteration\": 83, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5654408931732178, \"iteration\": 84, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5129584670066833, \"iteration\": 85, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.47703951597213745, \"iteration\": 86, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5487573742866516, \"iteration\": 87, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5295668244361877, \"iteration\": 88, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.42814481258392334, \"iteration\": 89, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.46963244676589966, \"iteration\": 90, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6455795764923096, \"iteration\": 91, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5862707495689392, \"iteration\": 92, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5327363014221191, \"iteration\": 93, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5173377394676208, \"iteration\": 94, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5227452516555786, \"iteration\": 95, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.49581295251846313, \"iteration\": 96, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5142872929573059, \"iteration\": 97, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5073761940002441, \"iteration\": 98, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5135372281074524, \"iteration\": 99, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5134633779525757, \"iteration\": 100, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.4829801917076111, \"iteration\": 101, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5078957080841064, \"iteration\": 102, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4498980939388275, \"iteration\": 103, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.4850674569606781, \"iteration\": 104, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.45663389563560486, \"iteration\": 105, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5402488708496094, \"iteration\": 106, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5927883982658386, \"iteration\": 107, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5476185083389282, \"iteration\": 108, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.44161972403526306, \"iteration\": 109, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.4998038113117218, \"iteration\": 110, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.43876588344573975, \"iteration\": 111, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.44217735528945923, \"iteration\": 112, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5002232193946838, \"iteration\": 113, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.42144978046417236, \"iteration\": 114, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.48092758655548096, \"iteration\": 115, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5280771255493164, \"iteration\": 116, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5205879807472229, \"iteration\": 117, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5141466856002808, \"iteration\": 118, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4680599272251129, \"iteration\": 119, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5128746032714844, \"iteration\": 120, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4903273582458496, \"iteration\": 121, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.48111316561698914, \"iteration\": 122, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4843573272228241, \"iteration\": 123, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.4761829376220703, \"iteration\": 124, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5068134069442749, \"iteration\": 125, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5016288161277771, \"iteration\": 126, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4375976622104645, \"iteration\": 127, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.49211668968200684, \"iteration\": 128, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4721321165561676, \"iteration\": 129, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4366026222705841, \"iteration\": 130, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5368213057518005, \"iteration\": 131, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4358603060245514, \"iteration\": 132, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.45920848846435547, \"iteration\": 133, \"epoch\": 1}, {\"training_acc\": 0.890625, \"training_loss\": 0.3847525417804718, \"iteration\": 134, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.49130064249038696, \"iteration\": 135, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.46373021602630615, \"iteration\": 136, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4837767779827118, \"iteration\": 137, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4855393171310425, \"iteration\": 138, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.48432740569114685, \"iteration\": 139, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.43683430552482605, \"iteration\": 140, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5203647613525391, \"iteration\": 141, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.470231831073761, \"iteration\": 142, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4220162034034729, \"iteration\": 143, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.3901664614677429, \"iteration\": 144, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4915691912174225, \"iteration\": 145, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4568916857242584, \"iteration\": 146, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4822932183742523, \"iteration\": 147, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4210101068019867, \"iteration\": 148, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.43383026123046875, \"iteration\": 149, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.47024595737457275, \"iteration\": 150, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.44215455651283264, \"iteration\": 151, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4250192642211914, \"iteration\": 152, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.43169498443603516, \"iteration\": 153, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.516364574432373, \"iteration\": 154, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.389521062374115, \"iteration\": 155, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5048470497131348, \"iteration\": 156, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5548670291900635, \"iteration\": 157, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4550660252571106, \"iteration\": 158, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.3862086534500122, \"iteration\": 159, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.46354565024375916, \"iteration\": 160, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4525553584098816, \"iteration\": 161, \"epoch\": 1}, {\"training_acc\": 0.859375, \"training_loss\": 0.3877054452896118, \"iteration\": 162, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4503386616706848, \"iteration\": 163, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.47712016105651855, \"iteration\": 164, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5207210183143616, \"iteration\": 165, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5012044906616211, \"iteration\": 166, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.450480580329895, \"iteration\": 167, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4429945647716522, \"iteration\": 168, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.4109783172607422, \"iteration\": 169, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.44698020815849304, \"iteration\": 170, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.48082873225212097, \"iteration\": 171, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5218546986579895, \"iteration\": 172, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4109762907028198, \"iteration\": 173, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.427469938993454, \"iteration\": 174, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5496252775192261, \"iteration\": 175, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.49838247895240784, \"iteration\": 176, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4690607488155365, \"iteration\": 177, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4364318549633026, \"iteration\": 178, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5080068111419678, \"iteration\": 179, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5041998624801636, \"iteration\": 180, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.5826682448387146, \"iteration\": 181, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4841610789299011, \"iteration\": 182, \"epoch\": 1}, {\"training_acc\": 0.890625, \"training_loss\": 0.3877496123313904, \"iteration\": 183, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.5017797946929932, \"iteration\": 184, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.4830842912197113, \"iteration\": 185, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.47562167048454285, \"iteration\": 186, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.47059008479118347, \"iteration\": 187, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5407269597053528, \"iteration\": 188, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5246171355247498, \"iteration\": 189, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5008240342140198, \"iteration\": 190, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4357728362083435, \"iteration\": 191, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4415701627731323, \"iteration\": 192, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.44748613238334656, \"iteration\": 193, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4279925525188446, \"iteration\": 194, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4104899764060974, \"iteration\": 195, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.4086170196533203, \"iteration\": 196, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4354139566421509, \"iteration\": 197, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5548588633537292, \"iteration\": 198, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.473995178937912, \"iteration\": 199, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5453121066093445, \"iteration\": 200, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5370978116989136, \"iteration\": 201, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5007550120353699, \"iteration\": 202, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4195045530796051, \"iteration\": 203, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.47899606823921204, \"iteration\": 204, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4879703223705292, \"iteration\": 205, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5130599141120911, \"iteration\": 206, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4477192759513855, \"iteration\": 207, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5329177975654602, \"iteration\": 208, \"epoch\": 1}, {\"training_acc\": 1.0, \"training_loss\": 0.4003230929374695, \"iteration\": 209, \"epoch\": 1}, {\"training_acc\": 0.875, \"training_loss\": 0.35264095664024353, \"iteration\": 210, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.31898099184036255, \"iteration\": 211, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.31158217787742615, \"iteration\": 212, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.33966681361198425, \"iteration\": 213, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.29791396856307983, \"iteration\": 214, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.324432909488678, \"iteration\": 215, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3226499855518341, \"iteration\": 216, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.33183205127716064, \"iteration\": 217, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3083767592906952, \"iteration\": 218, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.26851969957351685, \"iteration\": 219, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.317584365606308, \"iteration\": 220, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.31198304891586304, \"iteration\": 221, \"epoch\": 2}, {\"training_acc\": 0.953125, \"training_loss\": 0.24423092603683472, \"iteration\": 222, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.31975236535072327, \"iteration\": 223, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.28262069821357727, \"iteration\": 224, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3264975845813751, \"iteration\": 225, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3122275471687317, \"iteration\": 226, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.26748502254486084, \"iteration\": 227, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.28894713521003723, \"iteration\": 228, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.27440088987350464, \"iteration\": 229, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2931704819202423, \"iteration\": 230, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2830160856246948, \"iteration\": 231, \"epoch\": 2}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19798073172569275, \"iteration\": 232, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3545287251472473, \"iteration\": 233, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2863632142543793, \"iteration\": 234, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.33227869868278503, \"iteration\": 235, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2493445724248886, \"iteration\": 236, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.30343058705329895, \"iteration\": 237, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2459433376789093, \"iteration\": 238, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30767112970352173, \"iteration\": 239, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3165646195411682, \"iteration\": 240, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.31471186876296997, \"iteration\": 241, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2697926461696625, \"iteration\": 242, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.34379151463508606, \"iteration\": 243, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3545733690261841, \"iteration\": 244, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1904817521572113, \"iteration\": 245, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3336600959300995, \"iteration\": 246, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.37889981269836426, \"iteration\": 247, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3108540177345276, \"iteration\": 248, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3035111427307129, \"iteration\": 249, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2607010006904602, \"iteration\": 250, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2796245813369751, \"iteration\": 251, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.29753971099853516, \"iteration\": 252, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.37231987714767456, \"iteration\": 253, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.29919713735580444, \"iteration\": 254, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23220181465148926, \"iteration\": 255, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3295566439628601, \"iteration\": 256, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2712971270084381, \"iteration\": 257, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2976831793785095, \"iteration\": 258, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3512376844882965, \"iteration\": 259, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.20317117869853973, \"iteration\": 260, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.2828091084957123, \"iteration\": 261, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3084161877632141, \"iteration\": 262, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.21937641501426697, \"iteration\": 263, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.23233720660209656, \"iteration\": 264, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27193325757980347, \"iteration\": 265, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.33645376563072205, \"iteration\": 266, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2462272346019745, \"iteration\": 267, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.25717851519584656, \"iteration\": 268, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.28357797861099243, \"iteration\": 269, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4076063930988312, \"iteration\": 270, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.34222841262817383, \"iteration\": 271, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.28830692172050476, \"iteration\": 272, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.26701200008392334, \"iteration\": 273, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2571329176425934, \"iteration\": 274, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.21930262446403503, \"iteration\": 275, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.25028032064437866, \"iteration\": 276, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.24989250302314758, \"iteration\": 277, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.26393431425094604, \"iteration\": 278, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.25177711248397827, \"iteration\": 279, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.24289101362228394, \"iteration\": 280, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.22289666533470154, \"iteration\": 281, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.31135421991348267, \"iteration\": 282, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3806150257587433, \"iteration\": 283, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22249585390090942, \"iteration\": 284, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.2982674241065979, \"iteration\": 285, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30040085315704346, \"iteration\": 286, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.1947660744190216, \"iteration\": 287, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.23722773790359497, \"iteration\": 288, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2684374451637268, \"iteration\": 289, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.33363065123558044, \"iteration\": 290, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.4130103588104248, \"iteration\": 291, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2990034222602844, \"iteration\": 292, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2762385606765747, \"iteration\": 293, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2762605845928192, \"iteration\": 294, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.29276373982429504, \"iteration\": 295, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2806764543056488, \"iteration\": 296, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.28937673568725586, \"iteration\": 297, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.33143386244773865, \"iteration\": 298, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.29353341460227966, \"iteration\": 299, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.25343650579452515, \"iteration\": 300, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.23698978126049042, \"iteration\": 301, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2918560802936554, \"iteration\": 302, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.36808449029922485, \"iteration\": 303, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2638811767101288, \"iteration\": 304, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23709216713905334, \"iteration\": 305, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.21530918776988983, \"iteration\": 306, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.27907490730285645, \"iteration\": 307, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3793140947818756, \"iteration\": 308, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.23431019484996796, \"iteration\": 309, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2982694208621979, \"iteration\": 310, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.296986848115921, \"iteration\": 311, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.32270997762680054, \"iteration\": 312, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3638134300708771, \"iteration\": 313, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2834876775741577, \"iteration\": 314, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4087572693824768, \"iteration\": 315, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27883899211883545, \"iteration\": 316, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2392112910747528, \"iteration\": 317, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2815454602241516, \"iteration\": 318, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3131043314933777, \"iteration\": 319, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3321506679058075, \"iteration\": 320, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3329821228981018, \"iteration\": 321, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3162854015827179, \"iteration\": 322, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.30199575424194336, \"iteration\": 323, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.32255083322525024, \"iteration\": 324, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2152605652809143, \"iteration\": 325, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.35671597719192505, \"iteration\": 326, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2395550161600113, \"iteration\": 327, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.33009517192840576, \"iteration\": 328, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.264279842376709, \"iteration\": 329, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3503367304801941, \"iteration\": 330, \"epoch\": 2}, {\"training_acc\": 0.953125, \"training_loss\": 0.21728292107582092, \"iteration\": 331, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.32735589146614075, \"iteration\": 332, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3340567946434021, \"iteration\": 333, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2680571973323822, \"iteration\": 334, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4477977454662323, \"iteration\": 335, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.3845396339893341, \"iteration\": 336, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.37612947821617126, \"iteration\": 337, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3313152492046356, \"iteration\": 338, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30621808767318726, \"iteration\": 339, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2450987994670868, \"iteration\": 340, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.27296552062034607, \"iteration\": 341, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3499252200126648, \"iteration\": 342, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.24508652091026306, \"iteration\": 343, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2720932960510254, \"iteration\": 344, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.29543042182922363, \"iteration\": 345, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3618333041667938, \"iteration\": 346, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3152535855770111, \"iteration\": 347, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25310418009757996, \"iteration\": 348, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.33755964040756226, \"iteration\": 349, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.35277771949768066, \"iteration\": 350, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3779536187648773, \"iteration\": 351, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2710360884666443, \"iteration\": 352, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2903421223163605, \"iteration\": 353, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.31086641550064087, \"iteration\": 354, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.27616024017333984, \"iteration\": 355, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2983055114746094, \"iteration\": 356, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2590451240539551, \"iteration\": 357, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.22458156943321228, \"iteration\": 358, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27567338943481445, \"iteration\": 359, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.27363669872283936, \"iteration\": 360, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.35040825605392456, \"iteration\": 361, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3524537980556488, \"iteration\": 362, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.38687196373939514, \"iteration\": 363, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.32489609718322754, \"iteration\": 364, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.35368281602859497, \"iteration\": 365, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3598940670490265, \"iteration\": 366, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.33870965242385864, \"iteration\": 367, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.31084540486335754, \"iteration\": 368, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4199971556663513, \"iteration\": 369, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.28993678092956543, \"iteration\": 370, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3586233854293823, \"iteration\": 371, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30844175815582275, \"iteration\": 372, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.36704716086387634, \"iteration\": 373, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.24514734745025635, \"iteration\": 374, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2993379235267639, \"iteration\": 375, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3200703561306, \"iteration\": 376, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2363322675228119, \"iteration\": 377, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3041963577270508, \"iteration\": 378, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2647181749343872, \"iteration\": 379, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.38052237033843994, \"iteration\": 380, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3239578902721405, \"iteration\": 381, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.46261242032051086, \"iteration\": 382, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.38247278332710266, \"iteration\": 383, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.29278868436813354, \"iteration\": 384, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3218074142932892, \"iteration\": 385, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.27273398637771606, \"iteration\": 386, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.37423157691955566, \"iteration\": 387, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30105656385421753, \"iteration\": 388, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.508435845375061, \"iteration\": 389, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22738492488861084, \"iteration\": 390, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2879733443260193, \"iteration\": 391, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3279412090778351, \"iteration\": 392, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3943359851837158, \"iteration\": 393, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.32003355026245117, \"iteration\": 394, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4361647367477417, \"iteration\": 395, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.41591474413871765, \"iteration\": 396, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2833245098590851, \"iteration\": 397, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.24855157732963562, \"iteration\": 398, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.26197418570518494, \"iteration\": 399, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23984217643737793, \"iteration\": 400, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3242170810699463, \"iteration\": 401, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.32037991285324097, \"iteration\": 402, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3615696132183075, \"iteration\": 403, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.34185561537742615, \"iteration\": 404, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2443426251411438, \"iteration\": 405, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.31734660267829895, \"iteration\": 406, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2963079810142517, \"iteration\": 407, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3590202331542969, \"iteration\": 408, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.37505802512168884, \"iteration\": 409, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3619092106819153, \"iteration\": 410, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30742985010147095, \"iteration\": 411, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.375039666891098, \"iteration\": 412, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3051089644432068, \"iteration\": 413, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.4392501711845398, \"iteration\": 414, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.37137195467948914, \"iteration\": 415, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.38253578543663025, \"iteration\": 416, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.49851226806640625, \"iteration\": 417, \"epoch\": 2}, {\"training_acc\": 1.0, \"training_loss\": 0.039161566644907, \"iteration\": 418, \"epoch\": 2}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17928892374038696, \"iteration\": 419, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.16578437387943268, \"iteration\": 420, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1585579812526703, \"iteration\": 421, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19084352254867554, \"iteration\": 422, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.16884607076644897, \"iteration\": 423, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20001283288002014, \"iteration\": 424, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16859492659568787, \"iteration\": 425, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1592000275850296, \"iteration\": 426, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.16512322425842285, \"iteration\": 427, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.1957417130470276, \"iteration\": 428, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17967011034488678, \"iteration\": 429, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1966215968132019, \"iteration\": 430, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.14164023101329803, \"iteration\": 431, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.16084080934524536, \"iteration\": 432, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.1984810084104538, \"iteration\": 433, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1543276607990265, \"iteration\": 434, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.15387390553951263, \"iteration\": 435, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.12317579239606857, \"iteration\": 436, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14025917649269104, \"iteration\": 437, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14490137994289398, \"iteration\": 438, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18655522167682648, \"iteration\": 439, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17270086705684662, \"iteration\": 440, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1352991908788681, \"iteration\": 441, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12709347903728485, \"iteration\": 442, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.15363359451293945, \"iteration\": 443, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18727220594882965, \"iteration\": 444, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.13808834552764893, \"iteration\": 445, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17944465577602386, \"iteration\": 446, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17500072717666626, \"iteration\": 447, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1700630635023117, \"iteration\": 448, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1810290813446045, \"iteration\": 449, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18728020787239075, \"iteration\": 450, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.12799398601055145, \"iteration\": 451, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.12161661684513092, \"iteration\": 452, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15377004444599152, \"iteration\": 453, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.15324737131595612, \"iteration\": 454, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.12600545585155487, \"iteration\": 455, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20434603095054626, \"iteration\": 456, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.12895217537879944, \"iteration\": 457, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14700530469417572, \"iteration\": 458, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1795211285352707, \"iteration\": 459, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1385343372821808, \"iteration\": 460, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12810443341732025, \"iteration\": 461, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.15145468711853027, \"iteration\": 462, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.10964620113372803, \"iteration\": 463, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14389613270759583, \"iteration\": 464, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2352546602487564, \"iteration\": 465, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19889207184314728, \"iteration\": 466, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17763881385326385, \"iteration\": 467, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17489032447338104, \"iteration\": 468, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14429698884487152, \"iteration\": 469, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.09992913901805878, \"iteration\": 470, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.20063692331314087, \"iteration\": 471, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2185516655445099, \"iteration\": 472, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.13683456182479858, \"iteration\": 473, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14351314306259155, \"iteration\": 474, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14887285232543945, \"iteration\": 475, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.23050111532211304, \"iteration\": 476, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.16658902168273926, \"iteration\": 477, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17001011967658997, \"iteration\": 478, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.13901609182357788, \"iteration\": 479, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.16617463529109955, \"iteration\": 480, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17083121836185455, \"iteration\": 481, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.1942427158355713, \"iteration\": 482, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1354447603225708, \"iteration\": 483, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16530826687812805, \"iteration\": 484, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1521206647157669, \"iteration\": 485, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.19719788432121277, \"iteration\": 486, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12383722513914108, \"iteration\": 487, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2032308280467987, \"iteration\": 488, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1504148244857788, \"iteration\": 489, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.1595563441514969, \"iteration\": 490, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1516197770833969, \"iteration\": 491, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23544920980930328, \"iteration\": 492, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1460968554019928, \"iteration\": 493, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.21560952067375183, \"iteration\": 494, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1255284994840622, \"iteration\": 495, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.14102217555046082, \"iteration\": 496, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.0935024619102478, \"iteration\": 497, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2485818713903427, \"iteration\": 498, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.12064540386199951, \"iteration\": 499, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.12806445360183716, \"iteration\": 500, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.14500315487384796, \"iteration\": 501, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1306808888912201, \"iteration\": 502, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.17204426229000092, \"iteration\": 503, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1570202261209488, \"iteration\": 504, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.202760249376297, \"iteration\": 505, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18380551040172577, \"iteration\": 506, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1656324863433838, \"iteration\": 507, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1185891404747963, \"iteration\": 508, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21683694422245026, \"iteration\": 509, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.19536061584949493, \"iteration\": 510, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1922275573015213, \"iteration\": 511, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.13539811968803406, \"iteration\": 512, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1385665088891983, \"iteration\": 513, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1633070707321167, \"iteration\": 514, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.16843605041503906, \"iteration\": 515, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18280313909053802, \"iteration\": 516, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.10877515375614166, \"iteration\": 517, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21260446310043335, \"iteration\": 518, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17229989171028137, \"iteration\": 519, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.15828539431095123, \"iteration\": 520, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.22920146584510803, \"iteration\": 521, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17625321447849274, \"iteration\": 522, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.23627576231956482, \"iteration\": 523, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.13173533976078033, \"iteration\": 524, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2340623438358307, \"iteration\": 525, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.16899126768112183, \"iteration\": 526, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12325183302164078, \"iteration\": 527, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15186740458011627, \"iteration\": 528, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.18086470663547516, \"iteration\": 529, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.14108192920684814, \"iteration\": 530, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22122690081596375, \"iteration\": 531, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16283901035785675, \"iteration\": 532, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20667371153831482, \"iteration\": 533, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23359718918800354, \"iteration\": 534, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1961064636707306, \"iteration\": 535, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14815382659435272, \"iteration\": 536, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10786272585391998, \"iteration\": 537, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16745206713676453, \"iteration\": 538, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17828252911567688, \"iteration\": 539, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.12521447241306305, \"iteration\": 540, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.22611407935619354, \"iteration\": 541, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.1960204839706421, \"iteration\": 542, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.15778423845767975, \"iteration\": 543, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15409338474273682, \"iteration\": 544, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14516912400722504, \"iteration\": 545, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.13193991780281067, \"iteration\": 546, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.1798391193151474, \"iteration\": 547, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1379833072423935, \"iteration\": 548, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1438673734664917, \"iteration\": 549, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.18329700827598572, \"iteration\": 550, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27506640553474426, \"iteration\": 551, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12621797621250153, \"iteration\": 552, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1723882555961609, \"iteration\": 553, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21236790716648102, \"iteration\": 554, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.21471604704856873, \"iteration\": 555, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.16920936107635498, \"iteration\": 556, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.148553267121315, \"iteration\": 557, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.17820307612419128, \"iteration\": 558, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20551033318042755, \"iteration\": 559, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17156541347503662, \"iteration\": 560, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19133396446704865, \"iteration\": 561, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2282746136188507, \"iteration\": 562, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.25912392139434814, \"iteration\": 563, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21268118917942047, \"iteration\": 564, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21983052790164948, \"iteration\": 565, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25419488549232483, \"iteration\": 566, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22230491042137146, \"iteration\": 567, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22023290395736694, \"iteration\": 568, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.16943736374378204, \"iteration\": 569, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.16205021739006042, \"iteration\": 570, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.18144851922988892, \"iteration\": 571, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2440904974937439, \"iteration\": 572, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1739315390586853, \"iteration\": 573, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1534251719713211, \"iteration\": 574, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13936719298362732, \"iteration\": 575, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2753371596336365, \"iteration\": 576, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21564367413520813, \"iteration\": 577, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.1653401255607605, \"iteration\": 578, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.28589943051338196, \"iteration\": 579, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19796939194202423, \"iteration\": 580, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21886050701141357, \"iteration\": 581, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17652493715286255, \"iteration\": 582, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3668521046638489, \"iteration\": 583, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.259495347738266, \"iteration\": 584, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1822112649679184, \"iteration\": 585, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15873146057128906, \"iteration\": 586, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.22073490917682648, \"iteration\": 587, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.28293392062187195, \"iteration\": 588, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.12523503601551056, \"iteration\": 589, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.18461450934410095, \"iteration\": 590, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.13740293681621552, \"iteration\": 591, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18422885239124298, \"iteration\": 592, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.16659091413021088, \"iteration\": 593, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2282758355140686, \"iteration\": 594, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19189336895942688, \"iteration\": 595, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15318794548511505, \"iteration\": 596, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.20888325572013855, \"iteration\": 597, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23637689650058746, \"iteration\": 598, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2613365948200226, \"iteration\": 599, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2481662482023239, \"iteration\": 600, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.30312231183052063, \"iteration\": 601, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.24813298881053925, \"iteration\": 602, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27238357067108154, \"iteration\": 603, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.15092581510543823, \"iteration\": 604, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.14785189926624298, \"iteration\": 605, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23146970570087433, \"iteration\": 606, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2157609760761261, \"iteration\": 607, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2262870818376541, \"iteration\": 608, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2954462170600891, \"iteration\": 609, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.18760302662849426, \"iteration\": 610, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2793614864349365, \"iteration\": 611, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19940638542175293, \"iteration\": 612, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23593942821025848, \"iteration\": 613, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.20411473512649536, \"iteration\": 614, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.1176254078745842, \"iteration\": 615, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.25539129972457886, \"iteration\": 616, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.16447271406650543, \"iteration\": 617, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21649709343910217, \"iteration\": 618, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.10979966074228287, \"iteration\": 619, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.32848232984542847, \"iteration\": 620, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.19162185490131378, \"iteration\": 621, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.37317103147506714, \"iteration\": 622, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.23391295969486237, \"iteration\": 623, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.1902381330728531, \"iteration\": 624, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.29564401507377625, \"iteration\": 625, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22348037362098694, \"iteration\": 626, \"epoch\": 3}, {\"training_acc\": 0.8571428571428571, \"training_loss\": 0.45308390259742737, \"iteration\": 627, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.09149851649999619, \"iteration\": 628, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07689877599477768, \"iteration\": 629, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13766981661319733, \"iteration\": 630, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.09107517451047897, \"iteration\": 631, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.13600040972232819, \"iteration\": 632, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09779351949691772, \"iteration\": 633, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.13560867309570312, \"iteration\": 634, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08147923648357391, \"iteration\": 635, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10538547486066818, \"iteration\": 636, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1516045480966568, \"iteration\": 637, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10008825361728668, \"iteration\": 638, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09844526648521423, \"iteration\": 639, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08419451117515564, \"iteration\": 640, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06076878309249878, \"iteration\": 641, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06504598259925842, \"iteration\": 642, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12036767601966858, \"iteration\": 643, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08036530762910843, \"iteration\": 644, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0879024937748909, \"iteration\": 645, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11561813950538635, \"iteration\": 646, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06438832730054855, \"iteration\": 647, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08028843253850937, \"iteration\": 648, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07497171312570572, \"iteration\": 649, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09543559700250626, \"iteration\": 650, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10635052621364594, \"iteration\": 651, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06065688654780388, \"iteration\": 652, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11326513439416885, \"iteration\": 653, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08949121087789536, \"iteration\": 654, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10441683977842331, \"iteration\": 655, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0938127189874649, \"iteration\": 656, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.09866722673177719, \"iteration\": 657, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0685562938451767, \"iteration\": 658, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.1032383143901825, \"iteration\": 659, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0901666060090065, \"iteration\": 660, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11577413231134415, \"iteration\": 661, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09692902863025665, \"iteration\": 662, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0952545553445816, \"iteration\": 663, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07964012026786804, \"iteration\": 664, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08706160634756088, \"iteration\": 665, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04476713389158249, \"iteration\": 666, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.05792324244976044, \"iteration\": 667, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08770300447940826, \"iteration\": 668, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.08529176563024521, \"iteration\": 669, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.05938604474067688, \"iteration\": 670, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.056523777544498444, \"iteration\": 671, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.0925600528717041, \"iteration\": 672, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05286940187215805, \"iteration\": 673, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04621057212352753, \"iteration\": 674, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1217186227440834, \"iteration\": 675, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09212259948253632, \"iteration\": 676, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11702261865139008, \"iteration\": 677, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.059408124536275864, \"iteration\": 678, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.10417210310697556, \"iteration\": 679, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09740900993347168, \"iteration\": 680, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11249680072069168, \"iteration\": 681, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04693584516644478, \"iteration\": 682, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07896635681390762, \"iteration\": 683, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10795614868402481, \"iteration\": 684, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0935036763548851, \"iteration\": 685, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.03657178953289986, \"iteration\": 686, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07272376120090485, \"iteration\": 687, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09040474146604538, \"iteration\": 688, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.050700630992650986, \"iteration\": 689, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0811360627412796, \"iteration\": 690, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06322474777698517, \"iteration\": 691, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1597238928079605, \"iteration\": 692, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.072055883705616, \"iteration\": 693, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11812473833560944, \"iteration\": 694, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.19274888932704926, \"iteration\": 695, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08037816733121872, \"iteration\": 696, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.13904030621051788, \"iteration\": 697, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11476844549179077, \"iteration\": 698, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10349174588918686, \"iteration\": 699, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12474691867828369, \"iteration\": 700, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1176610216498375, \"iteration\": 701, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11961335688829422, \"iteration\": 702, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14875665307044983, \"iteration\": 703, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10125357657670975, \"iteration\": 704, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07749025523662567, \"iteration\": 705, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.082180917263031, \"iteration\": 706, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10345372557640076, \"iteration\": 707, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09611901640892029, \"iteration\": 708, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06348762661218643, \"iteration\": 709, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05761246383190155, \"iteration\": 710, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09727989137172699, \"iteration\": 711, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.12225255370140076, \"iteration\": 712, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10977547615766525, \"iteration\": 713, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06061283499002457, \"iteration\": 714, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12325982749462128, \"iteration\": 715, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0633912980556488, \"iteration\": 716, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1137366071343422, \"iteration\": 717, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.05387284234166145, \"iteration\": 718, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09208331257104874, \"iteration\": 719, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10971119999885559, \"iteration\": 720, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07916153222322464, \"iteration\": 721, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07635100185871124, \"iteration\": 722, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08617084473371506, \"iteration\": 723, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0642860159277916, \"iteration\": 724, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06586768478155136, \"iteration\": 725, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04975803568959236, \"iteration\": 726, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08648240566253662, \"iteration\": 727, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1647653877735138, \"iteration\": 728, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16965241730213165, \"iteration\": 729, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.05782715231180191, \"iteration\": 730, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12074930965900421, \"iteration\": 731, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.060346923768520355, \"iteration\": 732, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05714765936136246, \"iteration\": 733, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1135023981332779, \"iteration\": 734, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04140164703130722, \"iteration\": 735, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08925504982471466, \"iteration\": 736, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.1035383939743042, \"iteration\": 737, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.09352390468120575, \"iteration\": 738, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.10713531821966171, \"iteration\": 739, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0838015154004097, \"iteration\": 740, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11897120624780655, \"iteration\": 741, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10301245748996735, \"iteration\": 742, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16143502295017242, \"iteration\": 743, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05096284672617912, \"iteration\": 744, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09364919364452362, \"iteration\": 745, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11680310219526291, \"iteration\": 746, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07975161820650101, \"iteration\": 747, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1757393777370453, \"iteration\": 748, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.16420094668865204, \"iteration\": 749, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03512481600046158, \"iteration\": 750, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16665370762348175, \"iteration\": 751, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07762940227985382, \"iteration\": 752, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08829747885465622, \"iteration\": 753, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11460626870393753, \"iteration\": 754, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10824070870876312, \"iteration\": 755, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11688702553510666, \"iteration\": 756, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10512083023786545, \"iteration\": 757, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09183817356824875, \"iteration\": 758, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12020590901374817, \"iteration\": 759, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11256305128335953, \"iteration\": 760, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.14146357774734497, \"iteration\": 761, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06883752346038818, \"iteration\": 762, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11069734394550323, \"iteration\": 763, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08806653320789337, \"iteration\": 764, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09589262306690216, \"iteration\": 765, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.05610613524913788, \"iteration\": 766, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16380248963832855, \"iteration\": 767, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09670041501522064, \"iteration\": 768, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.15150725841522217, \"iteration\": 769, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07082019746303558, \"iteration\": 770, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11085673421621323, \"iteration\": 771, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11952181160449982, \"iteration\": 772, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04022824391722679, \"iteration\": 773, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10049588978290558, \"iteration\": 774, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11217892915010452, \"iteration\": 775, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04590433090925217, \"iteration\": 776, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10888244211673737, \"iteration\": 777, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12858083844184875, \"iteration\": 778, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07100130617618561, \"iteration\": 779, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08207409083843231, \"iteration\": 780, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19187816977500916, \"iteration\": 781, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.14697808027267456, \"iteration\": 782, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.14812663197517395, \"iteration\": 783, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0753299817442894, \"iteration\": 784, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08538582921028137, \"iteration\": 785, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11409623920917511, \"iteration\": 786, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.13512544333934784, \"iteration\": 787, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15054050087928772, \"iteration\": 788, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.18197961151599884, \"iteration\": 789, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16674843430519104, \"iteration\": 790, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1387712061405182, \"iteration\": 791, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10347466915845871, \"iteration\": 792, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15564024448394775, \"iteration\": 793, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.14058056473731995, \"iteration\": 794, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16159459948539734, \"iteration\": 795, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.09966612607240677, \"iteration\": 796, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10029041022062302, \"iteration\": 797, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1277027279138565, \"iteration\": 798, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11477293819189072, \"iteration\": 799, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07601183652877808, \"iteration\": 800, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06303337961435318, \"iteration\": 801, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.14281243085861206, \"iteration\": 802, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.13376878201961517, \"iteration\": 803, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11674803495407104, \"iteration\": 804, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1410224437713623, \"iteration\": 805, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10414960980415344, \"iteration\": 806, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0681847482919693, \"iteration\": 807, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04723962023854256, \"iteration\": 808, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11283398419618607, \"iteration\": 809, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08812454342842102, \"iteration\": 810, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1583901196718216, \"iteration\": 811, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08817193657159805, \"iteration\": 812, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11499705910682678, \"iteration\": 813, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15030719339847565, \"iteration\": 814, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19856402277946472, \"iteration\": 815, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0666460245847702, \"iteration\": 816, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11540732532739639, \"iteration\": 817, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07258094102144241, \"iteration\": 818, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13245947659015656, \"iteration\": 819, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.09540106356143951, \"iteration\": 820, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14404720067977905, \"iteration\": 821, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15293166041374207, \"iteration\": 822, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.061657026410102844, \"iteration\": 823, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09879465401172638, \"iteration\": 824, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11635160446166992, \"iteration\": 825, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13374534249305725, \"iteration\": 826, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.18386754393577576, \"iteration\": 827, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.0757291316986084, \"iteration\": 828, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.15287239849567413, \"iteration\": 829, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04297536239027977, \"iteration\": 830, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.09500652551651001, \"iteration\": 831, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.03153519332408905, \"iteration\": 832, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10530681163072586, \"iteration\": 833, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.14480450749397278, \"iteration\": 834, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06824108213186264, \"iteration\": 835, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.14036427438259125, \"iteration\": 836, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.024001365527510643, \"iteration\": 837, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.030166976153850555, \"iteration\": 838, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06453673541545868, \"iteration\": 839, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06074151024222374, \"iteration\": 840, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05104393512010574, \"iteration\": 841, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.04100130498409271, \"iteration\": 842, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.028501097112894058, \"iteration\": 843, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10545399785041809, \"iteration\": 844, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0544857494533062, \"iteration\": 845, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.045891642570495605, \"iteration\": 846, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.026241198182106018, \"iteration\": 847, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07401590794324875, \"iteration\": 848, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.036023031920194626, \"iteration\": 849, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0650264322757721, \"iteration\": 850, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07278857380151749, \"iteration\": 851, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07088100910186768, \"iteration\": 852, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04358045756816864, \"iteration\": 853, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022439153864979744, \"iteration\": 854, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03224920853972435, \"iteration\": 855, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06650999188423157, \"iteration\": 856, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02830946072936058, \"iteration\": 857, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.04340901970863342, \"iteration\": 858, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.10109835863113403, \"iteration\": 859, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027743298560380936, \"iteration\": 860, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06533616781234741, \"iteration\": 861, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06491804867982864, \"iteration\": 862, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023556705564260483, \"iteration\": 863, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03846970200538635, \"iteration\": 864, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04103637859225273, \"iteration\": 865, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.036279477179050446, \"iteration\": 866, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08347101509571075, \"iteration\": 867, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07251076400279999, \"iteration\": 868, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08928123861551285, \"iteration\": 869, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.045235417783260345, \"iteration\": 870, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0172028299421072, \"iteration\": 871, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.04004175215959549, \"iteration\": 872, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06955430656671524, \"iteration\": 873, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0410473607480526, \"iteration\": 874, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.037418656051158905, \"iteration\": 875, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03757379949092865, \"iteration\": 876, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03651728481054306, \"iteration\": 877, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03738096356391907, \"iteration\": 878, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03456684574484825, \"iteration\": 879, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022910529747605324, \"iteration\": 880, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023740287870168686, \"iteration\": 881, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05100888013839722, \"iteration\": 882, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05709678307175636, \"iteration\": 883, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.012238822877407074, \"iteration\": 884, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03559471294283867, \"iteration\": 885, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.011377319693565369, \"iteration\": 886, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03667938709259033, \"iteration\": 887, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.0434911772608757, \"iteration\": 888, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01666039228439331, \"iteration\": 889, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.014977749437093735, \"iteration\": 890, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06106436252593994, \"iteration\": 891, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06225130707025528, \"iteration\": 892, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.019946381449699402, \"iteration\": 893, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03191080689430237, \"iteration\": 894, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03810783848166466, \"iteration\": 895, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05037426948547363, \"iteration\": 896, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.060990482568740845, \"iteration\": 897, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.056487344205379486, \"iteration\": 898, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.056410521268844604, \"iteration\": 899, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03571908175945282, \"iteration\": 900, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04604063928127289, \"iteration\": 901, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05248629301786423, \"iteration\": 902, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03587812930345535, \"iteration\": 903, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04445894807577133, \"iteration\": 904, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022101307287812233, \"iteration\": 905, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020853832364082336, \"iteration\": 906, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018720699474215508, \"iteration\": 907, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03768150880932808, \"iteration\": 908, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02746340073645115, \"iteration\": 909, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09576531499624252, \"iteration\": 910, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03849153593182564, \"iteration\": 911, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.026150764897465706, \"iteration\": 912, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05187259614467621, \"iteration\": 913, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.026966869831085205, \"iteration\": 914, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.012538190931081772, \"iteration\": 915, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05742258206009865, \"iteration\": 916, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03286611661314964, \"iteration\": 917, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08615563809871674, \"iteration\": 918, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.029820144176483154, \"iteration\": 919, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.025386404246091843, \"iteration\": 920, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.058583274483680725, \"iteration\": 921, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.05278928577899933, \"iteration\": 922, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.029345380142331123, \"iteration\": 923, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07985910028219223, \"iteration\": 924, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06502441316843033, \"iteration\": 925, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.036671463400125504, \"iteration\": 926, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02214980125427246, \"iteration\": 927, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.024170489981770515, \"iteration\": 928, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.056047018617391586, \"iteration\": 929, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.040122102946043015, \"iteration\": 930, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027099497616291046, \"iteration\": 931, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0794539600610733, \"iteration\": 932, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.057771049439907074, \"iteration\": 933, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05818190425634384, \"iteration\": 934, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02087126113474369, \"iteration\": 935, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022929996252059937, \"iteration\": 936, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.026177695021033287, \"iteration\": 937, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03226727992296219, \"iteration\": 938, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.036143213510513306, \"iteration\": 939, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04604936018586159, \"iteration\": 940, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03397820517420769, \"iteration\": 941, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0792660266160965, \"iteration\": 942, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023332875221967697, \"iteration\": 943, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.026280511170625687, \"iteration\": 944, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04342807084321976, \"iteration\": 945, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027482807636260986, \"iteration\": 946, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0676938146352768, \"iteration\": 947, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04078853130340576, \"iteration\": 948, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.050018537789583206, \"iteration\": 949, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.062165383249521255, \"iteration\": 950, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01847933791577816, \"iteration\": 951, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.029009858146309853, \"iteration\": 952, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.049639735370874405, \"iteration\": 953, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.026322079822421074, \"iteration\": 954, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07208975404500961, \"iteration\": 955, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06681665778160095, \"iteration\": 956, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04551476985216141, \"iteration\": 957, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.039086565375328064, \"iteration\": 958, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06079321354627609, \"iteration\": 959, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.012872334569692612, \"iteration\": 960, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.021225715056061745, \"iteration\": 961, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.016151027753949165, \"iteration\": 962, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.059692271053791046, \"iteration\": 963, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06243069842457771, \"iteration\": 964, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.038284532725811005, \"iteration\": 965, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.030069880187511444, \"iteration\": 966, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.041715897619724274, \"iteration\": 967, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02462952211499214, \"iteration\": 968, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1744987666606903, \"iteration\": 969, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.058584727346897125, \"iteration\": 970, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017108464613556862, \"iteration\": 971, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.030060777440667152, \"iteration\": 972, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02023138292133808, \"iteration\": 973, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02642100118100643, \"iteration\": 974, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.050310831516981125, \"iteration\": 975, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05165687948465347, \"iteration\": 976, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07830563932657242, \"iteration\": 977, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02338225767016411, \"iteration\": 978, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0700049176812172, \"iteration\": 979, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02220175601541996, \"iteration\": 980, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.045781753957271576, \"iteration\": 981, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03928951919078827, \"iteration\": 982, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.053383734077215195, \"iteration\": 983, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.016468973830342293, \"iteration\": 984, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023081675171852112, \"iteration\": 985, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.062319740653038025, \"iteration\": 986, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.021324990317225456, \"iteration\": 987, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03521386533975601, \"iteration\": 988, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0393051877617836, \"iteration\": 989, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05693172290921211, \"iteration\": 990, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02409222349524498, \"iteration\": 991, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.042273517698049545, \"iteration\": 992, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02277216501533985, \"iteration\": 993, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.09363099187612534, \"iteration\": 994, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022717570886015892, \"iteration\": 995, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.047354791313409805, \"iteration\": 996, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10885989665985107, \"iteration\": 997, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026714403182268143, \"iteration\": 998, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0341145358979702, \"iteration\": 999, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03763238340616226, \"iteration\": 1000, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03091060370206833, \"iteration\": 1001, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0639035552740097, \"iteration\": 1002, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.013352269306778908, \"iteration\": 1003, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.034036725759506226, \"iteration\": 1004, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.029670221731066704, \"iteration\": 1005, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0148657551035285, \"iteration\": 1006, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.035971254110336304, \"iteration\": 1007, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02943391725420952, \"iteration\": 1008, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.0601823516190052, \"iteration\": 1009, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05146921053528786, \"iteration\": 1010, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.008844039402902126, \"iteration\": 1011, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022447729483246803, \"iteration\": 1012, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05930732190608978, \"iteration\": 1013, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05824747681617737, \"iteration\": 1014, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02162797376513481, \"iteration\": 1015, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02232247032225132, \"iteration\": 1016, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1209065318107605, \"iteration\": 1017, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026746533811092377, \"iteration\": 1018, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03422639146447182, \"iteration\": 1019, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04100489616394043, \"iteration\": 1020, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.034517113119363785, \"iteration\": 1021, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.043569985777139664, \"iteration\": 1022, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04779105633497238, \"iteration\": 1023, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06190316379070282, \"iteration\": 1024, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.0699944943189621, \"iteration\": 1025, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.015003149397671223, \"iteration\": 1026, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.029371438547968864, \"iteration\": 1027, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02501887083053589, \"iteration\": 1028, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02017996273934841, \"iteration\": 1029, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.06578153371810913, \"iteration\": 1030, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03870724141597748, \"iteration\": 1031, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.029411830008029938, \"iteration\": 1032, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.047203633934259415, \"iteration\": 1033, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.047370098531246185, \"iteration\": 1034, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.041854433715343475, \"iteration\": 1035, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.026605386286973953, \"iteration\": 1036, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0995170995593071, \"iteration\": 1037, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09215141832828522, \"iteration\": 1038, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.015145010314881802, \"iteration\": 1039, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022810718044638634, \"iteration\": 1040, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09265551716089249, \"iteration\": 1041, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018587367609143257, \"iteration\": 1042, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10044816881418228, \"iteration\": 1043, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05873144790530205, \"iteration\": 1044, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0017211284721270204, \"iteration\": 1045, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.012566197663545609, \"iteration\": 1046, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02088610641658306, \"iteration\": 1047, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012930938974022865, \"iteration\": 1048, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009066524915397167, \"iteration\": 1049, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009328572079539299, \"iteration\": 1050, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007870352827012539, \"iteration\": 1051, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00797016266733408, \"iteration\": 1052, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006898591294884682, \"iteration\": 1053, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006822911091148853, \"iteration\": 1054, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007305636070668697, \"iteration\": 1055, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014233001507818699, \"iteration\": 1056, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004719016142189503, \"iteration\": 1057, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010498068295419216, \"iteration\": 1058, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010282923467457294, \"iteration\": 1059, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011107422411441803, \"iteration\": 1060, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010449116118252277, \"iteration\": 1061, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00859889853745699, \"iteration\": 1062, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00753793865442276, \"iteration\": 1063, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013984180986881256, \"iteration\": 1064, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004708070773631334, \"iteration\": 1065, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004231702070683241, \"iteration\": 1066, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01959533430635929, \"iteration\": 1067, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006640705280005932, \"iteration\": 1068, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006819155998528004, \"iteration\": 1069, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.044060658663511276, \"iteration\": 1070, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011739037930965424, \"iteration\": 1071, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009629379957914352, \"iteration\": 1072, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026863109320402145, \"iteration\": 1073, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.015211345627903938, \"iteration\": 1074, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008906880393624306, \"iteration\": 1075, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03354353457689285, \"iteration\": 1076, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013685180805623531, \"iteration\": 1077, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.034484222531318665, \"iteration\": 1078, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009678566828370094, \"iteration\": 1079, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01563325710594654, \"iteration\": 1080, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0080214599147439, \"iteration\": 1081, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005138225853443146, \"iteration\": 1082, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010781599208712578, \"iteration\": 1083, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007116727996617556, \"iteration\": 1084, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02459927462041378, \"iteration\": 1085, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004505470395088196, \"iteration\": 1086, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011961955577135086, \"iteration\": 1087, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014356533996760845, \"iteration\": 1088, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011443184688687325, \"iteration\": 1089, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007281406316906214, \"iteration\": 1090, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009898393414914608, \"iteration\": 1091, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.020887793973088264, \"iteration\": 1092, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009746843948960304, \"iteration\": 1093, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005492549389600754, \"iteration\": 1094, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0072955600917339325, \"iteration\": 1095, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008679535239934921, \"iteration\": 1096, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01683720014989376, \"iteration\": 1097, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00883950013667345, \"iteration\": 1098, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006967407185584307, \"iteration\": 1099, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01294443104416132, \"iteration\": 1100, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008178533054888248, \"iteration\": 1101, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007138200104236603, \"iteration\": 1102, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010491646826267242, \"iteration\": 1103, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014021072536706924, \"iteration\": 1104, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0056588915176689625, \"iteration\": 1105, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.026420611888170242, \"iteration\": 1106, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013872583396732807, \"iteration\": 1107, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007761840708553791, \"iteration\": 1108, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005662835203111172, \"iteration\": 1109, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.03590064495801926, \"iteration\": 1110, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011029427871108055, \"iteration\": 1111, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.015835462138056755, \"iteration\": 1112, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007178532425314188, \"iteration\": 1113, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0069427527487277985, \"iteration\": 1114, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01850583776831627, \"iteration\": 1115, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00462938379496336, \"iteration\": 1116, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006164629478007555, \"iteration\": 1117, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005595849361270666, \"iteration\": 1118, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005268104840070009, \"iteration\": 1119, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012613704428076744, \"iteration\": 1120, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01581583358347416, \"iteration\": 1121, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01284991204738617, \"iteration\": 1122, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00848808791488409, \"iteration\": 1123, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009515346959233284, \"iteration\": 1124, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0054046413861215115, \"iteration\": 1125, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.035726215690374374, \"iteration\": 1126, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.042661864310503006, \"iteration\": 1127, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011094894260168076, \"iteration\": 1128, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.016502881422638893, \"iteration\": 1129, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00869377888739109, \"iteration\": 1130, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05699058994650841, \"iteration\": 1131, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021967440843582153, \"iteration\": 1132, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0077576544135808945, \"iteration\": 1133, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010228956118226051, \"iteration\": 1134, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0021819949615746737, \"iteration\": 1135, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009330283850431442, \"iteration\": 1136, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.025346525013446808, \"iteration\": 1137, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.033306196331977844, \"iteration\": 1138, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006335394456982613, \"iteration\": 1139, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008486400358378887, \"iteration\": 1140, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006847008131444454, \"iteration\": 1141, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008151805028319359, \"iteration\": 1142, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009347878396511078, \"iteration\": 1143, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04065581038594246, \"iteration\": 1144, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006147116422653198, \"iteration\": 1145, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003720958484336734, \"iteration\": 1146, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.032013021409511566, \"iteration\": 1147, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006122966762632132, \"iteration\": 1148, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008104609325528145, \"iteration\": 1149, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008666165173053741, \"iteration\": 1150, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.015823040157556534, \"iteration\": 1151, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.016024712473154068, \"iteration\": 1152, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005707199219614267, \"iteration\": 1153, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03008677065372467, \"iteration\": 1154, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014393020421266556, \"iteration\": 1155, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0064909388311207294, \"iteration\": 1156, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008170944638550282, \"iteration\": 1157, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0056805601343512535, \"iteration\": 1158, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012836755253374577, \"iteration\": 1159, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0047331880778074265, \"iteration\": 1160, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003857984906062484, \"iteration\": 1161, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008128453977406025, \"iteration\": 1162, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01619681902229786, \"iteration\": 1163, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00327604403719306, \"iteration\": 1164, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.016363294795155525, \"iteration\": 1165, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0204410832375288, \"iteration\": 1166, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0032665005419403315, \"iteration\": 1167, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00594672467559576, \"iteration\": 1168, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04394374415278435, \"iteration\": 1169, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007336590904742479, \"iteration\": 1170, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04982103034853935, \"iteration\": 1171, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004670232534408569, \"iteration\": 1172, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.025178778916597366, \"iteration\": 1173, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004127916414290667, \"iteration\": 1174, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004385123495012522, \"iteration\": 1175, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005741511937230825, \"iteration\": 1176, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.016443349421024323, \"iteration\": 1177, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0054824696853756905, \"iteration\": 1178, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0026897394564002752, \"iteration\": 1179, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02494988776743412, \"iteration\": 1180, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004218770656734705, \"iteration\": 1181, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005294762086123228, \"iteration\": 1182, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009792999364435673, \"iteration\": 1183, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006485385820269585, \"iteration\": 1184, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011634069494903088, \"iteration\": 1185, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003081119619309902, \"iteration\": 1186, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0054536606185138226, \"iteration\": 1187, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014000814408063889, \"iteration\": 1188, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003786703571677208, \"iteration\": 1189, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006557763088494539, \"iteration\": 1190, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005224213469773531, \"iteration\": 1191, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01775430515408516, \"iteration\": 1192, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006639563944190741, \"iteration\": 1193, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007386386394500732, \"iteration\": 1194, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.038755834102630615, \"iteration\": 1195, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006494468078017235, \"iteration\": 1196, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011067675426602364, \"iteration\": 1197, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008045784197747707, \"iteration\": 1198, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007147474214434624, \"iteration\": 1199, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03345223516225815, \"iteration\": 1200, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010914907790720463, \"iteration\": 1201, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006087142042815685, \"iteration\": 1202, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010130883194506168, \"iteration\": 1203, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.019812308251857758, \"iteration\": 1204, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006196367088705301, \"iteration\": 1205, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00639047846198082, \"iteration\": 1206, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009794504381716251, \"iteration\": 1207, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005612509325146675, \"iteration\": 1208, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007181241177022457, \"iteration\": 1209, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007686681114137173, \"iteration\": 1210, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0049267904832959175, \"iteration\": 1211, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0075250351801514626, \"iteration\": 1212, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0037703500129282475, \"iteration\": 1213, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008848993107676506, \"iteration\": 1214, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.04809726029634476, \"iteration\": 1215, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02322046458721161, \"iteration\": 1216, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00298829167149961, \"iteration\": 1217, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0040490818209946156, \"iteration\": 1218, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00872263964265585, \"iteration\": 1219, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011725439690053463, \"iteration\": 1220, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005074959248304367, \"iteration\": 1221, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004231875296682119, \"iteration\": 1222, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00407221307978034, \"iteration\": 1223, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.025712409988045692, \"iteration\": 1224, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006976361386477947, \"iteration\": 1225, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010456059128046036, \"iteration\": 1226, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01601225882768631, \"iteration\": 1227, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00741160660982132, \"iteration\": 1228, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004895157180726528, \"iteration\": 1229, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004447431303560734, \"iteration\": 1230, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007968008518218994, \"iteration\": 1231, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003662414848804474, \"iteration\": 1232, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02231309749186039, \"iteration\": 1233, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003081095404922962, \"iteration\": 1234, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005158389452844858, \"iteration\": 1235, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.002715817652642727, \"iteration\": 1236, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0069824084639549255, \"iteration\": 1237, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014039485715329647, \"iteration\": 1238, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006472408305853605, \"iteration\": 1239, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.017743965610861778, \"iteration\": 1240, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00670924736186862, \"iteration\": 1241, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004551107529550791, \"iteration\": 1242, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00773027166724205, \"iteration\": 1243, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004365319851785898, \"iteration\": 1244, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.018202031031250954, \"iteration\": 1245, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0030553778633475304, \"iteration\": 1246, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006940935272723436, \"iteration\": 1247, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004123283084481955, \"iteration\": 1248, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004010177217423916, \"iteration\": 1249, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007322163786739111, \"iteration\": 1250, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00958646647632122, \"iteration\": 1251, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0021810063626617193, \"iteration\": 1252, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006481077987700701, \"iteration\": 1253, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003042214782908559, \"iteration\": 1254, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.002761098789051175, \"iteration\": 1255, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002707286737859249, \"iteration\": 1256, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00231668702326715, \"iteration\": 1257, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003393184393644333, \"iteration\": 1258, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018768699374049902, \"iteration\": 1259, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01949283666908741, \"iteration\": 1260, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0025569326244294643, \"iteration\": 1261, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0022792790550738573, \"iteration\": 1262, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.009306199848651886, \"iteration\": 1263, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015919775469228625, \"iteration\": 1264, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002910128328949213, \"iteration\": 1265, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003416782943531871, \"iteration\": 1266, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001889354083687067, \"iteration\": 1267, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020985030569136143, \"iteration\": 1268, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002822709968313575, \"iteration\": 1269, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002148389583453536, \"iteration\": 1270, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0024494214449077845, \"iteration\": 1271, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013718183618038893, \"iteration\": 1272, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016928386176005006, \"iteration\": 1273, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01775415800511837, \"iteration\": 1274, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002110188826918602, \"iteration\": 1275, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013958708150312304, \"iteration\": 1276, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0029819279443472624, \"iteration\": 1277, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017416218761354685, \"iteration\": 1278, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0023392566945403814, \"iteration\": 1279, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016866992227733135, \"iteration\": 1280, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002809366676956415, \"iteration\": 1281, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008663706830702722, \"iteration\": 1282, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002094726078212261, \"iteration\": 1283, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016061626374721527, \"iteration\": 1284, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001481622806750238, \"iteration\": 1285, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0026971730403602123, \"iteration\": 1286, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0027827026788145304, \"iteration\": 1287, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001589854364283383, \"iteration\": 1288, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016843626508489251, \"iteration\": 1289, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013379761949181557, \"iteration\": 1290, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013375761918723583, \"iteration\": 1291, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018492829985916615, \"iteration\": 1292, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015278497012332082, \"iteration\": 1293, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011266723740845919, \"iteration\": 1294, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002636941848322749, \"iteration\": 1295, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016055043088272214, \"iteration\": 1296, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018395185470581055, \"iteration\": 1297, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001996621023863554, \"iteration\": 1298, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013361754827201366, \"iteration\": 1299, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00226786220446229, \"iteration\": 1300, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014794273301959038, \"iteration\": 1301, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020326972007751465, \"iteration\": 1302, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0034592929296195507, \"iteration\": 1303, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002667596796527505, \"iteration\": 1304, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014966257149353623, \"iteration\": 1305, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016952260630205274, \"iteration\": 1306, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018617999739944935, \"iteration\": 1307, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03277520835399628, \"iteration\": 1308, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003323122626170516, \"iteration\": 1309, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0027877390384674072, \"iteration\": 1310, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011114324443042278, \"iteration\": 1311, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011930931359529495, \"iteration\": 1312, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013434134889394045, \"iteration\": 1313, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013388555962592363, \"iteration\": 1314, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001681324327364564, \"iteration\": 1315, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002856701612472534, \"iteration\": 1316, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0021290248259902, \"iteration\": 1317, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012898495187982917, \"iteration\": 1318, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00150387326721102, \"iteration\": 1319, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013784506591036916, \"iteration\": 1320, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001569788553752005, \"iteration\": 1321, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.018236026167869568, \"iteration\": 1322, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001224569627083838, \"iteration\": 1323, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0021345247514545918, \"iteration\": 1324, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015770168974995613, \"iteration\": 1325, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002213958417996764, \"iteration\": 1326, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008948311442509294, \"iteration\": 1327, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0019793123938143253, \"iteration\": 1328, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001559668336994946, \"iteration\": 1329, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.05804165452718735, \"iteration\": 1330, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014819643693044782, \"iteration\": 1331, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015341792022809386, \"iteration\": 1332, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016712202923372388, \"iteration\": 1333, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017886257264763117, \"iteration\": 1334, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015157802263274789, \"iteration\": 1335, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010681112762540579, \"iteration\": 1336, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003571960609406233, \"iteration\": 1337, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014404569519683719, \"iteration\": 1338, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002561519155278802, \"iteration\": 1339, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011997923720628023, \"iteration\": 1340, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.009715347550809383, \"iteration\": 1341, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016443986678496003, \"iteration\": 1342, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010457041207700968, \"iteration\": 1343, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001297156559303403, \"iteration\": 1344, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016409535892307758, \"iteration\": 1345, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0037649720907211304, \"iteration\": 1346, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03394659608602524, \"iteration\": 1347, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017449845327064395, \"iteration\": 1348, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0036994547117501497, \"iteration\": 1349, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015880245482549071, \"iteration\": 1350, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.017277441918849945, \"iteration\": 1351, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014903827104717493, \"iteration\": 1352, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012262818636372685, \"iteration\": 1353, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015112761175259948, \"iteration\": 1354, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0019671060144901276, \"iteration\": 1355, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002649093745276332, \"iteration\": 1356, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0038152476772665977, \"iteration\": 1357, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0032036842312663794, \"iteration\": 1358, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018914785468950868, \"iteration\": 1359, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001140646287240088, \"iteration\": 1360, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014642663300037384, \"iteration\": 1361, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0022038815077394247, \"iteration\": 1362, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001987329451367259, \"iteration\": 1363, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0027184723876416683, \"iteration\": 1364, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017407996347174048, \"iteration\": 1365, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012068053474649787, \"iteration\": 1366, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011221738532185555, \"iteration\": 1367, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0021399774122983217, \"iteration\": 1368, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014206445775926113, \"iteration\": 1369, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003112552687525749, \"iteration\": 1370, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013786291237920523, \"iteration\": 1371, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015207466203719378, \"iteration\": 1372, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012026047334074974, \"iteration\": 1373, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.010895493440330029, \"iteration\": 1374, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011839179787784815, \"iteration\": 1375, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018173969583585858, \"iteration\": 1376, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0030089542269706726, \"iteration\": 1377, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018740356899797916, \"iteration\": 1378, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015282644890248775, \"iteration\": 1379, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014277283335104585, \"iteration\": 1380, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0028085466474294662, \"iteration\": 1381, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0023214854300022125, \"iteration\": 1382, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001968216383829713, \"iteration\": 1383, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001418248750269413, \"iteration\": 1384, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020321477204561234, \"iteration\": 1385, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0019305553287267685, \"iteration\": 1386, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00123299821279943, \"iteration\": 1387, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008162134909071028, \"iteration\": 1388, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000995282200165093, \"iteration\": 1389, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001398037769831717, \"iteration\": 1390, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012684680987149477, \"iteration\": 1391, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0028721534181386232, \"iteration\": 1392, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010555281769484282, \"iteration\": 1393, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014611121732741594, \"iteration\": 1394, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006115590687841177, \"iteration\": 1395, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011792839504778385, \"iteration\": 1396, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020047929137945175, \"iteration\": 1397, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018776189535856247, \"iteration\": 1398, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010862816125154495, \"iteration\": 1399, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010569548467174172, \"iteration\": 1400, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010817379225045443, \"iteration\": 1401, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001728010131046176, \"iteration\": 1402, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020947165321558714, \"iteration\": 1403, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007122212555259466, \"iteration\": 1404, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0028867267537862062, \"iteration\": 1405, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.031648196280002594, \"iteration\": 1406, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001142197521403432, \"iteration\": 1407, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020657323766499758, \"iteration\": 1408, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012911295052617788, \"iteration\": 1409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00121702637989074, \"iteration\": 1410, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014118386898189783, \"iteration\": 1411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001160795334726572, \"iteration\": 1412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011831404408439994, \"iteration\": 1413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017490448663011193, \"iteration\": 1414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0019256514497101307, \"iteration\": 1415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013675864320248365, \"iteration\": 1416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015872075455263257, \"iteration\": 1417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0032586888410151005, \"iteration\": 1418, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009464658214710653, \"iteration\": 1419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003834279254078865, \"iteration\": 1420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012703508837148547, \"iteration\": 1421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001170009491033852, \"iteration\": 1422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001538371085189283, \"iteration\": 1423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020163417793810368, \"iteration\": 1424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014938842505216599, \"iteration\": 1425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007517379708588123, \"iteration\": 1426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002437261864542961, \"iteration\": 1427, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001688990043476224, \"iteration\": 1428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010959883220493793, \"iteration\": 1429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014785861130803823, \"iteration\": 1430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013789861695840955, \"iteration\": 1431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001566717168316245, \"iteration\": 1432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017576355021446943, \"iteration\": 1433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008766137761995196, \"iteration\": 1434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018154067220166326, \"iteration\": 1435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004584852140396833, \"iteration\": 1436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0022543410304933786, \"iteration\": 1437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015569140668958426, \"iteration\": 1438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012796667870134115, \"iteration\": 1439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013214393984526396, \"iteration\": 1440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008063985733315349, \"iteration\": 1441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001452433760277927, \"iteration\": 1442, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014480594545602798, \"iteration\": 1443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008751306449994445, \"iteration\": 1444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011994255473837256, \"iteration\": 1445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011212971294298768, \"iteration\": 1446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016135215992107987, \"iteration\": 1447, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008673866395838559, \"iteration\": 1448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011927682207897305, \"iteration\": 1449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013375661801546812, \"iteration\": 1450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011478605447337031, \"iteration\": 1451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020146891474723816, \"iteration\": 1452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010746612679213285, \"iteration\": 1453, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016415448626503348, \"iteration\": 1454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000907531997654587, \"iteration\": 1455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010509334970265627, \"iteration\": 1456, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015988362720236182, \"iteration\": 1457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000580053252633661, \"iteration\": 1458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010576363420113921, \"iteration\": 1459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011153101222589612, \"iteration\": 1460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014702568296343088, \"iteration\": 1461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006098984740674496, \"iteration\": 1462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005505468579940498, \"iteration\": 1463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009404404554516077, \"iteration\": 1464, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012626808602362871, \"iteration\": 1465, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007196258520707488, \"iteration\": 1466, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000722925120498985, \"iteration\": 1467, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004880811902694404, \"iteration\": 1468, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00047776586143299937, \"iteration\": 1469, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000567335169762373, \"iteration\": 1470, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005878034862689674, \"iteration\": 1471, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005326442769728601, \"iteration\": 1472, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005920277326367795, \"iteration\": 1473, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006349687464535236, \"iteration\": 1474, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005957768298685551, \"iteration\": 1475, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000644355604890734, \"iteration\": 1476, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005859090015292168, \"iteration\": 1477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007325092446990311, \"iteration\": 1478, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03138482943177223, \"iteration\": 1479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005958686233498156, \"iteration\": 1480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00045708182733505964, \"iteration\": 1481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004917468759231269, \"iteration\": 1482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002068473491817713, \"iteration\": 1483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000653059221804142, \"iteration\": 1484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006793978973291814, \"iteration\": 1485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005173679674044251, \"iteration\": 1486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008605489274486899, \"iteration\": 1487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002214597538113594, \"iteration\": 1488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006973030976951122, \"iteration\": 1489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0014840018702670932, \"iteration\": 1490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00038215669337660074, \"iteration\": 1491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006182452780194581, \"iteration\": 1492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0022271901834756136, \"iteration\": 1493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004098601348232478, \"iteration\": 1494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006509289378300309, \"iteration\": 1495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004349062219262123, \"iteration\": 1496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005979117704555392, \"iteration\": 1497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00042171194218099117, \"iteration\": 1498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00043897770228795707, \"iteration\": 1499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008478694944642484, \"iteration\": 1500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00043686479330062866, \"iteration\": 1501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000470867904368788, \"iteration\": 1502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004309520882088691, \"iteration\": 1503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008229441591538489, \"iteration\": 1504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004689978377427906, \"iteration\": 1505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001051414874382317, \"iteration\": 1506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004554795450530946, \"iteration\": 1507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003882187884300947, \"iteration\": 1508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00038577266968786716, \"iteration\": 1509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.016563206911087036, \"iteration\": 1510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004949114518240094, \"iteration\": 1511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005884081474505365, \"iteration\": 1512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00065531738800928, \"iteration\": 1513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005977963446639478, \"iteration\": 1514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009061397868208587, \"iteration\": 1515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009026892366819084, \"iteration\": 1516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005767412949353456, \"iteration\": 1517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005400971276685596, \"iteration\": 1518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00046666187699884176, \"iteration\": 1519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003175997408106923, \"iteration\": 1520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000368231616448611, \"iteration\": 1521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005699272151105106, \"iteration\": 1522, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.020626969635486603, \"iteration\": 1523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004185666621197015, \"iteration\": 1524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013136512134224176, \"iteration\": 1525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001439560204744339, \"iteration\": 1526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007114901090972126, \"iteration\": 1527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010017618769779801, \"iteration\": 1528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006137298187240958, \"iteration\": 1529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010220828698948026, \"iteration\": 1530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012144435895606875, \"iteration\": 1531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015082847094163299, \"iteration\": 1532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001721356064081192, \"iteration\": 1533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006632831646129489, \"iteration\": 1534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00042186619248241186, \"iteration\": 1535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005765961250290275, \"iteration\": 1536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007447107927873731, \"iteration\": 1537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007365522324107587, \"iteration\": 1538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005914398934692144, \"iteration\": 1539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005901065887883306, \"iteration\": 1540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006512115942314267, \"iteration\": 1541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000534324673935771, \"iteration\": 1542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000590637675486505, \"iteration\": 1543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004330567317083478, \"iteration\": 1544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006627379916608334, \"iteration\": 1545, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004756456473842263, \"iteration\": 1546, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003251375164836645, \"iteration\": 1547, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02909722737967968, \"iteration\": 1548, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006630261777900159, \"iteration\": 1549, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007766716880723834, \"iteration\": 1550, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00042836516513489187, \"iteration\": 1551, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007739579887129366, \"iteration\": 1552, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017339756013825536, \"iteration\": 1553, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005181714659556746, \"iteration\": 1554, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00047421574709005654, \"iteration\": 1555, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007698124973103404, \"iteration\": 1556, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006310004973784089, \"iteration\": 1557, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008873767219483852, \"iteration\": 1558, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017683920450508595, \"iteration\": 1559, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009085788042284548, \"iteration\": 1560, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00041204996523447335, \"iteration\": 1561, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005814782343804836, \"iteration\": 1562, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00047513528261333704, \"iteration\": 1563, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008232602849602699, \"iteration\": 1564, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012700902298092842, \"iteration\": 1565, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00040340147097595036, \"iteration\": 1566, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003654113388620317, \"iteration\": 1567, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006959956954233348, \"iteration\": 1568, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003687195712700486, \"iteration\": 1569, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003676856867969036, \"iteration\": 1570, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06984510272741318, \"iteration\": 1571, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005279763718135655, \"iteration\": 1572, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005181835731491446, \"iteration\": 1573, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006481438176706433, \"iteration\": 1574, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00043546699453145266, \"iteration\": 1575, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002714814036153257, \"iteration\": 1576, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007985722622834146, \"iteration\": 1577, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003228082205168903, \"iteration\": 1578, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004966973210684955, \"iteration\": 1579, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006419071578420699, \"iteration\": 1580, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017113592475652695, \"iteration\": 1581, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003386829630471766, \"iteration\": 1582, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006108966190367937, \"iteration\": 1583, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006475228583440185, \"iteration\": 1584, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006106806686148047, \"iteration\": 1585, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005886994767934084, \"iteration\": 1586, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005383624229580164, \"iteration\": 1587, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00033892985084094107, \"iteration\": 1588, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005383662646636367, \"iteration\": 1589, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017144490266218781, \"iteration\": 1590, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005682624760083854, \"iteration\": 1591, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005411143065430224, \"iteration\": 1592, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00044218223774805665, \"iteration\": 1593, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004397421143949032, \"iteration\": 1594, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004919655621051788, \"iteration\": 1595, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006557506858371198, \"iteration\": 1596, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00038680926081724465, \"iteration\": 1597, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.014721383340656757, \"iteration\": 1598, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005302377976477146, \"iteration\": 1599, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005454958882182837, \"iteration\": 1600, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004445353406481445, \"iteration\": 1601, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008095469675026834, \"iteration\": 1602, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002618336584419012, \"iteration\": 1603, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005367586272768676, \"iteration\": 1604, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00043266406282782555, \"iteration\": 1605, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003489104565232992, \"iteration\": 1606, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026184070855379105, \"iteration\": 1607, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004013500874862075, \"iteration\": 1608, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009444005554541945, \"iteration\": 1609, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005332056898623705, \"iteration\": 1610, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00024788861628621817, \"iteration\": 1611, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003247913846280426, \"iteration\": 1612, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007446113741025329, \"iteration\": 1613, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005186940543353558, \"iteration\": 1614, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006323258858174086, \"iteration\": 1615, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00040751066990196705, \"iteration\": 1616, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004052795411553234, \"iteration\": 1617, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000693188514560461, \"iteration\": 1618, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003184489323757589, \"iteration\": 1619, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009030688088387251, \"iteration\": 1620, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007584976265206933, \"iteration\": 1621, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002906565787270665, \"iteration\": 1622, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004667463945224881, \"iteration\": 1623, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004962091334164143, \"iteration\": 1624, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003534238785505295, \"iteration\": 1625, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00031192839378491044, \"iteration\": 1626, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004318531136959791, \"iteration\": 1627, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005943894502706826, \"iteration\": 1628, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003913613618351519, \"iteration\": 1629, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002427112078294158, \"iteration\": 1630, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00036109506618231535, \"iteration\": 1631, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004125605628360063, \"iteration\": 1632, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004301572625990957, \"iteration\": 1633, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009122699848376215, \"iteration\": 1634, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006431896472349763, \"iteration\": 1635, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00039128900971263647, \"iteration\": 1636, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00037577541661448777, \"iteration\": 1637, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00043462312896735966, \"iteration\": 1638, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000613118929322809, \"iteration\": 1639, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005869626183994114, \"iteration\": 1640, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00036950927460566163, \"iteration\": 1641, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009256512275896966, \"iteration\": 1642, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000594532466493547, \"iteration\": 1643, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006366711459122598, \"iteration\": 1644, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005160756409168243, \"iteration\": 1645, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00038133421912789345, \"iteration\": 1646, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00036779697984457016, \"iteration\": 1647, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003895965637639165, \"iteration\": 1648, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00036400670069269836, \"iteration\": 1649, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006285944255068898, \"iteration\": 1650, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003375403175596148, \"iteration\": 1651, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0002566578914411366, \"iteration\": 1652, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004534870386123657, \"iteration\": 1653, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00033760687801986933, \"iteration\": 1654, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003135965089313686, \"iteration\": 1655, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003823706356342882, \"iteration\": 1656, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004310992662794888, \"iteration\": 1657, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005711024859920144, \"iteration\": 1658, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00042737339390441775, \"iteration\": 1659, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001469979528337717, \"iteration\": 1660, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00044793635606765747, \"iteration\": 1661, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003619496419560164, \"iteration\": 1662, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003563371719792485, \"iteration\": 1663, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005546058528125286, \"iteration\": 1664, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003247760469093919, \"iteration\": 1665, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004514781176112592, \"iteration\": 1666, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005554627859964967, \"iteration\": 1667, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004787747282534838, \"iteration\": 1668, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006987589877098799, \"iteration\": 1669, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003948811790905893, \"iteration\": 1670, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00020729293464682996, \"iteration\": 1671, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 3.190397546859458e-05, \"iteration\": 1672, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00024665374075993896, \"iteration\": 1673, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002356903423788026, \"iteration\": 1674, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008080166298896074, \"iteration\": 1675, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006356564117595553, \"iteration\": 1676, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003307134029455483, \"iteration\": 1677, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006475566187873483, \"iteration\": 1678, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00027688260888680816, \"iteration\": 1679, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005130754434503615, \"iteration\": 1680, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004947552224621177, \"iteration\": 1681, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003794750082306564, \"iteration\": 1682, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002560949942562729, \"iteration\": 1683, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002746731915976852, \"iteration\": 1684, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003136894665658474, \"iteration\": 1685, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00026219215942546725, \"iteration\": 1686, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003516263677738607, \"iteration\": 1687, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003616108442656696, \"iteration\": 1688, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002928213507402688, \"iteration\": 1689, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008067095186561346, \"iteration\": 1690, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00029258430004119873, \"iteration\": 1691, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023434309696312994, \"iteration\": 1692, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000551614270079881, \"iteration\": 1693, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00041367358062416315, \"iteration\": 1694, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00034708890598267317, \"iteration\": 1695, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006524931523017585, \"iteration\": 1696, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002573283272795379, \"iteration\": 1697, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00026188389165326953, \"iteration\": 1698, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003788048052228987, \"iteration\": 1699, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023446828708983958, \"iteration\": 1700, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00032976194052025676, \"iteration\": 1701, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009717411594465375, \"iteration\": 1702, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00027185890940018, \"iteration\": 1703, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00015601905761286616, \"iteration\": 1704, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023735083232168108, \"iteration\": 1705, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00025377512793056667, \"iteration\": 1706, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003478288126643747, \"iteration\": 1707, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007298488635569811, \"iteration\": 1708, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00014792072761338204, \"iteration\": 1709, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002410321612842381, \"iteration\": 1710, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002342588995816186, \"iteration\": 1711, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002451876935083419, \"iteration\": 1712, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023370613052975386, \"iteration\": 1713, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00019953929586336017, \"iteration\": 1714, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00031145691173151135, \"iteration\": 1715, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00037118952604942024, \"iteration\": 1716, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003343131975270808, \"iteration\": 1717, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00022836186690256, \"iteration\": 1718, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00022962370712775737, \"iteration\": 1719, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010197320953011513, \"iteration\": 1720, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00027040860732086003, \"iteration\": 1721, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00027410784969106317, \"iteration\": 1722, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00046666181879118085, \"iteration\": 1723, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009749367018230259, \"iteration\": 1724, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023228288046084344, \"iteration\": 1725, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00017489527817815542, \"iteration\": 1726, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002857359650079161, \"iteration\": 1727, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00025486230151727796, \"iteration\": 1728, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002666923974175006, \"iteration\": 1729, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.011819776147603989, \"iteration\": 1730, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002018908126046881, \"iteration\": 1731, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011436515487730503, \"iteration\": 1732, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003644611279014498, \"iteration\": 1733, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004755545814987272, \"iteration\": 1734, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0033074025996029377, \"iteration\": 1735, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00027623886126093566, \"iteration\": 1736, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00019108100968878716, \"iteration\": 1737, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002860858803614974, \"iteration\": 1738, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00021221983479335904, \"iteration\": 1739, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00034241966204717755, \"iteration\": 1740, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00034140615025535226, \"iteration\": 1741, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00022877409355714917, \"iteration\": 1742, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004983959370292723, \"iteration\": 1743, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0110706752166152, \"iteration\": 1744, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00021806139557156712, \"iteration\": 1745, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00017835349717643112, \"iteration\": 1746, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00019738011178560555, \"iteration\": 1747, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003082791226916015, \"iteration\": 1748, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03161200135946274, \"iteration\": 1749, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00030791142489761114, \"iteration\": 1750, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003153030411340296, \"iteration\": 1751, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002508434699848294, \"iteration\": 1752, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002599947911221534, \"iteration\": 1753, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004849800025112927, \"iteration\": 1754, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004048833216074854, \"iteration\": 1755, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003703611728269607, \"iteration\": 1756, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002609965158626437, \"iteration\": 1757, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003334955545142293, \"iteration\": 1758, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004677346732933074, \"iteration\": 1759, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002140389260603115, \"iteration\": 1760, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002799943904392421, \"iteration\": 1761, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00017980972188524902, \"iteration\": 1762, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00022527185501530766, \"iteration\": 1763, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00040822476148605347, \"iteration\": 1764, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00028496322920545936, \"iteration\": 1765, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002035726938629523, \"iteration\": 1766, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002735536836553365, \"iteration\": 1767, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003534446004778147, \"iteration\": 1768, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00017270850366912782, \"iteration\": 1769, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00020374369341880083, \"iteration\": 1770, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02081766352057457, \"iteration\": 1771, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001997751387534663, \"iteration\": 1772, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001538382493890822, \"iteration\": 1773, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00029579459805972874, \"iteration\": 1774, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00026730322861112654, \"iteration\": 1775, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00030974208493717015, \"iteration\": 1776, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003028421779163182, \"iteration\": 1777, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00026374205481261015, \"iteration\": 1778, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00019931566203013062, \"iteration\": 1779, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00018797823577187955, \"iteration\": 1780, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003234767937101424, \"iteration\": 1781, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023151382629293948, \"iteration\": 1782, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00017525114526506513, \"iteration\": 1783, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002513037179596722, \"iteration\": 1784, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001496411714470014, \"iteration\": 1785, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00019760499708354473, \"iteration\": 1786, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00019165617413818836, \"iteration\": 1787, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0038383156061172485, \"iteration\": 1788, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00031480565667152405, \"iteration\": 1789, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002926248707808554, \"iteration\": 1790, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00022247711603995413, \"iteration\": 1791, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00030353787587955594, \"iteration\": 1792, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00022115818865131587, \"iteration\": 1793, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00027125427732244134, \"iteration\": 1794, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000200351991225034, \"iteration\": 1795, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002484270662534982, \"iteration\": 1796, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023460215015802532, \"iteration\": 1797, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00029329475364647806, \"iteration\": 1798, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00021576357539743185, \"iteration\": 1799, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00022899996838532388, \"iteration\": 1800, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00022397044813260436, \"iteration\": 1801, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00017327722162008286, \"iteration\": 1802, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002456831280142069, \"iteration\": 1803, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007140853558667004, \"iteration\": 1804, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002944397274404764, \"iteration\": 1805, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004667971807066351, \"iteration\": 1806, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001918253838084638, \"iteration\": 1807, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002379491925239563, \"iteration\": 1808, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002327769761905074, \"iteration\": 1809, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002852166653610766, \"iteration\": 1810, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003380349953658879, \"iteration\": 1811, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002110814821207896, \"iteration\": 1812, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002571408695075661, \"iteration\": 1813, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00020260132441762835, \"iteration\": 1814, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009435475221835077, \"iteration\": 1815, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023212328960653394, \"iteration\": 1816, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00031948642572388053, \"iteration\": 1817, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00033595753484405577, \"iteration\": 1818, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00024129376106429845, \"iteration\": 1819, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00025574996834620833, \"iteration\": 1820, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002309177361894399, \"iteration\": 1821, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002712159475777298, \"iteration\": 1822, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023677317949477583, \"iteration\": 1823, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00015399190306197852, \"iteration\": 1824, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008961428538896143, \"iteration\": 1825, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002148378116544336, \"iteration\": 1826, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00033000961411744356, \"iteration\": 1827, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00024464179296046495, \"iteration\": 1828, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000259769061813131, \"iteration\": 1829, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002143769816029817, \"iteration\": 1830, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00024228285474237055, \"iteration\": 1831, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009709214209578931, \"iteration\": 1832, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00022312966757453978, \"iteration\": 1833, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00016330201469827443, \"iteration\": 1834, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001906713005155325, \"iteration\": 1835, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00025711479247547686, \"iteration\": 1836, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003110494581051171, \"iteration\": 1837, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006141579942777753, \"iteration\": 1838, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002759295457508415, \"iteration\": 1839, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00017629914509598166, \"iteration\": 1840, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003053824766539037, \"iteration\": 1841, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002802083035930991, \"iteration\": 1842, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001948326244018972, \"iteration\": 1843, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0001929570862557739, \"iteration\": 1844, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00017993187066167593, \"iteration\": 1845, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00031830259831622243, \"iteration\": 1846, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004107701242901385, \"iteration\": 1847, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023982256243471056, \"iteration\": 1848, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023559413966722786, \"iteration\": 1849, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011759731569327414, \"iteration\": 1850, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00013321339793037623, \"iteration\": 1851, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.01081775687634945, \"iteration\": 1852, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002301581553183496, \"iteration\": 1853, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006517033907584846, \"iteration\": 1854, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00019007989612873644, \"iteration\": 1855, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00022076262393966317, \"iteration\": 1856, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023520334798377007, \"iteration\": 1857, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00019510096171870828, \"iteration\": 1858, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00011429490405134857, \"iteration\": 1859, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00013947846309747547, \"iteration\": 1860, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00030180136673152447, \"iteration\": 1861, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003231256559956819, \"iteration\": 1862, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00016661551489960402, \"iteration\": 1863, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00016504718223586679, \"iteration\": 1864, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00036333728348836303, \"iteration\": 1865, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003175698802806437, \"iteration\": 1866, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00046597886830568314, \"iteration\": 1867, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005671127582900226, \"iteration\": 1868, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002254435676150024, \"iteration\": 1869, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00028453610138967633, \"iteration\": 1870, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004799895395990461, \"iteration\": 1871, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00048080203123390675, \"iteration\": 1872, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002762145595625043, \"iteration\": 1873, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003196817706339061, \"iteration\": 1874, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00014692498371005058, \"iteration\": 1875, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00021066382760182023, \"iteration\": 1876, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00015709498256910592, \"iteration\": 1877, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002247723168693483, \"iteration\": 1878, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00018589071987662464, \"iteration\": 1879, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002339543279958889, \"iteration\": 1880, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003569223918020725, \"iteration\": 1881, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003677888016682118, \"iteration\": 1882, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016584619879722595, \"iteration\": 1883, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001468863629270345, \"iteration\": 1884, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017872985335998237, \"iteration\": 1885, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024197773018386215, \"iteration\": 1886, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003444347530603409, \"iteration\": 1887, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00011147947225254029, \"iteration\": 1888, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021289724099915475, \"iteration\": 1889, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00012717951904051006, \"iteration\": 1890, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00011527803144417703, \"iteration\": 1891, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002826388517860323, \"iteration\": 1892, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016505751409567893, \"iteration\": 1893, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016325907199643552, \"iteration\": 1894, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 8.1103433331009e-05, \"iteration\": 1895, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004958861391060054, \"iteration\": 1896, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014001279487274587, \"iteration\": 1897, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022174755576997995, \"iteration\": 1898, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001445887319277972, \"iteration\": 1899, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014830658619757742, \"iteration\": 1900, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00025684971478767693, \"iteration\": 1901, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.037725623697042465, \"iteration\": 1902, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015883696323726326, \"iteration\": 1903, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03448951989412308, \"iteration\": 1904, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014505034778267145, \"iteration\": 1905, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019791489467024803, \"iteration\": 1906, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003462678869254887, \"iteration\": 1907, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00038827923708595335, \"iteration\": 1908, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001575177739141509, \"iteration\": 1909, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002513091021683067, \"iteration\": 1910, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002069607435259968, \"iteration\": 1911, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003316280781291425, \"iteration\": 1912, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00028232517070136964, \"iteration\": 1913, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009028229978866875, \"iteration\": 1914, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005519576370716095, \"iteration\": 1915, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005240648752078414, \"iteration\": 1916, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00025063357315957546, \"iteration\": 1917, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003306043508928269, \"iteration\": 1918, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000353001756593585, \"iteration\": 1919, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023189136118162423, \"iteration\": 1920, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027115154080092907, \"iteration\": 1921, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002998069394379854, \"iteration\": 1922, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00011452380567789078, \"iteration\": 1923, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000360974227078259, \"iteration\": 1924, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014209086657501757, \"iteration\": 1925, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00028325675521045923, \"iteration\": 1926, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019188599253538996, \"iteration\": 1927, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001477369078202173, \"iteration\": 1928, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00026640930445864797, \"iteration\": 1929, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021697681222576648, \"iteration\": 1930, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00030698554473929107, \"iteration\": 1931, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001479842176195234, \"iteration\": 1932, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005610856460407376, \"iteration\": 1933, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00010617492080200464, \"iteration\": 1934, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014206705964170396, \"iteration\": 1935, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013559800572693348, \"iteration\": 1936, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027308944845572114, \"iteration\": 1937, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017155524983536452, \"iteration\": 1938, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001880082709249109, \"iteration\": 1939, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021860975539311767, \"iteration\": 1940, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00012582054478116333, \"iteration\": 1941, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022389039804693311, \"iteration\": 1942, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022320383868645877, \"iteration\": 1943, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017367771943099797, \"iteration\": 1944, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002360207581659779, \"iteration\": 1945, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001551502791699022, \"iteration\": 1946, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001895696623250842, \"iteration\": 1947, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019573878671508282, \"iteration\": 1948, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019071307906415313, \"iteration\": 1949, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016875173605512828, \"iteration\": 1950, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002450289612170309, \"iteration\": 1951, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00012022825103485957, \"iteration\": 1952, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018579309107735753, \"iteration\": 1953, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018090091180056334, \"iteration\": 1954, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018387888849247247, \"iteration\": 1955, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007210053154267371, \"iteration\": 1956, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 8.532314677722752e-05, \"iteration\": 1957, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017140284762717783, \"iteration\": 1958, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.006474301218986511, \"iteration\": 1959, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001741989399306476, \"iteration\": 1960, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023217685520648956, \"iteration\": 1961, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015358779637608677, \"iteration\": 1962, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021262405789457262, \"iteration\": 1963, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015144801000133157, \"iteration\": 1964, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008968312758952379, \"iteration\": 1965, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004054203163832426, \"iteration\": 1966, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001472826552344486, \"iteration\": 1967, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00038042975938878953, \"iteration\": 1968, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001798320299712941, \"iteration\": 1969, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021587798255495727, \"iteration\": 1970, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023460242664441466, \"iteration\": 1971, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019777145644184202, \"iteration\": 1972, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016840054013300687, \"iteration\": 1973, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001553489564685151, \"iteration\": 1974, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002059826219920069, \"iteration\": 1975, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020629423670470715, \"iteration\": 1976, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004219540278427303, \"iteration\": 1977, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018626311793923378, \"iteration\": 1978, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016785903426352888, \"iteration\": 1979, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00010049260163214058, \"iteration\": 1980, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024324974219780415, \"iteration\": 1981, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00025876887957565486, \"iteration\": 1982, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001955896441359073, \"iteration\": 1983, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006384690059348941, \"iteration\": 1984, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001081357360817492, \"iteration\": 1985, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002458132803440094, \"iteration\": 1986, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002729293191805482, \"iteration\": 1987, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000143111014040187, \"iteration\": 1988, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002979495329782367, \"iteration\": 1989, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013961634249426425, \"iteration\": 1990, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016104747192002833, \"iteration\": 1991, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017863688117358834, \"iteration\": 1992, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020210215006954968, \"iteration\": 1993, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004856614687014371, \"iteration\": 1994, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018080523295793682, \"iteration\": 1995, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000264222762780264, \"iteration\": 1996, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002976962423417717, \"iteration\": 1997, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014960231783334166, \"iteration\": 1998, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00012865291500929743, \"iteration\": 1999, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005722800851799548, \"iteration\": 2000, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018717428611125797, \"iteration\": 2001, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002129895583493635, \"iteration\": 2002, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016103583038784564, \"iteration\": 2003, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020466586283873767, \"iteration\": 2004, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00010443090286571532, \"iteration\": 2005, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016166534624062479, \"iteration\": 2006, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016203970881178975, \"iteration\": 2007, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 9.126538498094305e-05, \"iteration\": 2008, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010004182113334537, \"iteration\": 2009, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013060623314231634, \"iteration\": 2010, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 8.788579725660384e-05, \"iteration\": 2011, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020420814689714462, \"iteration\": 2012, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020699985907413065, \"iteration\": 2013, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020885227422695607, \"iteration\": 2014, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002805400872603059, \"iteration\": 2015, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.018784737214446068, \"iteration\": 2016, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001805346691980958, \"iteration\": 2017, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00012409289774950594, \"iteration\": 2018, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014463411935139447, \"iteration\": 2019, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018378363165538758, \"iteration\": 2020, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013724986638408154, \"iteration\": 2021, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014068697055336088, \"iteration\": 2022, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001548713626107201, \"iteration\": 2023, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015257114137057215, \"iteration\": 2024, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014894874766469002, \"iteration\": 2025, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014545803423970938, \"iteration\": 2026, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017750528058968484, \"iteration\": 2027, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001692355435807258, \"iteration\": 2028, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020126113668084145, \"iteration\": 2029, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017411298176739365, \"iteration\": 2030, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013321028382051736, \"iteration\": 2031, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007339039584621787, \"iteration\": 2032, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014266632206272334, \"iteration\": 2033, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001907467667479068, \"iteration\": 2034, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001084821778931655, \"iteration\": 2035, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015917670680209994, \"iteration\": 2036, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015463335148524493, \"iteration\": 2037, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001360033784294501, \"iteration\": 2038, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019674879149533808, \"iteration\": 2039, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001796727883629501, \"iteration\": 2040, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017651963571552187, \"iteration\": 2041, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00010484701488167048, \"iteration\": 2042, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000614024989772588, \"iteration\": 2043, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017312882118858397, \"iteration\": 2044, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017212574311997741, \"iteration\": 2045, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013409381499513984, \"iteration\": 2046, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023713796690572053, \"iteration\": 2047, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013045080413576216, \"iteration\": 2048, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00011379236093489453, \"iteration\": 2049, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018295492918696254, \"iteration\": 2050, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002971603302285075, \"iteration\": 2051, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001391335972584784, \"iteration\": 2052, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014924575225450099, \"iteration\": 2053, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00011147478653583676, \"iteration\": 2054, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001390505931340158, \"iteration\": 2055, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 8.965149754658341e-05, \"iteration\": 2056, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002078253892250359, \"iteration\": 2057, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00011395729234209284, \"iteration\": 2058, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.41903786547482e-05, \"iteration\": 2059, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 8.288716344395652e-05, \"iteration\": 2060, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023302815679926425, \"iteration\": 2061, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 5.212946416577324e-05, \"iteration\": 2062, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013079152267891914, \"iteration\": 2063, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00011460106907179579, \"iteration\": 2064, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 9.724673873279244e-05, \"iteration\": 2065, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004905444220639765, \"iteration\": 2066, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013272062642499804, \"iteration\": 2067, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014005086268298328, \"iteration\": 2068, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00010906048555625603, \"iteration\": 2069, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001607285812497139, \"iteration\": 2070, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014896896027494222, \"iteration\": 2071, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 7.881482451921329e-05, \"iteration\": 2072, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 9.74252907326445e-05, \"iteration\": 2073, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015465161413885653, \"iteration\": 2074, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001815881987567991, \"iteration\": 2075, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00010169569577556103, \"iteration\": 2076, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002136103721568361, \"iteration\": 2077, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001107327698264271, \"iteration\": 2078, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014454226766247302, \"iteration\": 2079, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019882047490682453, \"iteration\": 2080, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 8.154800889315084e-05, \"iteration\": 2081, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014324199582915753, \"iteration\": 2082, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015823665307834744, \"iteration\": 2083, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013097778719384223, \"iteration\": 2084, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000161282965564169, \"iteration\": 2085, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013314351963344961, \"iteration\": 2086, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001695785904303193, \"iteration\": 2087, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 6.930885137990117e-05, \"iteration\": 2088, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000915678683668375, \"iteration\": 2089, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 4.592975528794341e-05, \"iteration\": 2090, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"Prepare data...\")\n",
    "train_nn_words = encode_data(train_raw, words_encoder)\n",
    "test_nn_words = encode_data(test_raw, words_encoder)\n",
    "\n",
    "logger.info(\"Train model...\")\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_nn_words, batch_size=128, shuffle=True)\n",
    "\n",
    "USE_CACHE = True\n",
    "\n",
    "model_nn_words = NeuralNetwork(\n",
    "    input_size=len(words_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn_words.pt').exists() and USE_CACHE:\n",
    "    model_nn_words = load_model(model_nn_words, models_dir, 'model_nn_words')\n",
    "else:\n",
    "    model_nn_words.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn_words, models_dir, \"model_nn_words\")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    X_test_nn = torch.stack([test[0] for test in test_nn_words]).cpu()\n",
    "    y_test_nn = torch.stack([test[1] for test in test_nn_words]).cpu()\n",
    "    y_pred_nn_words = model_nn_words.predict(X_test_nn)\n",
    "    logits_nn_words = model_nn_words.forward(X_test_nn)\n",
    "\n",
    "result_nn_words = evaluate(y_test_nn.cpu(), y_pred_nn_words.cpu(), logits_nn_words.cpu())\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(\"model_nn_words - Training accuracy & Loss\", model_nn_words, train_config, dataloader)\n",
    "\n",
    "# Move model to CPU to save CUDA memory\n",
    "model_nn_words.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 17:48:22,685:experiment_gb:INFO:Prepare data...\n",
      "2024-09-27 17:48:47,016:experiment_gb:INFO:Train model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7513, Precision: 0.7564, Recall: 0.8182, F1: 0.7861, AUC: 0.8244\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-2a0e6bfe951e45baa0d20eab811760b4.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-2a0e6bfe951e45baa0d20eab811760b4.vega-embed details,\n",
       "  #altair-viz-2a0e6bfe951e45baa0d20eab811760b4.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-2a0e6bfe951e45baa0d20eab811760b4\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2a0e6bfe951e45baa0d20eab811760b4\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2a0e6bfe951e45baa0d20eab811760b4\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-45317fd3e80493b2ea8be41f99f293f8\"}, \"title\": \"model_nn_chars - Training accuracy & Loss\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-45317fd3e80493b2ea8be41f99f293f8\": [{\"training_acc\": 0.5, \"training_loss\": 0.6961812376976013, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.8671875, \"training_loss\": 0.6930201053619385, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6930077075958252, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6900905966758728, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6872428059577942, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 0.6905770301818848, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6769598722457886, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.677554726600647, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6793346405029297, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6743869781494141, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6813269257545471, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6534443497657776, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6522922515869141, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6874235272407532, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6571434736251831, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 0.7081213593482971, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 0.7200679183006287, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.648392915725708, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6623501777648926, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 0.6915329098701477, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6632277965545654, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.668806254863739, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6526607275009155, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6470874547958374, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6621013283729553, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6464896202087402, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6490527391433716, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.644334614276886, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.620537519454956, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.658938467502594, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6390812993049622, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 0.7172667980194092, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6149762272834778, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6507965922355652, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.629538357257843, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.6238452792167664, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.6262025237083435, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.6301038265228271, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.6199737191200256, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.593498170375824, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.6004843711853027, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6075880527496338, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.6012343168258667, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.5843091011047363, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5808798670768738, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.609082818031311, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5960626006126404, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5943878889083862, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5653740763664246, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5988487005233765, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5598776340484619, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5534535646438599, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5508389472961426, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5267412662506104, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5627397298812866, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5585622787475586, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5267720222473145, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5145691633224487, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6020851731300354, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5411990284919739, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5999152064323425, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4756639003753662, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5281097292900085, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5244662165641785, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4858317971229553, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5442781448364258, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5358529090881348, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4597422182559967, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5193330645561218, \"iteration\": 69, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.53783118724823, \"iteration\": 70, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5765424966812134, \"iteration\": 71, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4604342579841614, \"iteration\": 72, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.495810866355896, \"iteration\": 73, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5103093385696411, \"iteration\": 74, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4514143168926239, \"iteration\": 75, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5489311218261719, \"iteration\": 76, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5393179655075073, \"iteration\": 77, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5143961310386658, \"iteration\": 78, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5201785564422607, \"iteration\": 79, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5145000219345093, \"iteration\": 80, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.45963001251220703, \"iteration\": 81, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.44309499859809875, \"iteration\": 82, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.49182409048080444, \"iteration\": 83, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.463310569524765, \"iteration\": 84, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4650087356567383, \"iteration\": 85, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4567544460296631, \"iteration\": 86, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.42785346508026123, \"iteration\": 87, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.45618781447410583, \"iteration\": 88, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6388399600982666, \"iteration\": 89, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4509596526622772, \"iteration\": 90, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.47548770904541016, \"iteration\": 91, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5204578638076782, \"iteration\": 92, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4909267723560333, \"iteration\": 93, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5363894104957581, \"iteration\": 94, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5591013431549072, \"iteration\": 95, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4904479384422302, \"iteration\": 96, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4384775161743164, \"iteration\": 97, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.47967973351478577, \"iteration\": 98, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.4447094202041626, \"iteration\": 99, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.490161657333374, \"iteration\": 100, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.41356539726257324, \"iteration\": 101, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5262219309806824, \"iteration\": 102, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4642469882965088, \"iteration\": 103, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5043266415596008, \"iteration\": 104, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5064520835876465, \"iteration\": 105, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.483938992023468, \"iteration\": 106, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.4522203207015991, \"iteration\": 107, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.46777087450027466, \"iteration\": 108, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.46433302760124207, \"iteration\": 109, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.44309550523757935, \"iteration\": 110, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.44881701469421387, \"iteration\": 111, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.41397225856781006, \"iteration\": 112, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.43455857038497925, \"iteration\": 113, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.47827011346817017, \"iteration\": 114, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.4885159134864807, \"iteration\": 115, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.46341949701309204, \"iteration\": 116, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.45075395703315735, \"iteration\": 117, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.5035479664802551, \"iteration\": 118, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.45704302191734314, \"iteration\": 119, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4258769154548645, \"iteration\": 120, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.4986434578895569, \"iteration\": 121, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5138183832168579, \"iteration\": 122, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.42123040556907654, \"iteration\": 123, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.525091826915741, \"iteration\": 124, \"epoch\": 1}, {\"training_acc\": 0.859375, \"training_loss\": 0.42278409004211426, \"iteration\": 125, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4990900158882141, \"iteration\": 126, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.48597419261932373, \"iteration\": 127, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6392862796783447, \"iteration\": 128, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4353393316268921, \"iteration\": 129, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.41903555393218994, \"iteration\": 130, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.44414883852005005, \"iteration\": 131, \"epoch\": 1}, {\"training_acc\": 0.875, \"training_loss\": 0.4253600835800171, \"iteration\": 132, \"epoch\": 1}, {\"training_acc\": 0.875, \"training_loss\": 0.38464292883872986, \"iteration\": 133, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.43978607654571533, \"iteration\": 134, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.5069052577018738, \"iteration\": 135, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4270266890525818, \"iteration\": 136, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.4921618103981018, \"iteration\": 137, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.416412353515625, \"iteration\": 138, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.48140135407447815, \"iteration\": 139, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.44296735525131226, \"iteration\": 140, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4517984688282013, \"iteration\": 141, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4821125864982605, \"iteration\": 142, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4622165262699127, \"iteration\": 143, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.43599438667297363, \"iteration\": 144, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.37114420533180237, \"iteration\": 145, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4130648970603943, \"iteration\": 146, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4685138761997223, \"iteration\": 147, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5113130211830139, \"iteration\": 148, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.39915457367897034, \"iteration\": 149, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.4408743381500244, \"iteration\": 150, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.48181039094924927, \"iteration\": 151, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.400602787733078, \"iteration\": 152, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5027472376823425, \"iteration\": 153, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4061719477176666, \"iteration\": 154, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4335617423057556, \"iteration\": 155, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.40710896253585815, \"iteration\": 156, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.38961344957351685, \"iteration\": 157, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4391385614871979, \"iteration\": 158, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.39171284437179565, \"iteration\": 159, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.44228464365005493, \"iteration\": 160, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.4110727906227112, \"iteration\": 161, \"epoch\": 1}, {\"training_acc\": 0.859375, \"training_loss\": 0.3735111653804779, \"iteration\": 162, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4456741511821747, \"iteration\": 163, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.41141098737716675, \"iteration\": 164, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.46256157755851746, \"iteration\": 165, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4708441197872162, \"iteration\": 166, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.46818387508392334, \"iteration\": 167, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.4613027572631836, \"iteration\": 168, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.46846961975097656, \"iteration\": 169, \"epoch\": 1}, {\"training_acc\": 0.859375, \"training_loss\": 0.4283860921859741, \"iteration\": 170, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4049084484577179, \"iteration\": 171, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.45656320452690125, \"iteration\": 172, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.3849152624607086, \"iteration\": 173, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4524301290512085, \"iteration\": 174, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.48534825444221497, \"iteration\": 175, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5720238089561462, \"iteration\": 176, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4473639726638794, \"iteration\": 177, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.372512549161911, \"iteration\": 178, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.40452027320861816, \"iteration\": 179, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5265452861785889, \"iteration\": 180, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.46582067012786865, \"iteration\": 181, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4420730173587799, \"iteration\": 182, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4303757846355438, \"iteration\": 183, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.47803017497062683, \"iteration\": 184, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.49731603264808655, \"iteration\": 185, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.4125988781452179, \"iteration\": 186, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.40965795516967773, \"iteration\": 187, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4998604655265808, \"iteration\": 188, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.44921746850013733, \"iteration\": 189, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.571090579032898, \"iteration\": 190, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.43285006284713745, \"iteration\": 191, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.49239635467529297, \"iteration\": 192, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4742357134819031, \"iteration\": 193, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.41254574060440063, \"iteration\": 194, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.40001389384269714, \"iteration\": 195, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4588688313961029, \"iteration\": 196, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4384377598762512, \"iteration\": 197, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.46140971779823303, \"iteration\": 198, \"epoch\": 1}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3448074460029602, \"iteration\": 199, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.44696199893951416, \"iteration\": 200, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4496525824069977, \"iteration\": 201, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4080440402030945, \"iteration\": 202, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4085715115070343, \"iteration\": 203, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.4633469581604004, \"iteration\": 204, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.40033066272735596, \"iteration\": 205, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5189353227615356, \"iteration\": 206, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4344486594200134, \"iteration\": 207, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5035257935523987, \"iteration\": 208, \"epoch\": 1}, {\"training_acc\": 1.0, \"training_loss\": 0.2620031535625458, \"iteration\": 209, \"epoch\": 1}, {\"training_acc\": 0.9453125, \"training_loss\": 0.30686038732528687, \"iteration\": 210, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.360210120677948, \"iteration\": 211, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.38241472840309143, \"iteration\": 212, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.33158671855926514, \"iteration\": 213, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2912019193172455, \"iteration\": 214, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2867199182510376, \"iteration\": 215, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.31989216804504395, \"iteration\": 216, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3454645276069641, \"iteration\": 217, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.34683772921562195, \"iteration\": 218, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.29025018215179443, \"iteration\": 219, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3649730384349823, \"iteration\": 220, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3328293263912201, \"iteration\": 221, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.24357345700263977, \"iteration\": 222, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3047720789909363, \"iteration\": 223, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.37335941195487976, \"iteration\": 224, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3697606921195984, \"iteration\": 225, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.33210790157318115, \"iteration\": 226, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.28960099816322327, \"iteration\": 227, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.32176458835601807, \"iteration\": 228, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3467731475830078, \"iteration\": 229, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3291381299495697, \"iteration\": 230, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.3347892165184021, \"iteration\": 231, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.31717854738235474, \"iteration\": 232, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.36394262313842773, \"iteration\": 233, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3180771768093109, \"iteration\": 234, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.35795751214027405, \"iteration\": 235, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3591400682926178, \"iteration\": 236, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3577888607978821, \"iteration\": 237, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3355877101421356, \"iteration\": 238, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3194834291934967, \"iteration\": 239, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.33484211564064026, \"iteration\": 240, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.25437137484550476, \"iteration\": 241, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.37193185091018677, \"iteration\": 242, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3222495913505554, \"iteration\": 243, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.41040506958961487, \"iteration\": 244, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.26745349168777466, \"iteration\": 245, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.33196163177490234, \"iteration\": 246, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.31275200843811035, \"iteration\": 247, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3530800938606262, \"iteration\": 248, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.30856066942214966, \"iteration\": 249, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.27598005533218384, \"iteration\": 250, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.36115047335624695, \"iteration\": 251, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3267388939857483, \"iteration\": 252, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.36052748560905457, \"iteration\": 253, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.30400726199150085, \"iteration\": 254, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.33540841937065125, \"iteration\": 255, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.380135715007782, \"iteration\": 256, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4538743793964386, \"iteration\": 257, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.31879401206970215, \"iteration\": 258, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.28854236006736755, \"iteration\": 259, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.3975224494934082, \"iteration\": 260, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.29540959000587463, \"iteration\": 261, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.28755855560302734, \"iteration\": 262, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.36027759313583374, \"iteration\": 263, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.34314337372779846, \"iteration\": 264, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.21173951029777527, \"iteration\": 265, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.23754072189331055, \"iteration\": 266, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3699404001235962, \"iteration\": 267, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.37822797894477844, \"iteration\": 268, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.37822529673576355, \"iteration\": 269, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.35890400409698486, \"iteration\": 270, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.33257943391799927, \"iteration\": 271, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.2925938367843628, \"iteration\": 272, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.39721933007240295, \"iteration\": 273, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3835911154747009, \"iteration\": 274, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4038563072681427, \"iteration\": 275, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.38280099630355835, \"iteration\": 276, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.21785764396190643, \"iteration\": 277, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25383681058883667, \"iteration\": 278, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.29997333884239197, \"iteration\": 279, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3606121242046356, \"iteration\": 280, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.3958887457847595, \"iteration\": 281, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2992042005062103, \"iteration\": 282, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.28740620613098145, \"iteration\": 283, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.28500670194625854, \"iteration\": 284, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3620734214782715, \"iteration\": 285, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2876657247543335, \"iteration\": 286, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.38402214646339417, \"iteration\": 287, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.30548733472824097, \"iteration\": 288, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.44776371121406555, \"iteration\": 289, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.30976739525794983, \"iteration\": 290, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3728167712688446, \"iteration\": 291, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2930828630924225, \"iteration\": 292, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2635880410671234, \"iteration\": 293, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3006732761859894, \"iteration\": 294, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.37105512619018555, \"iteration\": 295, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2997199594974518, \"iteration\": 296, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3679344356060028, \"iteration\": 297, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4376007318496704, \"iteration\": 298, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.32998791337013245, \"iteration\": 299, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.367768257856369, \"iteration\": 300, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3718128204345703, \"iteration\": 301, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.391466349363327, \"iteration\": 302, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.349729984998703, \"iteration\": 303, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.29173097014427185, \"iteration\": 304, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.29745325446128845, \"iteration\": 305, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3302032947540283, \"iteration\": 306, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.37281227111816406, \"iteration\": 307, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.32399165630340576, \"iteration\": 308, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.3938676416873932, \"iteration\": 309, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.38535451889038086, \"iteration\": 310, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.34008699655532837, \"iteration\": 311, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4030624032020569, \"iteration\": 312, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.27685219049453735, \"iteration\": 313, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3401370048522949, \"iteration\": 314, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.3766944706439972, \"iteration\": 315, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3517531752586365, \"iteration\": 316, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4085403382778168, \"iteration\": 317, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3169127106666565, \"iteration\": 318, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.40293189883232117, \"iteration\": 319, \"epoch\": 2}, {\"training_acc\": 0.953125, \"training_loss\": 0.23625625669956207, \"iteration\": 320, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.397444486618042, \"iteration\": 321, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3494124114513397, \"iteration\": 322, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23522181808948517, \"iteration\": 323, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.37249478697776794, \"iteration\": 324, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3288317322731018, \"iteration\": 325, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2804616689682007, \"iteration\": 326, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.444579541683197, \"iteration\": 327, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.26606497168540955, \"iteration\": 328, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3366348445415497, \"iteration\": 329, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3345658481121063, \"iteration\": 330, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.32975703477859497, \"iteration\": 331, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3636381924152374, \"iteration\": 332, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.25007814168930054, \"iteration\": 333, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4753751754760742, \"iteration\": 334, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.28278639912605286, \"iteration\": 335, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.31803521513938904, \"iteration\": 336, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.34319359064102173, \"iteration\": 337, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.39152801036834717, \"iteration\": 338, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3385985791683197, \"iteration\": 339, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.39310458302497864, \"iteration\": 340, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2749309837818146, \"iteration\": 341, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3822784423828125, \"iteration\": 342, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.32352885603904724, \"iteration\": 343, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.35876771807670593, \"iteration\": 344, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3150767683982849, \"iteration\": 345, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3796220123767853, \"iteration\": 346, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23449505865573883, \"iteration\": 347, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3707762360572815, \"iteration\": 348, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3435341417789459, \"iteration\": 349, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2981800138950348, \"iteration\": 350, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.2910899817943573, \"iteration\": 351, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.39125147461891174, \"iteration\": 352, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.34769800305366516, \"iteration\": 353, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3579249978065491, \"iteration\": 354, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4353691637516022, \"iteration\": 355, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.3977552056312561, \"iteration\": 356, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3429262042045593, \"iteration\": 357, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.34021472930908203, \"iteration\": 358, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.35630500316619873, \"iteration\": 359, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3746213912963867, \"iteration\": 360, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3358522653579712, \"iteration\": 361, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.40671849250793457, \"iteration\": 362, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.35789790749549866, \"iteration\": 363, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4446079432964325, \"iteration\": 364, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3486541509628296, \"iteration\": 365, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3725746273994446, \"iteration\": 366, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3667505979537964, \"iteration\": 367, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.368412047624588, \"iteration\": 368, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3493387997150421, \"iteration\": 369, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2617245018482208, \"iteration\": 370, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.37963488698005676, \"iteration\": 371, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3423725664615631, \"iteration\": 372, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.40418657660484314, \"iteration\": 373, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2958233952522278, \"iteration\": 374, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3035701513290405, \"iteration\": 375, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.34638142585754395, \"iteration\": 376, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3244175910949707, \"iteration\": 377, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3337908983230591, \"iteration\": 378, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3928071856498718, \"iteration\": 379, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.38692837953567505, \"iteration\": 380, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3303774297237396, \"iteration\": 381, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.40987735986709595, \"iteration\": 382, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.37594449520111084, \"iteration\": 383, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.29133129119873047, \"iteration\": 384, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4095930755138397, \"iteration\": 385, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3209519684314728, \"iteration\": 386, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2677242159843445, \"iteration\": 387, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3540135622024536, \"iteration\": 388, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.46698981523513794, \"iteration\": 389, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.36201202869415283, \"iteration\": 390, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.271889865398407, \"iteration\": 391, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2565208077430725, \"iteration\": 392, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.282061904668808, \"iteration\": 393, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.31617698073387146, \"iteration\": 394, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.29481542110443115, \"iteration\": 395, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.38189592957496643, \"iteration\": 396, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.410605251789093, \"iteration\": 397, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.48916077613830566, \"iteration\": 398, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2770746946334839, \"iteration\": 399, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.4127793610095978, \"iteration\": 400, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2969933748245239, \"iteration\": 401, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.26853036880493164, \"iteration\": 402, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4353167414665222, \"iteration\": 403, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30787962675094604, \"iteration\": 404, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2683619558811188, \"iteration\": 405, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.25742295384407043, \"iteration\": 406, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3917756676673889, \"iteration\": 407, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.44218096137046814, \"iteration\": 408, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.33493712544441223, \"iteration\": 409, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.32946842908859253, \"iteration\": 410, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.31958094239234924, \"iteration\": 411, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.38703104853630066, \"iteration\": 412, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.37580248713493347, \"iteration\": 413, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3805863857269287, \"iteration\": 414, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3332744240760803, \"iteration\": 415, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3479836881160736, \"iteration\": 416, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3617825508117676, \"iteration\": 417, \"epoch\": 2}, {\"training_acc\": 0.8571428571428571, \"training_loss\": 0.7654889822006226, \"iteration\": 418, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20142719149589539, \"iteration\": 419, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3085435628890991, \"iteration\": 420, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.24187946319580078, \"iteration\": 421, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2647233307361603, \"iteration\": 422, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.25445109605789185, \"iteration\": 423, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.1992124766111374, \"iteration\": 424, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2522907257080078, \"iteration\": 425, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2399868220090866, \"iteration\": 426, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22630903124809265, \"iteration\": 427, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.31351327896118164, \"iteration\": 428, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2526575028896332, \"iteration\": 429, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.23428207635879517, \"iteration\": 430, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2644250690937042, \"iteration\": 431, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.24600662291049957, \"iteration\": 432, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.20313739776611328, \"iteration\": 433, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2242119312286377, \"iteration\": 434, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.26881372928619385, \"iteration\": 435, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2517700493335724, \"iteration\": 436, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2372887134552002, \"iteration\": 437, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2539377808570862, \"iteration\": 438, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.27258193492889404, \"iteration\": 439, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.3322339355945587, \"iteration\": 440, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2673543393611908, \"iteration\": 441, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.23933516442775726, \"iteration\": 442, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2598448395729065, \"iteration\": 443, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2539442777633667, \"iteration\": 444, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2735409140586853, \"iteration\": 445, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.24766527116298676, \"iteration\": 446, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2689287066459656, \"iteration\": 447, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.23359134793281555, \"iteration\": 448, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.28989818692207336, \"iteration\": 449, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2650083899497986, \"iteration\": 450, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21881471574306488, \"iteration\": 451, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.2727915644645691, \"iteration\": 452, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.3494359850883484, \"iteration\": 453, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20186346769332886, \"iteration\": 454, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.24625830352306366, \"iteration\": 455, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.23972783982753754, \"iteration\": 456, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.31547069549560547, \"iteration\": 457, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.26232442259788513, \"iteration\": 458, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2998800277709961, \"iteration\": 459, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20496390759944916, \"iteration\": 460, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.24936997890472412, \"iteration\": 461, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2401106357574463, \"iteration\": 462, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1545417457818985, \"iteration\": 463, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2427287995815277, \"iteration\": 464, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20789584517478943, \"iteration\": 465, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.21940603852272034, \"iteration\": 466, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22759097814559937, \"iteration\": 467, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.22742928564548492, \"iteration\": 468, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2464863508939743, \"iteration\": 469, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.30627384781837463, \"iteration\": 470, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2874071002006531, \"iteration\": 471, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3232501745223999, \"iteration\": 472, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.19339580833911896, \"iteration\": 473, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2519764304161072, \"iteration\": 474, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2936285436153412, \"iteration\": 475, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3186514973640442, \"iteration\": 476, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23475664854049683, \"iteration\": 477, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18844501674175262, \"iteration\": 478, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17101924121379852, \"iteration\": 479, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.24716061353683472, \"iteration\": 480, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2659633457660675, \"iteration\": 481, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20661723613739014, \"iteration\": 482, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22339019179344177, \"iteration\": 483, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.26848000288009644, \"iteration\": 484, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.24026791751384735, \"iteration\": 485, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.25127458572387695, \"iteration\": 486, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.19940130412578583, \"iteration\": 487, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2842482626438141, \"iteration\": 488, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2599354386329651, \"iteration\": 489, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2244214415550232, \"iteration\": 490, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17648322880268097, \"iteration\": 491, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2282535582780838, \"iteration\": 492, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2772356867790222, \"iteration\": 493, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1624651700258255, \"iteration\": 494, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.16922956705093384, \"iteration\": 495, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.24239817261695862, \"iteration\": 496, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.24983909726142883, \"iteration\": 497, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.24954062700271606, \"iteration\": 498, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.260848730802536, \"iteration\": 499, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2580612599849701, \"iteration\": 500, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.251502126455307, \"iteration\": 501, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.26243627071380615, \"iteration\": 502, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2569137215614319, \"iteration\": 503, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2958364486694336, \"iteration\": 504, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.23159794509410858, \"iteration\": 505, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.22324039041996002, \"iteration\": 506, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2000928670167923, \"iteration\": 507, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22785551846027374, \"iteration\": 508, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.28435882925987244, \"iteration\": 509, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22328375279903412, \"iteration\": 510, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2171115279197693, \"iteration\": 511, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.36131203174591064, \"iteration\": 512, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.19525644183158875, \"iteration\": 513, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.22173388302326202, \"iteration\": 514, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.22944560647010803, \"iteration\": 515, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.23486757278442383, \"iteration\": 516, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.23734621703624725, \"iteration\": 517, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3274824619293213, \"iteration\": 518, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2051742672920227, \"iteration\": 519, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2444719672203064, \"iteration\": 520, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2438071072101593, \"iteration\": 521, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15160974860191345, \"iteration\": 522, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.29978689551353455, \"iteration\": 523, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2218630313873291, \"iteration\": 524, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2172587513923645, \"iteration\": 525, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.20408986508846283, \"iteration\": 526, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.3644770085811615, \"iteration\": 527, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.2737533748149872, \"iteration\": 528, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3468503952026367, \"iteration\": 529, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2058824896812439, \"iteration\": 530, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.21649694442749023, \"iteration\": 531, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20001891255378723, \"iteration\": 532, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.22811999917030334, \"iteration\": 533, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.240116149187088, \"iteration\": 534, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.21518076956272125, \"iteration\": 535, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.22795219719409943, \"iteration\": 536, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22886016964912415, \"iteration\": 537, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2652871608734131, \"iteration\": 538, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.17530135810375214, \"iteration\": 539, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.31983816623687744, \"iteration\": 540, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18703693151474, \"iteration\": 541, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 0.39354681968688965, \"iteration\": 542, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.35341596603393555, \"iteration\": 543, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.179703027009964, \"iteration\": 544, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2778351604938507, \"iteration\": 545, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.36564186215400696, \"iteration\": 546, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.3136633634567261, \"iteration\": 547, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3445776700973511, \"iteration\": 548, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2786093056201935, \"iteration\": 549, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21732790768146515, \"iteration\": 550, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23268435895442963, \"iteration\": 551, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.352693647146225, \"iteration\": 552, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.16789820790290833, \"iteration\": 553, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.30531686544418335, \"iteration\": 554, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.3033456802368164, \"iteration\": 555, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2067253589630127, \"iteration\": 556, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.18872961401939392, \"iteration\": 557, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.36196309328079224, \"iteration\": 558, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30535298585891724, \"iteration\": 559, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.34393730759620667, \"iteration\": 560, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.22529590129852295, \"iteration\": 561, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3387724459171295, \"iteration\": 562, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.40525105595588684, \"iteration\": 563, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.27248415350914, \"iteration\": 564, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.25270700454711914, \"iteration\": 565, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.35963302850723267, \"iteration\": 566, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.37037742137908936, \"iteration\": 567, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3171200752258301, \"iteration\": 568, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.27086517214775085, \"iteration\": 569, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3345530927181244, \"iteration\": 570, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3524237275123596, \"iteration\": 571, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.41226574778556824, \"iteration\": 572, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2086353898048401, \"iteration\": 573, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.29283255338668823, \"iteration\": 574, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.222699373960495, \"iteration\": 575, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 0.39514875411987305, \"iteration\": 576, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3353813886642456, \"iteration\": 577, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.24555350840091705, \"iteration\": 578, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.28626108169555664, \"iteration\": 579, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3103080093860626, \"iteration\": 580, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3074280917644501, \"iteration\": 581, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.31146979331970215, \"iteration\": 582, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.24129758775234222, \"iteration\": 583, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3831292390823364, \"iteration\": 584, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.23492299020290375, \"iteration\": 585, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.29778093099594116, \"iteration\": 586, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2900439500808716, \"iteration\": 587, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3274257183074951, \"iteration\": 588, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.34175288677215576, \"iteration\": 589, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2784973382949829, \"iteration\": 590, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.28393304347991943, \"iteration\": 591, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.35071584582328796, \"iteration\": 592, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20761491358280182, \"iteration\": 593, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.29244911670684814, \"iteration\": 594, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26285824179649353, \"iteration\": 595, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.24576877057552338, \"iteration\": 596, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3628990650177002, \"iteration\": 597, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3455736041069031, \"iteration\": 598, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.24858948588371277, \"iteration\": 599, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26306116580963135, \"iteration\": 600, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2766752541065216, \"iteration\": 601, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.32758599519729614, \"iteration\": 602, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 0.356796532869339, \"iteration\": 603, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3997460603713989, \"iteration\": 604, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2674950361251831, \"iteration\": 605, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3120412826538086, \"iteration\": 606, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.2909257113933563, \"iteration\": 607, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.28623488545417786, \"iteration\": 608, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.3813864588737488, \"iteration\": 609, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3368772864341736, \"iteration\": 610, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3058898150920868, \"iteration\": 611, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.20858512818813324, \"iteration\": 612, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4206066131591797, \"iteration\": 613, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.31099504232406616, \"iteration\": 614, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.258192241191864, \"iteration\": 615, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23786848783493042, \"iteration\": 616, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27981656789779663, \"iteration\": 617, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1692417562007904, \"iteration\": 618, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23766708374023438, \"iteration\": 619, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.4024353623390198, \"iteration\": 620, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.28532856702804565, \"iteration\": 621, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.28021809458732605, \"iteration\": 622, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3467453420162201, \"iteration\": 623, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.2778867185115814, \"iteration\": 624, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2589361369609833, \"iteration\": 625, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2608736455440521, \"iteration\": 626, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.06300035864114761, \"iteration\": 627, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.17454053461551666, \"iteration\": 628, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.20215269923210144, \"iteration\": 629, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14267073571681976, \"iteration\": 630, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15162932872772217, \"iteration\": 631, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.186222106218338, \"iteration\": 632, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16906443238258362, \"iteration\": 633, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.13598652184009552, \"iteration\": 634, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15180200338363647, \"iteration\": 635, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.18196320533752441, \"iteration\": 636, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10962103307247162, \"iteration\": 637, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.13017310202121735, \"iteration\": 638, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.21650800108909607, \"iteration\": 639, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.18897448480129242, \"iteration\": 640, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15611955523490906, \"iteration\": 641, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1435548961162567, \"iteration\": 642, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1568068265914917, \"iteration\": 643, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.16002243757247925, \"iteration\": 644, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.18919998407363892, \"iteration\": 645, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.12316619604825974, \"iteration\": 646, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.16614554822444916, \"iteration\": 647, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13022884726524353, \"iteration\": 648, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20840519666671753, \"iteration\": 649, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17200037837028503, \"iteration\": 650, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19161711633205414, \"iteration\": 651, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17414681613445282, \"iteration\": 652, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14254863560199738, \"iteration\": 653, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23417216539382935, \"iteration\": 654, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14446386694908142, \"iteration\": 655, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17898568511009216, \"iteration\": 656, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.21620316803455353, \"iteration\": 657, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16375018656253815, \"iteration\": 658, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1637437343597412, \"iteration\": 659, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17558424174785614, \"iteration\": 660, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.2062000334262848, \"iteration\": 661, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14161214232444763, \"iteration\": 662, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12464301288127899, \"iteration\": 663, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.18236172199249268, \"iteration\": 664, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17986391484737396, \"iteration\": 665, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.16882160305976868, \"iteration\": 666, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16218534111976624, \"iteration\": 667, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1756211221218109, \"iteration\": 668, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.14206205308437347, \"iteration\": 669, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.19622917473316193, \"iteration\": 670, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17079326510429382, \"iteration\": 671, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1276213377714157, \"iteration\": 672, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1407383382320404, \"iteration\": 673, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.15092328190803528, \"iteration\": 674, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1479896456003189, \"iteration\": 675, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09752006083726883, \"iteration\": 676, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1685774326324463, \"iteration\": 677, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1380993127822876, \"iteration\": 678, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1367919147014618, \"iteration\": 679, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.16052159667015076, \"iteration\": 680, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19107133150100708, \"iteration\": 681, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1532188057899475, \"iteration\": 682, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25769880414009094, \"iteration\": 683, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.14198675751686096, \"iteration\": 684, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.19151878356933594, \"iteration\": 685, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.12069293111562729, \"iteration\": 686, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1179581731557846, \"iteration\": 687, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.15602363646030426, \"iteration\": 688, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.15655723214149475, \"iteration\": 689, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.12060656398534775, \"iteration\": 690, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17969459295272827, \"iteration\": 691, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17727361619472504, \"iteration\": 692, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1602572798728943, \"iteration\": 693, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.16723909974098206, \"iteration\": 694, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.190084770321846, \"iteration\": 695, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13992995023727417, \"iteration\": 696, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.17047592997550964, \"iteration\": 697, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.2924101948738098, \"iteration\": 698, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.23186451196670532, \"iteration\": 699, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17839273810386658, \"iteration\": 700, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18023252487182617, \"iteration\": 701, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1048709973692894, \"iteration\": 702, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.15725193917751312, \"iteration\": 703, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.2343873530626297, \"iteration\": 704, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.23216204345226288, \"iteration\": 705, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14639243483543396, \"iteration\": 706, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.21097329258918762, \"iteration\": 707, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22180405259132385, \"iteration\": 708, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.25500479340553284, \"iteration\": 709, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16413450241088867, \"iteration\": 710, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.1905931681394577, \"iteration\": 711, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19569151103496552, \"iteration\": 712, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16292335093021393, \"iteration\": 713, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.1082051694393158, \"iteration\": 714, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.16767746210098267, \"iteration\": 715, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13649414479732513, \"iteration\": 716, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11921173334121704, \"iteration\": 717, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16833001375198364, \"iteration\": 718, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13887815177440643, \"iteration\": 719, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19267457723617554, \"iteration\": 720, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.20982368290424347, \"iteration\": 721, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20849020779132843, \"iteration\": 722, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.20260120928287506, \"iteration\": 723, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2339407503604889, \"iteration\": 724, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.2526622712612152, \"iteration\": 725, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.2512623965740204, \"iteration\": 726, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1847938746213913, \"iteration\": 727, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.22673363983631134, \"iteration\": 728, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.15439823269844055, \"iteration\": 729, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11763149499893188, \"iteration\": 730, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1390177458524704, \"iteration\": 731, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1991797238588333, \"iteration\": 732, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22521185874938965, \"iteration\": 733, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.1981138437986374, \"iteration\": 734, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.20767270028591156, \"iteration\": 735, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20201070606708527, \"iteration\": 736, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.21944226324558258, \"iteration\": 737, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.21938899159431458, \"iteration\": 738, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.2245253622531891, \"iteration\": 739, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.17944826185703278, \"iteration\": 740, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.21382001042366028, \"iteration\": 741, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2318802922964096, \"iteration\": 742, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17971950769424438, \"iteration\": 743, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14495760202407837, \"iteration\": 744, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.13577869534492493, \"iteration\": 745, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.2141461819410324, \"iteration\": 746, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.2268020212650299, \"iteration\": 747, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.2136746644973755, \"iteration\": 748, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.14706455171108246, \"iteration\": 749, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 0.31261295080184937, \"iteration\": 750, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.24690166115760803, \"iteration\": 751, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.11467981338500977, \"iteration\": 752, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.21114645898342133, \"iteration\": 753, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19400236010551453, \"iteration\": 754, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2974599003791809, \"iteration\": 755, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23417465388774872, \"iteration\": 756, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.16511252522468567, \"iteration\": 757, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 0.29699191451072693, \"iteration\": 758, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.25589805841445923, \"iteration\": 759, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16086560487747192, \"iteration\": 760, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1919916719198227, \"iteration\": 761, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.13697798550128937, \"iteration\": 762, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.18637293577194214, \"iteration\": 763, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2968995273113251, \"iteration\": 764, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.17941322922706604, \"iteration\": 765, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23272374272346497, \"iteration\": 766, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.29302939772605896, \"iteration\": 767, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.17921388149261475, \"iteration\": 768, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1896100491285324, \"iteration\": 769, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16418229043483734, \"iteration\": 770, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17041099071502686, \"iteration\": 771, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22373229265213013, \"iteration\": 772, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3314034044742584, \"iteration\": 773, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.21341745555400848, \"iteration\": 774, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22774043679237366, \"iteration\": 775, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16603854298591614, \"iteration\": 776, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.2640586495399475, \"iteration\": 777, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1791185736656189, \"iteration\": 778, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.25810888409614563, \"iteration\": 779, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.24142423272132874, \"iteration\": 780, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19417226314544678, \"iteration\": 781, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.24136115610599518, \"iteration\": 782, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.26424452662467957, \"iteration\": 783, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.14979970455169678, \"iteration\": 784, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.19718360900878906, \"iteration\": 785, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.25772085785865784, \"iteration\": 786, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.3069699704647064, \"iteration\": 787, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2493925392627716, \"iteration\": 788, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.22301727533340454, \"iteration\": 789, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2663552761077881, \"iteration\": 790, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22901664674282074, \"iteration\": 791, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2930174171924591, \"iteration\": 792, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.21127325296401978, \"iteration\": 793, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.24077066779136658, \"iteration\": 794, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.24066726863384247, \"iteration\": 795, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2110319882631302, \"iteration\": 796, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1753692328929901, \"iteration\": 797, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1367633044719696, \"iteration\": 798, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14255093038082123, \"iteration\": 799, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2680293023586273, \"iteration\": 800, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.18938858807086945, \"iteration\": 801, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.22612370550632477, \"iteration\": 802, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 0.31963449716567993, \"iteration\": 803, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 0.24998760223388672, \"iteration\": 804, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.1974833607673645, \"iteration\": 805, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.3487970530986786, \"iteration\": 806, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.23790206015110016, \"iteration\": 807, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.16048909723758698, \"iteration\": 808, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17318131029605865, \"iteration\": 809, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15514817833900452, \"iteration\": 810, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.17281165719032288, \"iteration\": 811, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2438795417547226, \"iteration\": 812, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2823685109615326, \"iteration\": 813, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.26160120964050293, \"iteration\": 814, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.21061041951179504, \"iteration\": 815, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18565812706947327, \"iteration\": 816, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.15213382244110107, \"iteration\": 817, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.2504168152809143, \"iteration\": 818, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.32607415318489075, \"iteration\": 819, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.22047801315784454, \"iteration\": 820, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.15586158633232117, \"iteration\": 821, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.1954561173915863, \"iteration\": 822, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17328064143657684, \"iteration\": 823, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23424799740314484, \"iteration\": 824, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20122528076171875, \"iteration\": 825, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17121019959449768, \"iteration\": 826, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14537736773490906, \"iteration\": 827, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23222622275352478, \"iteration\": 828, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.28734904527664185, \"iteration\": 829, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.38203027844429016, \"iteration\": 830, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.246053546667099, \"iteration\": 831, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.22050033509731293, \"iteration\": 832, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2036365419626236, \"iteration\": 833, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.21338917315006256, \"iteration\": 834, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13177403807640076, \"iteration\": 835, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.26864951848983765, \"iteration\": 836, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10319581627845764, \"iteration\": 837, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08559410274028778, \"iteration\": 838, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13342301547527313, \"iteration\": 839, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11315128952264786, \"iteration\": 840, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11945845186710358, \"iteration\": 841, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08871283382177353, \"iteration\": 842, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1167369931936264, \"iteration\": 843, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.14233435690402985, \"iteration\": 844, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08110757172107697, \"iteration\": 845, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08878738433122635, \"iteration\": 846, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09446483105421066, \"iteration\": 847, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09109281003475189, \"iteration\": 848, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09046060591936111, \"iteration\": 849, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.05650928616523743, \"iteration\": 850, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20888635516166687, \"iteration\": 851, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.10029105842113495, \"iteration\": 852, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09199775755405426, \"iteration\": 853, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12654250860214233, \"iteration\": 854, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.08914388716220856, \"iteration\": 855, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11661084741353989, \"iteration\": 856, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.06625371426343918, \"iteration\": 857, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08350923657417297, \"iteration\": 858, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.06724277138710022, \"iteration\": 859, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11260262876749039, \"iteration\": 860, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.11955612897872925, \"iteration\": 861, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10226777195930481, \"iteration\": 862, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.10442116856575012, \"iteration\": 863, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11314081400632858, \"iteration\": 864, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.10169170051813126, \"iteration\": 865, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.19603084027767181, \"iteration\": 866, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10161810368299484, \"iteration\": 867, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08910633623600006, \"iteration\": 868, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09057144820690155, \"iteration\": 869, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08010021597146988, \"iteration\": 870, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.11174547672271729, \"iteration\": 871, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08761683851480484, \"iteration\": 872, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1093323603272438, \"iteration\": 873, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.12964124977588654, \"iteration\": 874, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08835290372371674, \"iteration\": 875, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08350536227226257, \"iteration\": 876, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08349117636680603, \"iteration\": 877, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1131320595741272, \"iteration\": 878, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11271330714225769, \"iteration\": 879, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11738856136798859, \"iteration\": 880, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09902600198984146, \"iteration\": 881, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09646706283092499, \"iteration\": 882, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.04608968645334244, \"iteration\": 883, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1371312141418457, \"iteration\": 884, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11502877622842789, \"iteration\": 885, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04790022596716881, \"iteration\": 886, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.057217929512262344, \"iteration\": 887, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.06299884617328644, \"iteration\": 888, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.09035774320363998, \"iteration\": 889, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11128358542919159, \"iteration\": 890, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.1173589825630188, \"iteration\": 891, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05668396130204201, \"iteration\": 892, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1239466592669487, \"iteration\": 893, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.10948081314563751, \"iteration\": 894, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09513259679079056, \"iteration\": 895, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1303408145904541, \"iteration\": 896, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08620590716600418, \"iteration\": 897, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.05691640079021454, \"iteration\": 898, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09119313210248947, \"iteration\": 899, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.09141513705253601, \"iteration\": 900, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14971505105495453, \"iteration\": 901, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.09610451012849808, \"iteration\": 902, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.05937643721699715, \"iteration\": 903, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.1268054097890854, \"iteration\": 904, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1329898089170456, \"iteration\": 905, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15856093168258667, \"iteration\": 906, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11583662778139114, \"iteration\": 907, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11010333150625229, \"iteration\": 908, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.10363557189702988, \"iteration\": 909, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11707372963428497, \"iteration\": 910, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09230019152164459, \"iteration\": 911, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12173247337341309, \"iteration\": 912, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09716330468654633, \"iteration\": 913, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09577494859695435, \"iteration\": 914, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.0870867446064949, \"iteration\": 915, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.06014937907457352, \"iteration\": 916, \"epoch\": 5}, {\"training_acc\": 0.9296875, \"training_loss\": 0.1671692132949829, \"iteration\": 917, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09968199580907822, \"iteration\": 918, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.0766717940568924, \"iteration\": 919, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.12688647210597992, \"iteration\": 920, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05631893128156662, \"iteration\": 921, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.120832160115242, \"iteration\": 922, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.12293475866317749, \"iteration\": 923, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0712432861328125, \"iteration\": 924, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10499037057161331, \"iteration\": 925, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11580187827348709, \"iteration\": 926, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.09617418050765991, \"iteration\": 927, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.10009720921516418, \"iteration\": 928, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.14869818091392517, \"iteration\": 929, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1542714387178421, \"iteration\": 930, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09279164671897888, \"iteration\": 931, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11446059495210648, \"iteration\": 932, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.11036700010299683, \"iteration\": 933, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.13027942180633545, \"iteration\": 934, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09517958015203476, \"iteration\": 935, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06814941018819809, \"iteration\": 936, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.13998259603977203, \"iteration\": 937, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.1642780303955078, \"iteration\": 938, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.04561125487089157, \"iteration\": 939, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06903796643018723, \"iteration\": 940, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08509351313114166, \"iteration\": 941, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.0967053696513176, \"iteration\": 942, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11307884752750397, \"iteration\": 943, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.157925084233284, \"iteration\": 944, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09836341440677643, \"iteration\": 945, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.13215696811676025, \"iteration\": 946, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0393487848341465, \"iteration\": 947, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09674866497516632, \"iteration\": 948, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09049607068300247, \"iteration\": 949, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10165578871965408, \"iteration\": 950, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16033563017845154, \"iteration\": 951, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.11142652481794357, \"iteration\": 952, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10964736342430115, \"iteration\": 953, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07497254014015198, \"iteration\": 954, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07047177851200104, \"iteration\": 955, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.10364406555891037, \"iteration\": 956, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10183271020650864, \"iteration\": 957, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10516513139009476, \"iteration\": 958, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.18324442207813263, \"iteration\": 959, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05794230103492737, \"iteration\": 960, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.10306474566459656, \"iteration\": 961, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08324502408504486, \"iteration\": 962, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.12831123173236847, \"iteration\": 963, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.13637416064739227, \"iteration\": 964, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08217717707157135, \"iteration\": 965, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11955743283033371, \"iteration\": 966, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10698188841342926, \"iteration\": 967, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.09893552213907242, \"iteration\": 968, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08781812340021133, \"iteration\": 969, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.1259264200925827, \"iteration\": 970, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09463261067867279, \"iteration\": 971, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09520649164915085, \"iteration\": 972, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07521757483482361, \"iteration\": 973, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.18024899065494537, \"iteration\": 974, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09548220783472061, \"iteration\": 975, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.12591694295406342, \"iteration\": 976, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10257834941148758, \"iteration\": 977, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11592970788478851, \"iteration\": 978, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.20319314301013947, \"iteration\": 979, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16744087636470795, \"iteration\": 980, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08208617568016052, \"iteration\": 981, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07792628556489944, \"iteration\": 982, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.11303967237472534, \"iteration\": 983, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1516270935535431, \"iteration\": 984, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11607097089290619, \"iteration\": 985, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1012449562549591, \"iteration\": 986, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11257050186395645, \"iteration\": 987, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17103227972984314, \"iteration\": 988, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.13252925872802734, \"iteration\": 989, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11420121788978577, \"iteration\": 990, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08788949996232986, \"iteration\": 991, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.16509686410427094, \"iteration\": 992, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1982363611459732, \"iteration\": 993, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15870173275470734, \"iteration\": 994, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.13347265124320984, \"iteration\": 995, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16324982047080994, \"iteration\": 996, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09713217616081238, \"iteration\": 997, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10700452327728271, \"iteration\": 998, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11483736336231232, \"iteration\": 999, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11846496909856796, \"iteration\": 1000, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.1449250876903534, \"iteration\": 1001, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.15138408541679382, \"iteration\": 1002, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12073356658220291, \"iteration\": 1003, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07750476896762848, \"iteration\": 1004, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08124177902936935, \"iteration\": 1005, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07037307322025299, \"iteration\": 1006, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1301947832107544, \"iteration\": 1007, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11620543897151947, \"iteration\": 1008, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.13016259670257568, \"iteration\": 1009, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.11189857125282288, \"iteration\": 1010, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.20301228761672974, \"iteration\": 1011, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.19409075379371643, \"iteration\": 1012, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11873027682304382, \"iteration\": 1013, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.13018153607845306, \"iteration\": 1014, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.16755607724189758, \"iteration\": 1015, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06909886002540588, \"iteration\": 1016, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.12525874376296997, \"iteration\": 1017, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.10976816713809967, \"iteration\": 1018, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.12021436542272568, \"iteration\": 1019, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.12705418467521667, \"iteration\": 1020, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.13549627363681793, \"iteration\": 1021, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09660996496677399, \"iteration\": 1022, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12256474792957306, \"iteration\": 1023, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16053426265716553, \"iteration\": 1024, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1483498364686966, \"iteration\": 1025, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0994764119386673, \"iteration\": 1026, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.12562379240989685, \"iteration\": 1027, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12272533029317856, \"iteration\": 1028, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12058325111865997, \"iteration\": 1029, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14022883772850037, \"iteration\": 1030, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.14599871635437012, \"iteration\": 1031, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1171996220946312, \"iteration\": 1032, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13525743782520294, \"iteration\": 1033, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08740019798278809, \"iteration\": 1034, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07064514607191086, \"iteration\": 1035, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11277599632740021, \"iteration\": 1036, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.18974590301513672, \"iteration\": 1037, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.12946927547454834, \"iteration\": 1038, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12291035056114197, \"iteration\": 1039, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1680712252855301, \"iteration\": 1040, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.15395113825798035, \"iteration\": 1041, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.16098566353321075, \"iteration\": 1042, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09498339891433716, \"iteration\": 1043, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.11371389776468277, \"iteration\": 1044, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.028671745210886, \"iteration\": 1045, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.04982774332165718, \"iteration\": 1046, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04549185186624527, \"iteration\": 1047, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0718616396188736, \"iteration\": 1048, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.032961323857307434, \"iteration\": 1049, \"epoch\": 6}, {\"training_acc\": 0.9609375, \"training_loss\": 0.08148070424795151, \"iteration\": 1050, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05734923481941223, \"iteration\": 1051, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.04899020120501518, \"iteration\": 1052, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08369126170873642, \"iteration\": 1053, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05226992070674896, \"iteration\": 1054, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03420109674334526, \"iteration\": 1055, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.06275817006826401, \"iteration\": 1056, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.049108318984508514, \"iteration\": 1057, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07160602509975433, \"iteration\": 1058, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02215036191046238, \"iteration\": 1059, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.030813220888376236, \"iteration\": 1060, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05045585706830025, \"iteration\": 1061, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07245992124080658, \"iteration\": 1062, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.030121419578790665, \"iteration\": 1063, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0432606115937233, \"iteration\": 1064, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03596584126353264, \"iteration\": 1065, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.044346582144498825, \"iteration\": 1066, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04472139850258827, \"iteration\": 1067, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04931366443634033, \"iteration\": 1068, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.030390890315175056, \"iteration\": 1069, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.035508885979652405, \"iteration\": 1070, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07426148653030396, \"iteration\": 1071, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07087009400129318, \"iteration\": 1072, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.022873928770422935, \"iteration\": 1073, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03606410324573517, \"iteration\": 1074, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04591156914830208, \"iteration\": 1075, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02544507570564747, \"iteration\": 1076, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03417900204658508, \"iteration\": 1077, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0577709786593914, \"iteration\": 1078, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.024862516671419144, \"iteration\": 1079, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08075807988643646, \"iteration\": 1080, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03308507800102234, \"iteration\": 1081, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06626646965742111, \"iteration\": 1082, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04304540902376175, \"iteration\": 1083, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07610654830932617, \"iteration\": 1084, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.06821884959936142, \"iteration\": 1085, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07465678453445435, \"iteration\": 1086, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05681944638490677, \"iteration\": 1087, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06740330159664154, \"iteration\": 1088, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.041246265172958374, \"iteration\": 1089, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.036754339933395386, \"iteration\": 1090, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04032736271619797, \"iteration\": 1091, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03798920661211014, \"iteration\": 1092, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07540606707334518, \"iteration\": 1093, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03622746095061302, \"iteration\": 1094, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0342961810529232, \"iteration\": 1095, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.024641307070851326, \"iteration\": 1096, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02268446795642376, \"iteration\": 1097, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.023064618930220604, \"iteration\": 1098, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05286959558725357, \"iteration\": 1099, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09652405232191086, \"iteration\": 1100, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.035118866711854935, \"iteration\": 1101, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06907530128955841, \"iteration\": 1102, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.048059504479169846, \"iteration\": 1103, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05285311117768288, \"iteration\": 1104, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03701696917414665, \"iteration\": 1105, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06455599516630173, \"iteration\": 1106, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.030557025223970413, \"iteration\": 1107, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05016755312681198, \"iteration\": 1108, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04170027747750282, \"iteration\": 1109, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07878649979829788, \"iteration\": 1110, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07790987938642502, \"iteration\": 1111, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04375815391540527, \"iteration\": 1112, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.06656758487224579, \"iteration\": 1113, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0346810407936573, \"iteration\": 1114, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06354697793722153, \"iteration\": 1115, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0312364362180233, \"iteration\": 1116, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05310684069991112, \"iteration\": 1117, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04776005446910858, \"iteration\": 1118, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.04939863458275795, \"iteration\": 1119, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.0748947486281395, \"iteration\": 1120, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.037057116627693176, \"iteration\": 1121, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.025736283510923386, \"iteration\": 1122, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.025358062237501144, \"iteration\": 1123, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02048654854297638, \"iteration\": 1124, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05304709076881409, \"iteration\": 1125, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0359552837908268, \"iteration\": 1126, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0240478552877903, \"iteration\": 1127, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06687518209218979, \"iteration\": 1128, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.043198201805353165, \"iteration\": 1129, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.056876279413700104, \"iteration\": 1130, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03504938259720802, \"iteration\": 1131, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0449104979634285, \"iteration\": 1132, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.059325091540813446, \"iteration\": 1133, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05976537615060806, \"iteration\": 1134, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04367709532380104, \"iteration\": 1135, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05908272787928581, \"iteration\": 1136, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0638803020119667, \"iteration\": 1137, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01955997198820114, \"iteration\": 1138, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03164368495345116, \"iteration\": 1139, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05861244350671768, \"iteration\": 1140, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08978357911109924, \"iteration\": 1141, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05361326038837433, \"iteration\": 1142, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.023002149537205696, \"iteration\": 1143, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0380074568092823, \"iteration\": 1144, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.050357528030872345, \"iteration\": 1145, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.05142182111740112, \"iteration\": 1146, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.09616978466510773, \"iteration\": 1147, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03502953052520752, \"iteration\": 1148, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03531045466661453, \"iteration\": 1149, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.03409605100750923, \"iteration\": 1150, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.015756189823150635, \"iteration\": 1151, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.019747965037822723, \"iteration\": 1152, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0337705984711647, \"iteration\": 1153, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0344768688082695, \"iteration\": 1154, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.05637944117188454, \"iteration\": 1155, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.029637178406119347, \"iteration\": 1156, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02054990828037262, \"iteration\": 1157, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04010533541440964, \"iteration\": 1158, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05918577313423157, \"iteration\": 1159, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.033389702439308167, \"iteration\": 1160, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.024065954610705376, \"iteration\": 1161, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.033978551626205444, \"iteration\": 1162, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.060777779668569565, \"iteration\": 1163, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.056230105459690094, \"iteration\": 1164, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04522445425391197, \"iteration\": 1165, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.029558900743722916, \"iteration\": 1166, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03983009606599808, \"iteration\": 1167, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05047363042831421, \"iteration\": 1168, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03187302500009537, \"iteration\": 1169, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06224135309457779, \"iteration\": 1170, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.045365702360868454, \"iteration\": 1171, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03906410560011864, \"iteration\": 1172, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.0396537221968174, \"iteration\": 1173, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0331374928355217, \"iteration\": 1174, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08007756620645523, \"iteration\": 1175, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.017696114256978035, \"iteration\": 1176, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05475221574306488, \"iteration\": 1177, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03754187375307083, \"iteration\": 1178, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.032030507922172546, \"iteration\": 1179, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.042662203311920166, \"iteration\": 1180, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05038958042860031, \"iteration\": 1181, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0544775165617466, \"iteration\": 1182, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03754093125462532, \"iteration\": 1183, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04087652266025543, \"iteration\": 1184, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.021785954013466835, \"iteration\": 1185, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03171167150139809, \"iteration\": 1186, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04105588421225548, \"iteration\": 1187, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.05656248703598976, \"iteration\": 1188, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.054573096334934235, \"iteration\": 1189, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.021179361268877983, \"iteration\": 1190, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.101402647793293, \"iteration\": 1191, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03676408529281616, \"iteration\": 1192, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.1406271904706955, \"iteration\": 1193, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05240171030163765, \"iteration\": 1194, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.025075763463974, \"iteration\": 1195, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0769428089261055, \"iteration\": 1196, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03438854217529297, \"iteration\": 1197, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.028659433126449585, \"iteration\": 1198, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.10485931485891342, \"iteration\": 1199, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04737846553325653, \"iteration\": 1200, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04525779187679291, \"iteration\": 1201, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03960605338215828, \"iteration\": 1202, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.042855240404605865, \"iteration\": 1203, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.027521733194589615, \"iteration\": 1204, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.05247814580798149, \"iteration\": 1205, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.030453016981482506, \"iteration\": 1206, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.054634395986795425, \"iteration\": 1207, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08845186233520508, \"iteration\": 1208, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.019032251089811325, \"iteration\": 1209, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.040027718991041183, \"iteration\": 1210, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03685677424073219, \"iteration\": 1211, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.045948922634124756, \"iteration\": 1212, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05572078749537468, \"iteration\": 1213, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03041970171034336, \"iteration\": 1214, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.023413782939314842, \"iteration\": 1215, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03677704557776451, \"iteration\": 1216, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03166816756129265, \"iteration\": 1217, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.06906545162200928, \"iteration\": 1218, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07956454157829285, \"iteration\": 1219, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03755956143140793, \"iteration\": 1220, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04541356489062309, \"iteration\": 1221, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05803213268518448, \"iteration\": 1222, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.11007831990718842, \"iteration\": 1223, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03270085155963898, \"iteration\": 1224, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.06586854159832001, \"iteration\": 1225, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.057704996317625046, \"iteration\": 1226, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07247645407915115, \"iteration\": 1227, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05785408616065979, \"iteration\": 1228, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05691957846283913, \"iteration\": 1229, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04061494767665863, \"iteration\": 1230, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.018670620396733284, \"iteration\": 1231, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05281175673007965, \"iteration\": 1232, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08175074309110641, \"iteration\": 1233, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04296363890171051, \"iteration\": 1234, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.021569635719060898, \"iteration\": 1235, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.043525759130716324, \"iteration\": 1236, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04158001393079758, \"iteration\": 1237, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.021670550107955933, \"iteration\": 1238, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05919273570179939, \"iteration\": 1239, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.043384574353694916, \"iteration\": 1240, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06695472449064255, \"iteration\": 1241, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.10332134366035461, \"iteration\": 1242, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.028658030554652214, \"iteration\": 1243, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01912560500204563, \"iteration\": 1244, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.033922191709280014, \"iteration\": 1245, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.029731987044215202, \"iteration\": 1246, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03815295919775963, \"iteration\": 1247, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09143397957086563, \"iteration\": 1248, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.046992745250463486, \"iteration\": 1249, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11851002275943756, \"iteration\": 1250, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08631815016269684, \"iteration\": 1251, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.038586899638175964, \"iteration\": 1252, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.031605977565050125, \"iteration\": 1253, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02105759270489216, \"iteration\": 1254, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05376293882727623, \"iteration\": 1255, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.05940279737114906, \"iteration\": 1256, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07992753386497498, \"iteration\": 1257, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02147867903113365, \"iteration\": 1258, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02070966549217701, \"iteration\": 1259, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03200288116931915, \"iteration\": 1260, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06654961407184601, \"iteration\": 1261, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.05557982623577118, \"iteration\": 1262, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012633715756237507, \"iteration\": 1263, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.007387419696897268, \"iteration\": 1264, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.025323696434497833, \"iteration\": 1265, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.053528767079114914, \"iteration\": 1266, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.040374547243118286, \"iteration\": 1267, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013687195256352425, \"iteration\": 1268, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009402272291481495, \"iteration\": 1269, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.038560010492801666, \"iteration\": 1270, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03307816758751869, \"iteration\": 1271, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04198421537876129, \"iteration\": 1272, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0449303574860096, \"iteration\": 1273, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03348366916179657, \"iteration\": 1274, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02699938230216503, \"iteration\": 1275, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03619815409183502, \"iteration\": 1276, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016785288229584694, \"iteration\": 1277, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.08078590780496597, \"iteration\": 1278, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014751571230590343, \"iteration\": 1279, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021507976576685905, \"iteration\": 1280, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009765129536390305, \"iteration\": 1281, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.025624437257647514, \"iteration\": 1282, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.025786414742469788, \"iteration\": 1283, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011720209382474422, \"iteration\": 1284, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.022524097934365273, \"iteration\": 1285, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010843455791473389, \"iteration\": 1286, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.019127316772937775, \"iteration\": 1287, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01505111251026392, \"iteration\": 1288, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.020111478865146637, \"iteration\": 1289, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010703491047024727, \"iteration\": 1290, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.022793084383010864, \"iteration\": 1291, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010599302127957344, \"iteration\": 1292, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008526704274117947, \"iteration\": 1293, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01668882928788662, \"iteration\": 1294, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0067923772148787975, \"iteration\": 1295, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014898419380187988, \"iteration\": 1296, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.0647268071770668, \"iteration\": 1297, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.019750183448195457, \"iteration\": 1298, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03387898951768875, \"iteration\": 1299, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014418026432394981, \"iteration\": 1300, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02251145802438259, \"iteration\": 1301, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.018036477267742157, \"iteration\": 1302, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.037596531212329865, \"iteration\": 1303, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.028400616720318794, \"iteration\": 1304, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.017647171393036842, \"iteration\": 1305, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03131735324859619, \"iteration\": 1306, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01431252621114254, \"iteration\": 1307, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03272140026092529, \"iteration\": 1308, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013043791987001896, \"iteration\": 1309, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006551004946231842, \"iteration\": 1310, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.055049050599336624, \"iteration\": 1311, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012376389466226101, \"iteration\": 1312, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.018574396148324013, \"iteration\": 1313, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.022413838654756546, \"iteration\": 1314, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0254228375852108, \"iteration\": 1315, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011064794845879078, \"iteration\": 1316, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028269900009036064, \"iteration\": 1317, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005970858968794346, \"iteration\": 1318, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013629366643726826, \"iteration\": 1319, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.026710625737905502, \"iteration\": 1320, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03144165128469467, \"iteration\": 1321, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.022213811054825783, \"iteration\": 1322, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014969666488468647, \"iteration\": 1323, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011724414303898811, \"iteration\": 1324, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012194545939564705, \"iteration\": 1325, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016961196437478065, \"iteration\": 1326, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.020268114283680916, \"iteration\": 1327, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015847602859139442, \"iteration\": 1328, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04505346715450287, \"iteration\": 1329, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006028496194630861, \"iteration\": 1330, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04480554908514023, \"iteration\": 1331, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012311017140746117, \"iteration\": 1332, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.018626132979989052, \"iteration\": 1333, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.041487306356430054, \"iteration\": 1334, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.025149885565042496, \"iteration\": 1335, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.024020934477448463, \"iteration\": 1336, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011338402517139912, \"iteration\": 1337, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.029117707163095474, \"iteration\": 1338, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02407723478972912, \"iteration\": 1339, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02041447162628174, \"iteration\": 1340, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00712237786501646, \"iteration\": 1341, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03047003410756588, \"iteration\": 1342, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011025902815163136, \"iteration\": 1343, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.08451896160840988, \"iteration\": 1344, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015047445893287659, \"iteration\": 1345, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.027061570435762405, \"iteration\": 1346, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.020819634199142456, \"iteration\": 1347, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013751850463449955, \"iteration\": 1348, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.024880768731236458, \"iteration\": 1349, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026847487315535545, \"iteration\": 1350, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011934356763958931, \"iteration\": 1351, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008714308962225914, \"iteration\": 1352, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012937557883560658, \"iteration\": 1353, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014730331487953663, \"iteration\": 1354, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010912902653217316, \"iteration\": 1355, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016564199700951576, \"iteration\": 1356, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02501254342496395, \"iteration\": 1357, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008082213811576366, \"iteration\": 1358, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01435139961540699, \"iteration\": 1359, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.017680440098047256, \"iteration\": 1360, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01858368143439293, \"iteration\": 1361, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.020996052771806717, \"iteration\": 1362, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012905134819447994, \"iteration\": 1363, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.025670409202575684, \"iteration\": 1364, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008414930664002895, \"iteration\": 1365, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010675028897821903, \"iteration\": 1366, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01324789971113205, \"iteration\": 1367, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02008064091205597, \"iteration\": 1368, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011881573125720024, \"iteration\": 1369, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008168726228177547, \"iteration\": 1370, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01900452934205532, \"iteration\": 1371, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007830752991139889, \"iteration\": 1372, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006008305121213198, \"iteration\": 1373, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006490718573331833, \"iteration\": 1374, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.042226582765579224, \"iteration\": 1375, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010812463238835335, \"iteration\": 1376, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02189727872610092, \"iteration\": 1377, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013240541331470013, \"iteration\": 1378, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02241455763578415, \"iteration\": 1379, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013789555057883263, \"iteration\": 1380, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011493184603750706, \"iteration\": 1381, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010422422550618649, \"iteration\": 1382, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012548049911856651, \"iteration\": 1383, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01641855388879776, \"iteration\": 1384, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009161773137748241, \"iteration\": 1385, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015789730474352837, \"iteration\": 1386, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01811075583100319, \"iteration\": 1387, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015134181827306747, \"iteration\": 1388, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.026088755577802658, \"iteration\": 1389, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.018593816086649895, \"iteration\": 1390, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013003421016037464, \"iteration\": 1391, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02035168744623661, \"iteration\": 1392, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03653086721897125, \"iteration\": 1393, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015260539017617702, \"iteration\": 1394, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.029822204262018204, \"iteration\": 1395, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.035471219569444656, \"iteration\": 1396, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010907763615250587, \"iteration\": 1397, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.021324532106518745, \"iteration\": 1398, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.020104920491576195, \"iteration\": 1399, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.055940691381692886, \"iteration\": 1400, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013094332069158554, \"iteration\": 1401, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.022012002766132355, \"iteration\": 1402, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.020603500306606293, \"iteration\": 1403, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03814055398106575, \"iteration\": 1404, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.026898516342043877, \"iteration\": 1405, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01291874423623085, \"iteration\": 1406, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012875547632575035, \"iteration\": 1407, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05254940316081047, \"iteration\": 1408, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01181054301559925, \"iteration\": 1409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0197825338691473, \"iteration\": 1410, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008397097699344158, \"iteration\": 1411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007142267655581236, \"iteration\": 1412, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.038679540157318115, \"iteration\": 1413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.036617960780858994, \"iteration\": 1414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005244917701929808, \"iteration\": 1415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01809990219771862, \"iteration\": 1416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.018140258267521858, \"iteration\": 1417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013727804645895958, \"iteration\": 1418, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007748356554657221, \"iteration\": 1419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01533589605242014, \"iteration\": 1420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008210597559809685, \"iteration\": 1421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.023899026215076447, \"iteration\": 1422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0076355282217264175, \"iteration\": 1423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014479179866611958, \"iteration\": 1424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.019279126077890396, \"iteration\": 1425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01011640578508377, \"iteration\": 1426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011057406663894653, \"iteration\": 1427, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013220514170825481, \"iteration\": 1428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015654124319553375, \"iteration\": 1429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03500598669052124, \"iteration\": 1430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00961349904537201, \"iteration\": 1431, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0490683987736702, \"iteration\": 1432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.028673376888036728, \"iteration\": 1433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01078424695879221, \"iteration\": 1434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007937004789710045, \"iteration\": 1435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02150043286383152, \"iteration\": 1436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012481799349188805, \"iteration\": 1437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010029014199972153, \"iteration\": 1438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012380454689264297, \"iteration\": 1439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.023429816588759422, \"iteration\": 1440, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03661827743053436, \"iteration\": 1441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01878909394145012, \"iteration\": 1442, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.020871445536613464, \"iteration\": 1443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00859920959919691, \"iteration\": 1444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0248334389179945, \"iteration\": 1445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.018662001937627792, \"iteration\": 1446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03585526719689369, \"iteration\": 1447, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04742506891489029, \"iteration\": 1448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015850359573960304, \"iteration\": 1449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009388739243149757, \"iteration\": 1450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012671046890318394, \"iteration\": 1451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005646401084959507, \"iteration\": 1452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0053757778368890285, \"iteration\": 1453, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028627781197428703, \"iteration\": 1454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008876047097146511, \"iteration\": 1455, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.024229127913713455, \"iteration\": 1456, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009722806513309479, \"iteration\": 1457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010266660712659359, \"iteration\": 1458, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.025998394936323166, \"iteration\": 1459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01781465858221054, \"iteration\": 1460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010762663558125496, \"iteration\": 1461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01030295342206955, \"iteration\": 1462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.029185375198721886, \"iteration\": 1463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015072470530867577, \"iteration\": 1464, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.021801922470331192, \"iteration\": 1465, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.013741230592131615, \"iteration\": 1466, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.03268652409315109, \"iteration\": 1467, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.019553672522306442, \"iteration\": 1468, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00815659761428833, \"iteration\": 1469, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00854983925819397, \"iteration\": 1470, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.058077357709407806, \"iteration\": 1471, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012456512078642845, \"iteration\": 1472, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0028183888643980026, \"iteration\": 1473, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009575686417520046, \"iteration\": 1474, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0027443685103207827, \"iteration\": 1475, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.021297650411725044, \"iteration\": 1476, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.016202162951231003, \"iteration\": 1477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0053727952763438225, \"iteration\": 1478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006131421774625778, \"iteration\": 1479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008519702591001987, \"iteration\": 1480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0028269817121326923, \"iteration\": 1481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004135976079851389, \"iteration\": 1482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004704751539975405, \"iteration\": 1483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.026282180100679398, \"iteration\": 1484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010976744815707207, \"iteration\": 1485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005737639497965574, \"iteration\": 1486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011065825819969177, \"iteration\": 1487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004258205182850361, \"iteration\": 1488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008831811137497425, \"iteration\": 1489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01627971976995468, \"iteration\": 1490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0037638377398252487, \"iteration\": 1491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00898041669279337, \"iteration\": 1492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003961915150284767, \"iteration\": 1493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00498611805960536, \"iteration\": 1494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006716470699757338, \"iteration\": 1495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014627041295170784, \"iteration\": 1496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004670869559049606, \"iteration\": 1497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005197653081268072, \"iteration\": 1498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0035326373763382435, \"iteration\": 1499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005174150224775076, \"iteration\": 1500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003415205981582403, \"iteration\": 1501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0026452315505594015, \"iteration\": 1502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007640854921191931, \"iteration\": 1503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006588974967598915, \"iteration\": 1504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005166231654584408, \"iteration\": 1505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0035904457326978445, \"iteration\": 1506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0023216528352349997, \"iteration\": 1507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0034999020863324404, \"iteration\": 1508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00906996801495552, \"iteration\": 1509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0033360884990543127, \"iteration\": 1510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006241408176720142, \"iteration\": 1511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0021939303260296583, \"iteration\": 1512, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03585821017622948, \"iteration\": 1513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008327268064022064, \"iteration\": 1514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009527170099318027, \"iteration\": 1515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0027377381920814514, \"iteration\": 1516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007384910713881254, \"iteration\": 1517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003496738849207759, \"iteration\": 1518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004779915325343609, \"iteration\": 1519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006818026769906282, \"iteration\": 1520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004869032651185989, \"iteration\": 1521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.016929272562265396, \"iteration\": 1522, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.023617038503289223, \"iteration\": 1523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003547993954271078, \"iteration\": 1524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008455288596451283, \"iteration\": 1525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0069388351403176785, \"iteration\": 1526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012229789979755878, \"iteration\": 1527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0023197520058602095, \"iteration\": 1528, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.022245844826102257, \"iteration\": 1529, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04135105386376381, \"iteration\": 1530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012935656122863293, \"iteration\": 1531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0074157994240522385, \"iteration\": 1532, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.054108042269945145, \"iteration\": 1533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006274920888245106, \"iteration\": 1534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002428683452308178, \"iteration\": 1535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0028085203375667334, \"iteration\": 1536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004340497311204672, \"iteration\": 1537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010244784876704216, \"iteration\": 1538, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028304757550358772, \"iteration\": 1539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0073290420696139336, \"iteration\": 1540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00372440367937088, \"iteration\": 1541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0055823978036642075, \"iteration\": 1542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01358107477426529, \"iteration\": 1543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00325215607881546, \"iteration\": 1544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002159140072762966, \"iteration\": 1545, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008084218949079514, \"iteration\": 1546, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014989984221756458, \"iteration\": 1547, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0056727007031440735, \"iteration\": 1548, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007292709778994322, \"iteration\": 1549, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005115077830851078, \"iteration\": 1550, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01172313466668129, \"iteration\": 1551, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010407821275293827, \"iteration\": 1552, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007199560292065144, \"iteration\": 1553, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008458022959530354, \"iteration\": 1554, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006461013574153185, \"iteration\": 1555, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01358321774750948, \"iteration\": 1556, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007536973804235458, \"iteration\": 1557, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0030562211759388447, \"iteration\": 1558, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.022852692753076553, \"iteration\": 1559, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.045728400349617004, \"iteration\": 1560, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0892103835940361, \"iteration\": 1561, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008578352630138397, \"iteration\": 1562, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010748185217380524, \"iteration\": 1563, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.022453036159276962, \"iteration\": 1564, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00889851339161396, \"iteration\": 1565, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003325663274154067, \"iteration\": 1566, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010914058424532413, \"iteration\": 1567, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004873975180089474, \"iteration\": 1568, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00303233927115798, \"iteration\": 1569, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006001576781272888, \"iteration\": 1570, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00530855730175972, \"iteration\": 1571, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.018615245819091797, \"iteration\": 1572, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006104066967964172, \"iteration\": 1573, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004259578883647919, \"iteration\": 1574, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004808058962225914, \"iteration\": 1575, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002754663582891226, \"iteration\": 1576, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01825181394815445, \"iteration\": 1577, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0064163813367486, \"iteration\": 1578, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00913949403911829, \"iteration\": 1579, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005180836655199528, \"iteration\": 1580, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0083770751953125, \"iteration\": 1581, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004994350951164961, \"iteration\": 1582, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002433956600725651, \"iteration\": 1583, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006903692148625851, \"iteration\": 1584, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007488076575100422, \"iteration\": 1585, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00617465004324913, \"iteration\": 1586, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.020736465230584145, \"iteration\": 1587, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004714412614703178, \"iteration\": 1588, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0037097169551998377, \"iteration\": 1589, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010221228003501892, \"iteration\": 1590, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04426862671971321, \"iteration\": 1591, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004788318648934364, \"iteration\": 1592, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005947137251496315, \"iteration\": 1593, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0065180035308003426, \"iteration\": 1594, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003089904086664319, \"iteration\": 1595, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003663425100967288, \"iteration\": 1596, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002880040556192398, \"iteration\": 1597, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0036691315472126007, \"iteration\": 1598, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006215418688952923, \"iteration\": 1599, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0036391783505678177, \"iteration\": 1600, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005674767307937145, \"iteration\": 1601, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01775544323027134, \"iteration\": 1602, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004968712572008371, \"iteration\": 1603, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00347609119489789, \"iteration\": 1604, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005438315216451883, \"iteration\": 1605, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006063299719244242, \"iteration\": 1606, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00800182856619358, \"iteration\": 1607, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010610067285597324, \"iteration\": 1608, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003161646192893386, \"iteration\": 1609, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015958863077685237, \"iteration\": 1610, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006367441266775131, \"iteration\": 1611, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.015153970569372177, \"iteration\": 1612, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0031277635134756565, \"iteration\": 1613, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0037791042122989893, \"iteration\": 1614, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007817083038389683, \"iteration\": 1615, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003310716012492776, \"iteration\": 1616, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008999563753604889, \"iteration\": 1617, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006799306720495224, \"iteration\": 1618, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.025677837431430817, \"iteration\": 1619, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005427242256700993, \"iteration\": 1620, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01046273298561573, \"iteration\": 1621, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005111037753522396, \"iteration\": 1622, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008443368598818779, \"iteration\": 1623, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0045000300742685795, \"iteration\": 1624, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0023942687548696995, \"iteration\": 1625, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005681169684976339, \"iteration\": 1626, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0014633340761065483, \"iteration\": 1627, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.015659265220165253, \"iteration\": 1628, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006609504576772451, \"iteration\": 1629, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004625603556632996, \"iteration\": 1630, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003711023135110736, \"iteration\": 1631, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005657258443534374, \"iteration\": 1632, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0031505292281508446, \"iteration\": 1633, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005103602074086666, \"iteration\": 1634, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005496692378073931, \"iteration\": 1635, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004075920674949884, \"iteration\": 1636, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0024871043860912323, \"iteration\": 1637, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.034802865236997604, \"iteration\": 1638, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010597702115774155, \"iteration\": 1639, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003928310237824917, \"iteration\": 1640, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003700614208355546, \"iteration\": 1641, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008482594043016434, \"iteration\": 1642, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005354105960577726, \"iteration\": 1643, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009719301015138626, \"iteration\": 1644, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00433860020712018, \"iteration\": 1645, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0032522669062018394, \"iteration\": 1646, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005396841559559107, \"iteration\": 1647, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003990922588855028, \"iteration\": 1648, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003843937534838915, \"iteration\": 1649, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005111017730087042, \"iteration\": 1650, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008031177334487438, \"iteration\": 1651, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003470544470474124, \"iteration\": 1652, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005642282776534557, \"iteration\": 1653, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0028941778000444174, \"iteration\": 1654, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0030532185919582844, \"iteration\": 1655, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.03476061299443245, \"iteration\": 1656, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0020741478074342012, \"iteration\": 1657, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003777863224968314, \"iteration\": 1658, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01593664102256298, \"iteration\": 1659, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002654850482940674, \"iteration\": 1660, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006592746824026108, \"iteration\": 1661, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005207464564591646, \"iteration\": 1662, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007184748072177172, \"iteration\": 1663, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0023954107891768217, \"iteration\": 1664, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002684633247554302, \"iteration\": 1665, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003991360776126385, \"iteration\": 1666, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006469810847193003, \"iteration\": 1667, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0036398926749825478, \"iteration\": 1668, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003179637249559164, \"iteration\": 1669, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004512717016041279, \"iteration\": 1670, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005372524727135897, \"iteration\": 1671, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00040079030441120267, \"iteration\": 1672, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0011475636856630445, \"iteration\": 1673, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0028315556701272726, \"iteration\": 1674, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0046160463243722916, \"iteration\": 1675, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017807111144065857, \"iteration\": 1676, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008948906906880438, \"iteration\": 1677, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017729272367432714, \"iteration\": 1678, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029762873891741037, \"iteration\": 1679, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00173128058668226, \"iteration\": 1680, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015386911109089851, \"iteration\": 1681, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0019125993130728602, \"iteration\": 1682, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014151009963825345, \"iteration\": 1683, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00145044329110533, \"iteration\": 1684, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005143298185430467, \"iteration\": 1685, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00319665321148932, \"iteration\": 1686, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014334018342196941, \"iteration\": 1687, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0021491239313036203, \"iteration\": 1688, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029016374610364437, \"iteration\": 1689, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0037766757886856794, \"iteration\": 1690, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004329721909016371, \"iteration\": 1691, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004164144862443209, \"iteration\": 1692, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013879076577723026, \"iteration\": 1693, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012047700583934784, \"iteration\": 1694, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001266039558686316, \"iteration\": 1695, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001233331160619855, \"iteration\": 1696, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0021135066635906696, \"iteration\": 1697, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0019591401796787977, \"iteration\": 1698, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001310282270424068, \"iteration\": 1699, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0024698199704289436, \"iteration\": 1700, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018880966817960143, \"iteration\": 1701, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0022270220797508955, \"iteration\": 1702, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001102855079807341, \"iteration\": 1703, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014357028994709253, \"iteration\": 1704, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014214512193575501, \"iteration\": 1705, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016371544916182756, \"iteration\": 1706, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010582907125353813, \"iteration\": 1707, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0037611667066812515, \"iteration\": 1708, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.014239930547773838, \"iteration\": 1709, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005338264163583517, \"iteration\": 1710, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002564286347478628, \"iteration\": 1711, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0022606889251619577, \"iteration\": 1712, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017361952923238277, \"iteration\": 1713, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023840018548071384, \"iteration\": 1714, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0027623255737125874, \"iteration\": 1715, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0033977667335420847, \"iteration\": 1716, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011117819231003523, \"iteration\": 1717, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005154829006642103, \"iteration\": 1718, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04418737441301346, \"iteration\": 1719, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011387155391275883, \"iteration\": 1720, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018788146553561091, \"iteration\": 1721, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008004421833902597, \"iteration\": 1722, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0028329172637313604, \"iteration\": 1723, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018370311008766294, \"iteration\": 1724, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010373730910941958, \"iteration\": 1725, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.016831154003739357, \"iteration\": 1726, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.010408441536128521, \"iteration\": 1727, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009940763702616096, \"iteration\": 1728, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004765381570905447, \"iteration\": 1729, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.007861134596168995, \"iteration\": 1730, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002212895080447197, \"iteration\": 1731, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00177611387334764, \"iteration\": 1732, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018480225699022412, \"iteration\": 1733, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0021184543147683144, \"iteration\": 1734, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001304088975302875, \"iteration\": 1735, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017270211828872561, \"iteration\": 1736, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020863469690084457, \"iteration\": 1737, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004724050406366587, \"iteration\": 1738, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008684844360686839, \"iteration\": 1739, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.053771279752254486, \"iteration\": 1740, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.007252944633364677, \"iteration\": 1741, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002066371962428093, \"iteration\": 1742, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004166741389781237, \"iteration\": 1743, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004805668722838163, \"iteration\": 1744, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006407391279935837, \"iteration\": 1745, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002085220767185092, \"iteration\": 1746, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001554244663566351, \"iteration\": 1747, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0021205698139965534, \"iteration\": 1748, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013968420680612326, \"iteration\": 1749, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003391070757061243, \"iteration\": 1750, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003792444011196494, \"iteration\": 1751, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0030217082239687443, \"iteration\": 1752, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009312609909102321, \"iteration\": 1753, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011970050400123, \"iteration\": 1754, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018074458930641413, \"iteration\": 1755, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023353383876383305, \"iteration\": 1756, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003117665182799101, \"iteration\": 1757, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012857008259743452, \"iteration\": 1758, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015871329233050346, \"iteration\": 1759, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016233526403084397, \"iteration\": 1760, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007750623626634479, \"iteration\": 1761, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008692083065398037, \"iteration\": 1762, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0024325973354279995, \"iteration\": 1763, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017501484835520387, \"iteration\": 1764, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.008318436332046986, \"iteration\": 1765, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0067123291082680225, \"iteration\": 1766, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0035659312270581722, \"iteration\": 1767, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0025776862166821957, \"iteration\": 1768, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009073942201212049, \"iteration\": 1769, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004315831232815981, \"iteration\": 1770, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016426953952759504, \"iteration\": 1771, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000824030430521816, \"iteration\": 1772, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001822354388423264, \"iteration\": 1773, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005392671562731266, \"iteration\": 1774, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002373702824115753, \"iteration\": 1775, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001417644671164453, \"iteration\": 1776, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018088960787281394, \"iteration\": 1777, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002386702923104167, \"iteration\": 1778, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007820679456926882, \"iteration\": 1779, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009470082586631179, \"iteration\": 1780, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009923145407810807, \"iteration\": 1781, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029588215984404087, \"iteration\": 1782, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006022509653121233, \"iteration\": 1783, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014199323486536741, \"iteration\": 1784, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013333107344806194, \"iteration\": 1785, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014898168155923486, \"iteration\": 1786, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020724309142678976, \"iteration\": 1787, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00189646752551198, \"iteration\": 1788, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012530474923551083, \"iteration\": 1789, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011944555444642901, \"iteration\": 1790, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020122635178267956, \"iteration\": 1791, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000762964366003871, \"iteration\": 1792, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013780285371467471, \"iteration\": 1793, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0024032392539083958, \"iteration\": 1794, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000965416373219341, \"iteration\": 1795, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0022980535868555307, \"iteration\": 1796, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003672816092148423, \"iteration\": 1797, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007449003751389682, \"iteration\": 1798, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009446273325011134, \"iteration\": 1799, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0019983137026429176, \"iteration\": 1800, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015641538193449378, \"iteration\": 1801, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011399710783734918, \"iteration\": 1802, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020172332879155874, \"iteration\": 1803, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004959750920534134, \"iteration\": 1804, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014877530047670007, \"iteration\": 1805, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006743390113115311, \"iteration\": 1806, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004738007206469774, \"iteration\": 1807, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003681546659208834, \"iteration\": 1808, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012618814362213016, \"iteration\": 1809, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008142659207805991, \"iteration\": 1810, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008424996049143374, \"iteration\": 1811, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013111585285514593, \"iteration\": 1812, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008750837878324091, \"iteration\": 1813, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006923952605575323, \"iteration\": 1814, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011587454937398434, \"iteration\": 1815, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00073202297789976, \"iteration\": 1816, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0024193162098526955, \"iteration\": 1817, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009907576022669673, \"iteration\": 1818, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05000997334718704, \"iteration\": 1819, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014607837656512856, \"iteration\": 1820, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010241076815873384, \"iteration\": 1821, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011013755574822426, \"iteration\": 1822, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004778323054779321, \"iteration\": 1823, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0026841829530894756, \"iteration\": 1824, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0031962143257260323, \"iteration\": 1825, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0019817347638309, \"iteration\": 1826, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009114470449276268, \"iteration\": 1827, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002412390662357211, \"iteration\": 1828, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006858504377305508, \"iteration\": 1829, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017807536059990525, \"iteration\": 1830, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001576700247824192, \"iteration\": 1831, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008923302520997822, \"iteration\": 1832, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013117397902533412, \"iteration\": 1833, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014457431389018893, \"iteration\": 1834, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005651740939356387, \"iteration\": 1835, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020975142251700163, \"iteration\": 1836, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013353596441447735, \"iteration\": 1837, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029728924855589867, \"iteration\": 1838, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004137576092034578, \"iteration\": 1839, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001051505794748664, \"iteration\": 1840, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018512840615585446, \"iteration\": 1841, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004681618884205818, \"iteration\": 1842, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014510295586660504, \"iteration\": 1843, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004057416692376137, \"iteration\": 1844, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023999493569135666, \"iteration\": 1845, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018000513082370162, \"iteration\": 1846, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017530538607388735, \"iteration\": 1847, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0025852930266410112, \"iteration\": 1848, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010510114952921867, \"iteration\": 1849, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0022785416804254055, \"iteration\": 1850, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006693579256534576, \"iteration\": 1851, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000830122095067054, \"iteration\": 1852, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011581790167838335, \"iteration\": 1853, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001478229183703661, \"iteration\": 1854, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004309372045099735, \"iteration\": 1855, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011372185545042157, \"iteration\": 1856, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009432588703930378, \"iteration\": 1857, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.039044253528118134, \"iteration\": 1858, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010593150509521365, \"iteration\": 1859, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002640213817358017, \"iteration\": 1860, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010946661932393909, \"iteration\": 1861, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016449923859909177, \"iteration\": 1862, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00130617490503937, \"iteration\": 1863, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023183904122561216, \"iteration\": 1864, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010019202018156648, \"iteration\": 1865, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029885582625865936, \"iteration\": 1866, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007099342765286565, \"iteration\": 1867, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007733171805739403, \"iteration\": 1868, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002891040872782469, \"iteration\": 1869, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001819714903831482, \"iteration\": 1870, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013763767201453447, \"iteration\": 1871, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000664090330246836, \"iteration\": 1872, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010601398535072803, \"iteration\": 1873, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017158747650682926, \"iteration\": 1874, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001724428148008883, \"iteration\": 1875, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08084453642368317, \"iteration\": 1876, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004658557299990207, \"iteration\": 1877, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017738310853019357, \"iteration\": 1878, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.013651848770678043, \"iteration\": 1879, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008827165002003312, \"iteration\": 1880, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003102629678323865, \"iteration\": 1881, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007722011068835855, \"iteration\": 1882, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006076298304833472, \"iteration\": 1883, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015874292002990842, \"iteration\": 1884, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0046684336848556995, \"iteration\": 1885, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005883734207600355, \"iteration\": 1886, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002966024447232485, \"iteration\": 1887, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011961706914007664, \"iteration\": 1888, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0012662119697779417, \"iteration\": 1889, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006678837817162275, \"iteration\": 1890, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02418489009141922, \"iteration\": 1891, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007673877407796681, \"iteration\": 1892, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010447022505104542, \"iteration\": 1893, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0029055641498416662, \"iteration\": 1894, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001861388562247157, \"iteration\": 1895, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001144757610745728, \"iteration\": 1896, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014361115172505379, \"iteration\": 1897, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0024514924734830856, \"iteration\": 1898, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00567296426743269, \"iteration\": 1899, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015475910622626543, \"iteration\": 1900, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009384598815813661, \"iteration\": 1901, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008091793861240149, \"iteration\": 1902, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017587796319276094, \"iteration\": 1903, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010757852578535676, \"iteration\": 1904, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00263995211571455, \"iteration\": 1905, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016701758140698075, \"iteration\": 1906, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010508237173780799, \"iteration\": 1907, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0024586645886301994, \"iteration\": 1908, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008791431318968534, \"iteration\": 1909, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010161882964894176, \"iteration\": 1910, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007285913452506065, \"iteration\": 1911, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009496661368757486, \"iteration\": 1912, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00101109454408288, \"iteration\": 1913, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007239191909320652, \"iteration\": 1914, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006094241980463266, \"iteration\": 1915, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006725005223415792, \"iteration\": 1916, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02337951958179474, \"iteration\": 1917, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008798696799203753, \"iteration\": 1918, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004905291134491563, \"iteration\": 1919, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005431764875538647, \"iteration\": 1920, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0021385345607995987, \"iteration\": 1921, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008242643089033663, \"iteration\": 1922, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006653055315837264, \"iteration\": 1923, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008049079915508628, \"iteration\": 1924, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006081194151192904, \"iteration\": 1925, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0012450682697817683, \"iteration\": 1926, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013614451745525002, \"iteration\": 1927, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005528859910555184, \"iteration\": 1928, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00042853126069530845, \"iteration\": 1929, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005693312850780785, \"iteration\": 1930, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009842353174462914, \"iteration\": 1931, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004868981195613742, \"iteration\": 1932, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0012481958838179708, \"iteration\": 1933, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00047781653120182455, \"iteration\": 1934, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006710191955789924, \"iteration\": 1935, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005486023146659136, \"iteration\": 1936, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010491380235180259, \"iteration\": 1937, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010467983083799481, \"iteration\": 1938, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02604551799595356, \"iteration\": 1939, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010891376296058297, \"iteration\": 1940, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007205597357824445, \"iteration\": 1941, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006196395261213183, \"iteration\": 1942, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005279253236949444, \"iteration\": 1943, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001198030193336308, \"iteration\": 1944, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000777674256823957, \"iteration\": 1945, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00030188041273504496, \"iteration\": 1946, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007270135101862252, \"iteration\": 1947, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006377325626090169, \"iteration\": 1948, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005692353006452322, \"iteration\": 1949, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003915655426681042, \"iteration\": 1950, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00034190938458777964, \"iteration\": 1951, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008302575442939997, \"iteration\": 1952, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000723356322851032, \"iteration\": 1953, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005301849450916052, \"iteration\": 1954, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006414763629436493, \"iteration\": 1955, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008340325439348817, \"iteration\": 1956, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005902097909711301, \"iteration\": 1957, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006192031432874501, \"iteration\": 1958, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002767512109130621, \"iteration\": 1959, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000727446167729795, \"iteration\": 1960, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004339235834777355, \"iteration\": 1961, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014754339354112744, \"iteration\": 1962, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0012031678343191743, \"iteration\": 1963, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005579427815973759, \"iteration\": 1964, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00045318089541979134, \"iteration\": 1965, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006120645557530224, \"iteration\": 1966, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003917463473044336, \"iteration\": 1967, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007774187834002078, \"iteration\": 1968, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008655650890432298, \"iteration\": 1969, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005033822380937636, \"iteration\": 1970, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0025304777082055807, \"iteration\": 1971, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009325320716015995, \"iteration\": 1972, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006168787949718535, \"iteration\": 1973, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002558815467637032, \"iteration\": 1974, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005087194731459022, \"iteration\": 1975, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005793718155473471, \"iteration\": 1976, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010753850219771266, \"iteration\": 1977, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007559949299320579, \"iteration\": 1978, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009074978297576308, \"iteration\": 1979, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00034472791594453156, \"iteration\": 1980, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005753816803917289, \"iteration\": 1981, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004863895010203123, \"iteration\": 1982, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004400969482958317, \"iteration\": 1983, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003868779633194208, \"iteration\": 1984, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0012061038287356496, \"iteration\": 1985, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013822638429701328, \"iteration\": 1986, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005407554563134909, \"iteration\": 1987, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005659773014485836, \"iteration\": 1988, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005367831327021122, \"iteration\": 1989, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008675023564137518, \"iteration\": 1990, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00093391805421561, \"iteration\": 1991, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007994906045496464, \"iteration\": 1992, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004649711772799492, \"iteration\": 1993, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005252320552244782, \"iteration\": 1994, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004027724207844585, \"iteration\": 1995, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0032198575790971518, \"iteration\": 1996, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023782416246831417, \"iteration\": 1997, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008601367590017617, \"iteration\": 1998, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006710559828206897, \"iteration\": 1999, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004292392695788294, \"iteration\": 2000, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009310946916230023, \"iteration\": 2001, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00042472826316952705, \"iteration\": 2002, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000402732752263546, \"iteration\": 2003, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002239846158772707, \"iteration\": 2004, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00038456416223198175, \"iteration\": 2005, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00041104431147687137, \"iteration\": 2006, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005455886130221188, \"iteration\": 2007, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007590959430672228, \"iteration\": 2008, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006466316990554333, \"iteration\": 2009, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008669768576510251, \"iteration\": 2010, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005009662709198892, \"iteration\": 2011, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007473304867744446, \"iteration\": 2012, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024230487179011106, \"iteration\": 2013, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006672091549262404, \"iteration\": 2014, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007901502540335059, \"iteration\": 2015, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0029960519168525934, \"iteration\": 2016, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004014854202978313, \"iteration\": 2017, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006298307562246919, \"iteration\": 2018, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00040432787500321865, \"iteration\": 2019, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002507582539692521, \"iteration\": 2020, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006058497237972915, \"iteration\": 2021, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008176593692041934, \"iteration\": 2022, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007201217231340706, \"iteration\": 2023, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00046621556975878775, \"iteration\": 2024, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00047193386126309633, \"iteration\": 2025, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005207304493524134, \"iteration\": 2026, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.00878651812672615, \"iteration\": 2027, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004198671376798302, \"iteration\": 2028, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000609684269875288, \"iteration\": 2029, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000712198147084564, \"iteration\": 2030, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010198922827839851, \"iteration\": 2031, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00038264531758613884, \"iteration\": 2032, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008109316695481539, \"iteration\": 2033, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.01266756746917963, \"iteration\": 2034, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004306057817302644, \"iteration\": 2035, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005393873434513807, \"iteration\": 2036, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011163181625306606, \"iteration\": 2037, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004351913812570274, \"iteration\": 2038, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004525036201812327, \"iteration\": 2039, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005138934357091784, \"iteration\": 2040, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000764617114327848, \"iteration\": 2041, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000482995412312448, \"iteration\": 2042, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00040887025534175336, \"iteration\": 2043, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019381395541131496, \"iteration\": 2044, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0074971579015254974, \"iteration\": 2045, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009378764661960304, \"iteration\": 2046, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005016610957682133, \"iteration\": 2047, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00043760318658314645, \"iteration\": 2048, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003452832461334765, \"iteration\": 2049, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003447448543738574, \"iteration\": 2050, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00039224783540703356, \"iteration\": 2051, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005257894517853856, \"iteration\": 2052, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006562062189914286, \"iteration\": 2053, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005012925248593092, \"iteration\": 2054, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005519990809261799, \"iteration\": 2055, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002237862441688776, \"iteration\": 2056, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008287106757052243, \"iteration\": 2057, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013647471787407994, \"iteration\": 2058, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006950460374355316, \"iteration\": 2059, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003156895691063255, \"iteration\": 2060, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009801449486985803, \"iteration\": 2061, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006799971451982856, \"iteration\": 2062, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000506036274600774, \"iteration\": 2063, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011950219050049782, \"iteration\": 2064, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009713599574752152, \"iteration\": 2065, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004491035651881248, \"iteration\": 2066, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004957066848874092, \"iteration\": 2067, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00047647167230024934, \"iteration\": 2068, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007740253931842744, \"iteration\": 2069, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00042109627975150943, \"iteration\": 2070, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00036812061443924904, \"iteration\": 2071, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004886609967797995, \"iteration\": 2072, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003455422993283719, \"iteration\": 2073, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006602652720175683, \"iteration\": 2074, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009744764538481832, \"iteration\": 2075, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000242001871811226, \"iteration\": 2076, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00036515542888082564, \"iteration\": 2077, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006058375583961606, \"iteration\": 2078, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008988440968096256, \"iteration\": 2079, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004171760519966483, \"iteration\": 2080, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006613839068450034, \"iteration\": 2081, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004934079479426146, \"iteration\": 2082, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003298523079138249, \"iteration\": 2083, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008996452670544386, \"iteration\": 2084, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002878932747989893, \"iteration\": 2085, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004017575702164322, \"iteration\": 2086, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003823490987997502, \"iteration\": 2087, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009205032256431878, \"iteration\": 2088, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00048731762217357755, \"iteration\": 2089, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009628810221329331, \"iteration\": 2090, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"Prepare data...\")\n",
    "test_nn_chars = encode_data(test_raw, chars_encoder)\n",
    "\n",
    "logger.info(\"Train model\")\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    ")\n",
    "\n",
    "USE_CACHE = True\n",
    "\n",
    "model_nn_chars = NeuralNetwork(\n",
    "    input_size=len(chars_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn_chars.pt').exists() and USE_CACHE:\n",
    "    model_nn_chars = load_model(model_nn_chars, models_dir, 'model_nn_chars')\n",
    "else:\n",
    "    train_nn_chars = encode_data(train_raw, chars_encoder)\n",
    "    dataloader = DataLoader(train_nn_chars, batch_size=128, shuffle=True)\n",
    "\n",
    "    model_nn_chars.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn_chars, models_dir, \"model_nn_chars\")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    X_test = torch.stack([test[0] for test in test_nn_chars]).to(model_nn_chars.device)\n",
    "    y_test = torch.stack([test[1] for test in test_nn_chars]).to(model_nn_chars.device)\n",
    "    y_pred = model_nn_chars.predict(X_test)\n",
    "    logits = model_nn_chars.forward(X_test)\n",
    "\n",
    "result_nn_chars = evaluate(y_test.cpu(), y_pred.cpu(), logits.cpu())\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(\"model_nn_chars - Training accuracy & Loss\", model_nn_chars, train_config, dataloader)\n",
    "\n",
    "# Move model to CPU to save CUDA memory\n",
    "model_nn_words.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "### Word feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger.info(\"Prepare data encoder...\")\n",
    "rnn_words_encoder = PositionalEncoder()\n",
    "rnn_words_encoder.fit(train_raw.texts)\n",
    "\n",
    "train_dataloader = DataLoader(train_raw, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_raw, batch_size=128, shuffle=False)\n",
    "\n",
    "# Prepare baseline config\n",
    "train_config = TrainConfig(\n",
    "    optimizer_params = {'lr': 0.01},\n",
    "    num_epochs       = 10,\n",
    "    early_stop       = False,\n",
    "    violation_limit  = 5\n",
    ")\n",
    "\n",
    "# Train baseline model\n",
    "model_lstm_words = RNNClassifier(\n",
    "    rnn_network         = nn.LSTM,\n",
    "    word_embedding_dim  = 32,\n",
    "    hidden_dim          = 64,\n",
    "    bidirectional       = False,\n",
    "    dropout             = 0,\n",
    "    encoder             = rnn_words_encoder,\n",
    "    device              = 'cuda'\n",
    ")\n",
    "\n",
    "USE_CACHE = True\n",
    "\n",
    "if (models_dir / 'model_lstm_words.pt').exists() and USE_CACHE:\n",
    "    model_lstm_words = load_model(model_lstm_words, 'model_lstm_words')\n",
    "else:\n",
    "    model_lstm_words.fit(train_dataloader, train_config, no_progress_bar=False)\n",
    "    save_model(model_lstm_words, models_dir, \"model_lstm_words\")\n",
    "\n",
    "test_dataloader = DataLoader(test_raw, batch_size=128, shuffle=False)\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    model_lstm_words.device = \"cpu\"\n",
    "    model_lstm_words.cpu()\n",
    "\n",
    "    pred_LSTM_words_lst = []\n",
    "    probs_LSTM_words_lst = []\n",
    "\n",
    "    for _, _, raw_inputs, raw_targets in tqdm(test_dataloader, unit=\"batch\", desc=\"Predicting\"):\n",
    "        batch_encoder = PositionalEncoder(vocabulary=rnn_words_encoder.vocabulary)\n",
    "        test_inputs = batch_encoder.fit_transform(raw_inputs).cpu()\n",
    "        test_targets = torch.as_tensor(raw_targets, dtype=torch.float).cpu()\n",
    "        \n",
    "        pred_LSTM_words_lst.append(model_lstm_words.predict(test_inputs))\n",
    "        probs_LSTM_words_lst.append(model_lstm_words._sigmoid(model_lstm_words.forward(test_inputs)).squeeze())\n",
    "\n",
    "\n",
    "pred_LSTM_words = torch.cat(pred_LSTM_words_lst).long().numpy()\n",
    "probs_LSTM_words = torch.concat(probs_LSTM_words_lst).numpy()\n",
    "\n",
    "model_lstm_words_result = evaluate(test_raw.labels, pred_LSTM_words, probs_LSTM_words)\n",
    "\n",
    "np.save(models_dir / 'model_lstm_words_results.npy', model_lstm_words_result)\n",
    "\n",
    "model_lstm_words.cpu()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger.info(\"Prepare data encoder...\")\n",
    "rnn_chars_encoder = PositionalEncoder(tokenizer=chars_encoder.build_tokenizer())\n",
    "rnn_chars_encoder.fit(train_raw.texts)\n",
    "\n",
    "train_dataloader = DataLoader(train_raw, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_raw, batch_size=128, shuffle=False)\n",
    "\n",
    "test_inputs = rnn_chars_encoder.transform(test_raw.texts)\n",
    "\n",
    "# Prepare baseline config\n",
    "train_config = TrainConfig(\n",
    "    optimizer_params = {'lr': 0.01},\n",
    "    num_epochs       = 10,\n",
    "    early_stop       = False,\n",
    "    violation_limit  = 5\n",
    ")\n",
    "\n",
    "# Train baseline model\n",
    "model_lstm_chars = RNNClassifier(\n",
    "    rnn_network         = nn.LSTM,\n",
    "    word_embedding_dim  = 32,\n",
    "    hidden_dim          = 64,\n",
    "    bidirectional       = False,\n",
    "    dropout             = 0,\n",
    "    encoder             = rnn_chars_encoder,\n",
    "    device              = 'cuda'\n",
    ")\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "if (models_dir / 'model_lstm_chars.pt').exists() and USE_CACHE:\n",
    "    model_lstm_chars = load_model(model_lstm_chars, 'model_lstm_chars')\n",
    "else:\n",
    "    model_lstm_chars.fit(train_dataloader, train_config, no_progress_bar=False)\n",
    "    save_model(model_lstm_chars, models_dir, \"model_lstm_chars\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_lstm_chars.device = \"cpu\"\n",
    "    model_lstm_chars.cpu()\n",
    "\n",
    "    pred_LSTM_chars = []\n",
    "    logits_LSTM_chars = []\n",
    "\n",
    "    for _, _, raw_inputs, raw_targets in tqdm(test_dataloader, unit=\"batch\", desc=\"Predicting\"):\n",
    "        batch_encoder = PositionalEncoder(vocabulary=rnn_chars_encoder.vocabulary)\n",
    "        test_inputs = batch_encoder.fit_transform(raw_inputs).cpu()\n",
    "        test_targets = torch.as_tensor(raw_targets, dtype=torch.float).cpu()\n",
    "\n",
    "        pred_LSTM_chars.append(model_lstm_chars.predict(test_inputs))\n",
    "        logits_LSTM_chars.append(model_lstm_chars.forward(test_inputs))\n",
    "\n",
    "pred_LSTM_chars = torch.concat(pred_LSTM_chars).numpy()\n",
    "logits_LSTM_chars = torch.concat(logits_LSTM_chars).numpy()\n",
    "\n",
    "model_lstm_chars_result = evaluate(test_raw.labels, pred_LSTM_chars, logits_LSTM_chars)\n",
    "\n",
    "np.save(models_dir / 'model_lstm_chars_results.npy', model_lstm_chars_result)\n",
    "model_lstm_words.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other classifiers from sklearn\n",
    "\n",
    "Models tested: \n",
    "- LinearSVC\n",
    "- LogisticRegression\n",
    "- SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train & test set\n",
    "X_train_skl_words = words_encoder.transform(train_raw.texts)\n",
    "X_test_skl_words = words_encoder.transform(test_raw.texts)\n",
    "\n",
    "X_train_skl_chars = chars_encoder.transform(train_raw.texts)\n",
    "X_test_skl_chars = chars_encoder.transform(test_raw.texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC\n",
    "Effective in high dimensional spaces.\n",
    "\n",
    "Still effective in cases where number of dimensions is greater than the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC with TfIdf did good on balanced English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# LinearSVC, word\n",
    "\n",
    "\n",
    "logger.info(\"Fit model\")\n",
    "base_svc = LinearSVC()\n",
    "model_LinearSVC_words = CalibratedClassifierCV(estimator=base_svc, cv=5)\n",
    "model_LinearSVC_words.fit(X_train_skl_words, train_raw.labels)\n",
    "\n",
    "pred_LinearSVC_words = model_LinearSVC_words.predict(X_test_skl_words)\n",
    "logits_linearSVC_words = model_LinearSVC_words.predict_proba(X_test_skl_words)\n",
    "\n",
    "result_linearSVC_words =  evaluate(test_raw.labels, pred_LinearSVC_words, logits_linearSVC_words[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "logger.info(\"Fit model\")\n",
    "base_svc = LinearSVC()\n",
    "model_LinearSVC_chars = CalibratedClassifierCV(estimator=base_svc, cv=5)\n",
    "model_LinearSVC_chars.fit(X_train_skl_chars, train_raw.labels)\n",
    "\n",
    "pred_LinearSVC_chars = model_LinearSVC_chars.predict(X_test_skl_chars)\n",
    "logits_linearSVC_chars = model_LinearSVC_chars.predict_proba(X_test_skl_chars)\n",
    "\n",
    "result_linearSVC_chars =  evaluate(test_raw.labels, pred_LinearSVC_chars, logits_linearSVC_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logger.info(\"Fit model\")\n",
    "model_logreg_words = LogisticRegression()\n",
    "model_logreg_words.fit(X_train_skl_words, train_raw.labels)\n",
    "\n",
    "pred_logreg_words = model_logreg_words.predict(X_test_skl_words)\n",
    "logits_logreg_words = model_logreg_words.predict_proba(X_test_skl_words)\n",
    "\n",
    "result_linearlogreg_words =  evaluate(test_raw.labels, pred_logreg_words, logits_logreg_words[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logger.info(\"Fit model\")\n",
    "model_logreg_chars = LogisticRegression()\n",
    "model_logreg_chars.fit(X_train_skl_chars, train_raw.labels)\n",
    "\n",
    "pred_logreg_chars = model_logreg_chars.predict(X_test_skl_chars)\n",
    "logits_logreg_chars = model_logreg_chars.predict_proba(X_test_skl_chars)\n",
    "\n",
    "result_linearlogreg_chars = evaluate(test_raw.labels, pred_logreg_chars, logits_logreg_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGDClassifier\n",
    "SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "\n",
    "SGD is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word features\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_sgd_words = SGDClassifier(loss='log_loss')\n",
    "model_sgd_words.fit(X_train_skl_words, train_raw.labels)\n",
    "\n",
    "pred_sgd_words = model_sgd_words.predict(X_test_skl_words)\n",
    "logits_sgd_words = model_sgd_words.predict_proba(X_test_skl_words)\n",
    "\n",
    "result_linearsgd_words =  evaluate(test_raw.labels, pred_sgd_words, logits_sgd_words[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars features\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_sgd_chars = SGDClassifier(loss='log_loss')\n",
    "model_sgd_chars.fit(X_train_skl_chars, train_raw.labels)\n",
    "\n",
    "pred_sgd_chars = model_sgd_chars.predict(X_test_skl_chars)\n",
    "logits_sgd_chars = model_sgd_chars.predict_proba(X_test_skl_chars)\n",
    "\n",
    "result_linearsgd_chars =  evaluate(test_raw.labels, pred_sgd_chars, logits_sgd_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Overall bad performance, not worth pursuing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words features\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_gnb_words = GaussianNB()\n",
    "model_gnb_words.fit(X_train_skl_words.toarray(), train_raw.labels)\n",
    "\n",
    "pred_gnb_words = model_gnb_words.predict(X_test_skl_words.toarray())\n",
    "logits_gnb_words = model_gnb_words.predict_proba(X_test_skl_words.toarray())\n",
    "\n",
    "result_lineargnb_words =  evaluate(test_raw.labels, pred_gnb_words, logits_gnb_words[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars features\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_gnb_chars = GaussianNB()\n",
    "model_gnb_chars.fit(X_train_skl_chars.toarray(), train_raw.labels)\n",
    "\n",
    "pred_gnb_chars = model_gnb_chars.predict(X_test_skl_chars.toarray())\n",
    "logits_gnb_chars = model_gnb_chars.predict_proba(X_test_skl_chars.toarray())\n",
    "\n",
    "result_lineargnb_chars =  evaluate(test_raw.labels, pred_gnb_chars, logits_gnb_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data for xgboost\n",
    "train_dmat_words = xgb.DMatrix(X_train_skl_words, pd.array(train_raw.labels).astype('category'))\n",
    "test_dmat_words = xgb.DMatrix(X_test_skl_words, pd.array(test_raw.labels).astype('category'))\n",
    "\n",
    "train_dmat_chars = xgb.DMatrix(X_train_skl_chars, pd.array(train_raw.labels).astype('category'))\n",
    "test_dmat_chars = xgb.DMatrix(X_test_skl_chars, pd.array(test_raw.labels).astype('category'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"device\": \"cpu\",\n",
    "    \"objective\": \"binary:logistic\",  # there is also binary:hinge but hinge does not output probability\n",
    "    \"tree_method\": \"hist\",  # default to hist\n",
    "    \"device\": \"cuda\",\n",
    "\n",
    "    # Params for tree booster\n",
    "    \"eta\": 0.3,\n",
    "    \"gamma\": 0.0,  # Min loss achieved to split the tree\n",
    "    \"max_depth\": 6,\n",
    "    \"reg_alpha\": 0,\n",
    "    \"reg_lambda\": 1,\n",
    "\n",
    "}\n",
    "evals_words = [(train_dmat_words, \"train\")]\n",
    "iterations = 2000\n",
    "\n",
    "model_xgb_words = xgb.train(\n",
    "    params = params,\n",
    "    dtrain = train_dmat_words,\n",
    "    num_boost_round = iterations,\n",
    "    evals = evals_words,\n",
    "    verbose_eval = 100\n",
    ")\n",
    "\n",
    "pred_xgb_words_probs = model_xgb_words.predict(test_dmat_words)\n",
    "result_xgb_words = evaluate(test_raw.labels, pred_xgb_words_probs > 0.5, pred_xgb_words_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use only half of the original max features\n",
    "xgb_chars_encoder = TfidfVectorizer(max_features=20000, analyzer=\"char\", ngram_range=(3,5), use_idf=True, sublinear_tf=True)\n",
    "xgb_chars_encoder.fit(train_raw.texts)\n",
    "\n",
    "# Prepare train & test set\n",
    "X_train_xgb_chars = xgb_chars_encoder.transform(train_raw.texts)\n",
    "X_test_xgb_chars = xgb_chars_encoder.transform(test_raw.texts)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "train_dmat_chars = xgb.DMatrix(X_train_xgb_chars, pd.array(train_raw.labels).astype('category'))\n",
    "test_dmat_chars = xgb.DMatrix(X_test_xgb_chars, pd.array(test_raw.labels).astype('category'))\n",
    "\n",
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"device\": \"cpu\",\n",
    "    \"objective\": \"binary:logistic\",  # there is also binary:hinge but hinge does not output probability\n",
    "    \"tree_method\": \"hist\",  # default to hist\n",
    "    \"device\": \"cuda\",\n",
    "\n",
    "    # Params for tree booster\n",
    "    \"eta\": 0.3,\n",
    "    \"gamma\": 0.0,  # Min loss achieved to split the tree\n",
    "    \"max_depth\": 6,\n",
    "    \"reg_alpha\": 0,\n",
    "    \"reg_lambda\": 1,\n",
    "\n",
    "}\n",
    "evals_chars = [(train_dmat_chars, \"train\")]\n",
    "iterations = 2000\n",
    "\n",
    "model_xgb_chars = xgb.train(\n",
    "    params = params,\n",
    "    dtrain = train_dmat_chars,\n",
    "    num_boost_round = iterations,\n",
    "    evals = evals_chars,\n",
    "    verbose_eval = 100\n",
    ")\n",
    "\n",
    "pred_xgb_chars_probs = model_xgb_chars.predict(test_dmat_chars)\n",
    "result_xgb_chars = evaluate(test_raw.labels, pred_xgb_chars_probs > 0.5, pred_xgb_chars_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-identification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

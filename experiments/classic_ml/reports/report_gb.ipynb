{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Total: 4,091.48 MB\n",
      "Reserved: 0.00 MB\n",
      "Allocated: 0.00 MB\n",
      "Free in reserved: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from models import NeuralNetwork, RNNClassifier, TrainConfig, evaluate, save_model, load_model, plot_results\n",
    "from utils import load_data, split_data, encode_data, check_cuda_memory, PositionalEncoder\n",
    "from pathlib import Path\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device: cuda\")\n",
    "    check_cuda_memory()\n",
    "else:\n",
    "    print(\"Device: cpu\")\n",
    "\n",
    "models_dir = Path('models/gb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 33257, % positive class: 56.39%\n"
     ]
    }
   ],
   "source": [
    "data = load_data(folder_path=\"data/train/power/\", file_list=['power-gb-train.tsv'],text_head='text_en')\n",
    "train_raw, test_raw = split_data(data, test_size=0.2, random_state=0)\n",
    "print(f\"Data size: {len(data)}, % positive class: {sum(data.labels) / len(data) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare words_encoder...\n",
      "Prepare chars_encoder...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, max_features=50000, ngram_range=(3, 5),\n",
       "                sublinear_tf=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, max_features=50000, ngram_range=(3, 5),\n",
       "                sublinear_tf=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(analyzer='char', max_features=50000, ngram_range=(3, 5),\n",
       "                sublinear_tf=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"Prepare words_encoder...\")\n",
    "words_encoder = TfidfVectorizer(max_features=50000)\n",
    "words_encoder.fit(train_raw.texts)\n",
    "\n",
    "print(\"Prepare chars_encoder...\")\n",
    "chars_encoder = TfidfVectorizer(max_features=50000, analyzer=\"char\", ngram_range=(3,5), use_idf=True, sublinear_tf=True)\n",
    "chars_encoder.fit(train_raw.texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data...\n",
      "Train model\n",
      "Accuracy: 0.7030, Precision: 0.7256, Recall: 0.7530, F1: 0.7391, AUC: 0.7781\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-f5f15d2f5d2f4d2c9de324e363208cba.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-f5f15d2f5d2f4d2c9de324e363208cba.vega-embed details,\n",
       "  #altair-viz-f5f15d2f5d2f4d2c9de324e363208cba.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-f5f15d2f5d2f4d2c9de324e363208cba\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f5f15d2f5d2f4d2c9de324e363208cba\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f5f15d2f5d2f4d2c9de324e363208cba\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-b5ceb128c396e31757cb56500764f7e1\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-b5ceb128c396e31757cb56500764f7e1\": [{\"training_acc\": 0.578125, \"training_loss\": 0.6928210854530334, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6927599310874939, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6913156509399414, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6905341148376465, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6921643018722534, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6886473894119263, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6907979249954224, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 0.6932523250579834, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6886590123176575, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 0.6911232471466064, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6815435886383057, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6900491714477539, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6814974546432495, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6813751459121704, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6760205030441284, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6775662899017334, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6759200692176819, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6672992706298828, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 0.6886138916015625, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6823589205741882, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6608538031578064, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6777998208999634, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6856748461723328, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6771410703659058, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6579996347427368, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.673478901386261, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6558278799057007, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6745773553848267, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6648664474487305, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.653499186038971, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6604929566383362, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6427059173583984, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6485670208930969, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.631697416305542, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6172294616699219, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 0.675791323184967, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6459453105926514, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6569175720214844, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6577154994010925, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.639750599861145, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6384697556495667, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.6121202707290649, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.6101042032241821, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.6071162223815918, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.6476186513900757, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.6171332597732544, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.632793664932251, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.6082767248153687, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5986759066581726, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6140221357345581, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5827536582946777, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.5638171434402466, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6539703607559204, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5977973341941833, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5771817564964294, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5798864960670471, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.565788745880127, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5734493136405945, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5559821724891663, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5493029356002808, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.5305653810501099, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5418747067451477, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5217863321304321, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5442982912063599, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5211137533187866, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6332716941833496, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5612279176712036, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5159449577331543, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5130170583724976, \"iteration\": 69, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5063984394073486, \"iteration\": 70, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5419347882270813, \"iteration\": 71, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5642355680465698, \"iteration\": 72, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5583540201187134, \"iteration\": 73, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5416989922523499, \"iteration\": 74, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5597817897796631, \"iteration\": 75, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5038669109344482, \"iteration\": 76, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4971317946910858, \"iteration\": 77, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.49518001079559326, \"iteration\": 78, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5164076089859009, \"iteration\": 79, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.46041420102119446, \"iteration\": 80, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5149716138839722, \"iteration\": 81, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.509466826915741, \"iteration\": 82, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5560035705566406, \"iteration\": 83, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4302501678466797, \"iteration\": 84, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5202511548995972, \"iteration\": 85, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5185558199882507, \"iteration\": 86, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5070643424987793, \"iteration\": 87, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5195769667625427, \"iteration\": 88, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5385397672653198, \"iteration\": 89, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.564728856086731, \"iteration\": 90, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5063198804855347, \"iteration\": 91, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.48237311840057373, \"iteration\": 92, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4770057201385498, \"iteration\": 93, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5460780262947083, \"iteration\": 94, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4223982095718384, \"iteration\": 95, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5361627340316772, \"iteration\": 96, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4473651051521301, \"iteration\": 97, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5464087724685669, \"iteration\": 98, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.44956207275390625, \"iteration\": 99, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5551714897155762, \"iteration\": 100, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5125131607055664, \"iteration\": 101, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4900512993335724, \"iteration\": 102, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.44505006074905396, \"iteration\": 103, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4967188835144043, \"iteration\": 104, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4938775300979614, \"iteration\": 105, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.44465845823287964, \"iteration\": 106, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5156700015068054, \"iteration\": 107, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4732722043991089, \"iteration\": 108, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5438292622566223, \"iteration\": 109, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.4492599368095398, \"iteration\": 110, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5629897117614746, \"iteration\": 111, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5127586722373962, \"iteration\": 112, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.550248920917511, \"iteration\": 113, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4962685704231262, \"iteration\": 114, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.446663498878479, \"iteration\": 115, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4875791370868683, \"iteration\": 116, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5136157870292664, \"iteration\": 117, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5246157646179199, \"iteration\": 118, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.39702922105789185, \"iteration\": 119, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.46448615193367004, \"iteration\": 120, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4807198643684387, \"iteration\": 121, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4652596712112427, \"iteration\": 122, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5095233917236328, \"iteration\": 123, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.48195183277130127, \"iteration\": 124, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.46844929456710815, \"iteration\": 125, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.40471410751342773, \"iteration\": 126, \"epoch\": 1}, {\"training_acc\": 0.859375, \"training_loss\": 0.39626264572143555, \"iteration\": 127, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.4413284361362457, \"iteration\": 128, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.44783803820610046, \"iteration\": 129, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5031490325927734, \"iteration\": 130, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5118229389190674, \"iteration\": 131, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4886033236980438, \"iteration\": 132, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.48297226428985596, \"iteration\": 133, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4478905498981476, \"iteration\": 134, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4454084634780884, \"iteration\": 135, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5026845335960388, \"iteration\": 136, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5208034515380859, \"iteration\": 137, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.4840788245201111, \"iteration\": 138, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5000153183937073, \"iteration\": 139, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4312034249305725, \"iteration\": 140, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4534071683883667, \"iteration\": 141, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5494952201843262, \"iteration\": 142, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.3932817578315735, \"iteration\": 143, \"epoch\": 1}, {\"training_acc\": 0.859375, \"training_loss\": 0.4031895101070404, \"iteration\": 144, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.4805402159690857, \"iteration\": 145, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.4897357225418091, \"iteration\": 146, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4743565618991852, \"iteration\": 147, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.4624802768230438, \"iteration\": 148, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4199884533882141, \"iteration\": 149, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.43224194645881653, \"iteration\": 150, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.4280235469341278, \"iteration\": 151, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4589442014694214, \"iteration\": 152, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.49763548374176025, \"iteration\": 153, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.4795016050338745, \"iteration\": 154, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.507090151309967, \"iteration\": 155, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5164312720298767, \"iteration\": 156, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.557826042175293, \"iteration\": 157, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5465211868286133, \"iteration\": 158, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.46058374643325806, \"iteration\": 159, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.4609168469905853, \"iteration\": 160, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.4760560989379883, \"iteration\": 161, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4996269941329956, \"iteration\": 162, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5962061285972595, \"iteration\": 163, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4696422815322876, \"iteration\": 164, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.530349612236023, \"iteration\": 165, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.4569128751754761, \"iteration\": 166, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4696652889251709, \"iteration\": 167, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4175993800163269, \"iteration\": 168, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.465544193983078, \"iteration\": 169, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4157121777534485, \"iteration\": 170, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5620733499526978, \"iteration\": 171, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.40758374333381653, \"iteration\": 172, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6470232605934143, \"iteration\": 173, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4949721395969391, \"iteration\": 174, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4688897728919983, \"iteration\": 175, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4226641058921814, \"iteration\": 176, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4994077682495117, \"iteration\": 177, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4662988781929016, \"iteration\": 178, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4917774200439453, \"iteration\": 179, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.49492472410202026, \"iteration\": 180, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4077380299568176, \"iteration\": 181, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.48808783292770386, \"iteration\": 182, \"epoch\": 1}, {\"training_acc\": 0.890625, \"training_loss\": 0.374245285987854, \"iteration\": 183, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5335899591445923, \"iteration\": 184, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.587104856967926, \"iteration\": 185, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.515178918838501, \"iteration\": 186, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.519301176071167, \"iteration\": 187, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.46218636631965637, \"iteration\": 188, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4715271294116974, \"iteration\": 189, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4320656657218933, \"iteration\": 190, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4478723406791687, \"iteration\": 191, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4262692332267761, \"iteration\": 192, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4696034789085388, \"iteration\": 193, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.444757342338562, \"iteration\": 194, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.49569711089134216, \"iteration\": 195, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4904414415359497, \"iteration\": 196, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.476184606552124, \"iteration\": 197, \"epoch\": 1}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3997403681278229, \"iteration\": 198, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5162614583969116, \"iteration\": 199, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.48973673582077026, \"iteration\": 200, \"epoch\": 1}, {\"training_acc\": 0.859375, \"training_loss\": 0.4258260726928711, \"iteration\": 201, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.46452027559280396, \"iteration\": 202, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.416526734828949, \"iteration\": 203, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.4898201823234558, \"iteration\": 204, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5031236410140991, \"iteration\": 205, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.43987032771110535, \"iteration\": 206, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4583328366279602, \"iteration\": 207, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.43054670095443726, \"iteration\": 208, \"epoch\": 1}, {\"training_acc\": 0.8571428571428571, \"training_loss\": 0.4913794994354248, \"iteration\": 209, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.35122570395469666, \"iteration\": 210, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3227396607398987, \"iteration\": 211, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.33418017625808716, \"iteration\": 212, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2651902139186859, \"iteration\": 213, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.37379953265190125, \"iteration\": 214, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.362868070602417, \"iteration\": 215, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.31655335426330566, \"iteration\": 216, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3487566113471985, \"iteration\": 217, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.31711676716804504, \"iteration\": 218, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2795003056526184, \"iteration\": 219, \"epoch\": 2}, {\"training_acc\": 0.953125, \"training_loss\": 0.2508474290370941, \"iteration\": 220, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2929952144622803, \"iteration\": 221, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.30149203538894653, \"iteration\": 222, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.35583949089050293, \"iteration\": 223, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.342828631401062, \"iteration\": 224, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.27749043703079224, \"iteration\": 225, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2697197198867798, \"iteration\": 226, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3290588855743408, \"iteration\": 227, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.31443867087364197, \"iteration\": 228, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.32594120502471924, \"iteration\": 229, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3277662992477417, \"iteration\": 230, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2656381130218506, \"iteration\": 231, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22474901378154755, \"iteration\": 232, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3319852352142334, \"iteration\": 233, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2715759873390198, \"iteration\": 234, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3129956126213074, \"iteration\": 235, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.32935985922813416, \"iteration\": 236, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3120496869087219, \"iteration\": 237, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.33137214183807373, \"iteration\": 238, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.32371169328689575, \"iteration\": 239, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2607927918434143, \"iteration\": 240, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.32099226117134094, \"iteration\": 241, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2845373749732971, \"iteration\": 242, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.33628252148628235, \"iteration\": 243, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3501197099685669, \"iteration\": 244, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2625213861465454, \"iteration\": 245, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.32447385787963867, \"iteration\": 246, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3000762462615967, \"iteration\": 247, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2894078195095062, \"iteration\": 248, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2894707918167114, \"iteration\": 249, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22128258645534515, \"iteration\": 250, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2467869222164154, \"iteration\": 251, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3552429676055908, \"iteration\": 252, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26099085807800293, \"iteration\": 253, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2540530562400818, \"iteration\": 254, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.25419723987579346, \"iteration\": 255, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3094624876976013, \"iteration\": 256, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.26113051176071167, \"iteration\": 257, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2539746165275574, \"iteration\": 258, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2715161442756653, \"iteration\": 259, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.25488314032554626, \"iteration\": 260, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2611532211303711, \"iteration\": 261, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2314072698354721, \"iteration\": 262, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25734615325927734, \"iteration\": 263, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.28341782093048096, \"iteration\": 264, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2407594472169876, \"iteration\": 265, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2900732159614563, \"iteration\": 266, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18998382985591888, \"iteration\": 267, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.268340140581131, \"iteration\": 268, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4103572964668274, \"iteration\": 269, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.25578564405441284, \"iteration\": 270, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30260956287384033, \"iteration\": 271, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25330784916877747, \"iteration\": 272, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20952782034873962, \"iteration\": 273, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3395729660987854, \"iteration\": 274, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3302435874938965, \"iteration\": 275, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.22789056599140167, \"iteration\": 276, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3372712731361389, \"iteration\": 277, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3502765893936157, \"iteration\": 278, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3033244013786316, \"iteration\": 279, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2793133854866028, \"iteration\": 280, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2753412127494812, \"iteration\": 281, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.45221245288848877, \"iteration\": 282, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3216167688369751, \"iteration\": 283, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3307608962059021, \"iteration\": 284, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.34383639693260193, \"iteration\": 285, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3060563802719116, \"iteration\": 286, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3801576495170593, \"iteration\": 287, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3142772912979126, \"iteration\": 288, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.21889501810073853, \"iteration\": 289, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3103826940059662, \"iteration\": 290, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.37106603384017944, \"iteration\": 291, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3042587339878082, \"iteration\": 292, \"epoch\": 2}, {\"training_acc\": 0.953125, \"training_loss\": 0.21768303215503693, \"iteration\": 293, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2302742898464203, \"iteration\": 294, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.29390618205070496, \"iteration\": 295, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.35795003175735474, \"iteration\": 296, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.27894487977027893, \"iteration\": 297, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.29441019892692566, \"iteration\": 298, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3344016671180725, \"iteration\": 299, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.32373884320259094, \"iteration\": 300, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3351656496524811, \"iteration\": 301, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.3136480450630188, \"iteration\": 302, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.26580673456192017, \"iteration\": 303, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.2874208688735962, \"iteration\": 304, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.36653637886047363, \"iteration\": 305, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3190222978591919, \"iteration\": 306, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2547774016857147, \"iteration\": 307, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2606940269470215, \"iteration\": 308, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2941237688064575, \"iteration\": 309, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2823987603187561, \"iteration\": 310, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3028688132762909, \"iteration\": 311, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2588731050491333, \"iteration\": 312, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.28048834204673767, \"iteration\": 313, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.22858452796936035, \"iteration\": 314, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2443186193704605, \"iteration\": 315, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3832494914531708, \"iteration\": 316, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3054236173629761, \"iteration\": 317, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3319302797317505, \"iteration\": 318, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3236827850341797, \"iteration\": 319, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.305383563041687, \"iteration\": 320, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3088279366493225, \"iteration\": 321, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20744524896144867, \"iteration\": 322, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.36152514815330505, \"iteration\": 323, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.279967725276947, \"iteration\": 324, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2484220266342163, \"iteration\": 325, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3650813102722168, \"iteration\": 326, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3193488121032715, \"iteration\": 327, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30916377902030945, \"iteration\": 328, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.35653722286224365, \"iteration\": 329, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.19263377785682678, \"iteration\": 330, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.23585200309753418, \"iteration\": 331, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2554638981819153, \"iteration\": 332, \"epoch\": 2}, {\"training_acc\": 0.9453125, \"training_loss\": 0.23348329961299896, \"iteration\": 333, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3230038583278656, \"iteration\": 334, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.34926530718803406, \"iteration\": 335, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3231511414051056, \"iteration\": 336, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.29304999113082886, \"iteration\": 337, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.21497204899787903, \"iteration\": 338, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.26568982005119324, \"iteration\": 339, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.39911314845085144, \"iteration\": 340, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.21221935749053955, \"iteration\": 341, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3047941327095032, \"iteration\": 342, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.23777425289154053, \"iteration\": 343, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.30894848704338074, \"iteration\": 344, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.29216480255126953, \"iteration\": 345, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2751884162425995, \"iteration\": 346, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.21896788477897644, \"iteration\": 347, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2731497287750244, \"iteration\": 348, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.26235178112983704, \"iteration\": 349, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2711813151836395, \"iteration\": 350, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.322338342666626, \"iteration\": 351, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.29143184423446655, \"iteration\": 352, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2827906012535095, \"iteration\": 353, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4038233757019043, \"iteration\": 354, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22017592191696167, \"iteration\": 355, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.36492031812667847, \"iteration\": 356, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30836230516433716, \"iteration\": 357, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.27666187286376953, \"iteration\": 358, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3896259069442749, \"iteration\": 359, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3220306932926178, \"iteration\": 360, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2911578416824341, \"iteration\": 361, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.34509018063545227, \"iteration\": 362, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2601945400238037, \"iteration\": 363, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.31208962202072144, \"iteration\": 364, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3453316390514374, \"iteration\": 365, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3326950669288635, \"iteration\": 366, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.27747437357902527, \"iteration\": 367, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3805162310600281, \"iteration\": 368, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2446267008781433, \"iteration\": 369, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3065919578075409, \"iteration\": 370, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.23120775818824768, \"iteration\": 371, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.38364940881729126, \"iteration\": 372, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2920151352882385, \"iteration\": 373, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4049986004829407, \"iteration\": 374, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.31046414375305176, \"iteration\": 375, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27800464630126953, \"iteration\": 376, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.28419890999794006, \"iteration\": 377, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30239975452423096, \"iteration\": 378, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4587428867816925, \"iteration\": 379, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.28710314631462097, \"iteration\": 380, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3010566532611847, \"iteration\": 381, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.37951236963272095, \"iteration\": 382, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3174823522567749, \"iteration\": 383, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.3170572817325592, \"iteration\": 384, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2974132299423218, \"iteration\": 385, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3469341993331909, \"iteration\": 386, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.28279656171798706, \"iteration\": 387, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.29148465394973755, \"iteration\": 388, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.24896174669265747, \"iteration\": 389, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.2955629825592041, \"iteration\": 390, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.31974542140960693, \"iteration\": 391, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3751274049282074, \"iteration\": 392, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2951841652393341, \"iteration\": 393, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3259608745574951, \"iteration\": 394, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.28699731826782227, \"iteration\": 395, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.333645224571228, \"iteration\": 396, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3135778605937958, \"iteration\": 397, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.29971766471862793, \"iteration\": 398, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.30855923891067505, \"iteration\": 399, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.32204848527908325, \"iteration\": 400, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3021867275238037, \"iteration\": 401, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26476675271987915, \"iteration\": 402, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3557564914226532, \"iteration\": 403, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2917270064353943, \"iteration\": 404, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.35061734914779663, \"iteration\": 405, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.31121599674224854, \"iteration\": 406, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.33327507972717285, \"iteration\": 407, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.33101585507392883, \"iteration\": 408, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.28900718688964844, \"iteration\": 409, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.29285502433776855, \"iteration\": 410, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2863769829273224, \"iteration\": 411, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.38483062386512756, \"iteration\": 412, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.29895931482315063, \"iteration\": 413, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.32248708605766296, \"iteration\": 414, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3360559642314911, \"iteration\": 415, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.37403184175491333, \"iteration\": 416, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3858591318130493, \"iteration\": 417, \"epoch\": 2}, {\"training_acc\": 1.0, \"training_loss\": 0.23765408992767334, \"iteration\": 418, \"epoch\": 2}, {\"training_acc\": 0.953125, \"training_loss\": 0.19052639603614807, \"iteration\": 419, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17575126886367798, \"iteration\": 420, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14669226109981537, \"iteration\": 421, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.13105034828186035, \"iteration\": 422, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17448866367340088, \"iteration\": 423, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14584079384803772, \"iteration\": 424, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2025974690914154, \"iteration\": 425, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.13191485404968262, \"iteration\": 426, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.16201256215572357, \"iteration\": 427, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19038048386573792, \"iteration\": 428, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1470140814781189, \"iteration\": 429, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.26179349422454834, \"iteration\": 430, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16672813892364502, \"iteration\": 431, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22336405515670776, \"iteration\": 432, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.169783353805542, \"iteration\": 433, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1670527458190918, \"iteration\": 434, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.11947418749332428, \"iteration\": 435, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.2032291442155838, \"iteration\": 436, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12055665254592896, \"iteration\": 437, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1638777256011963, \"iteration\": 438, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.1936819851398468, \"iteration\": 439, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20594507455825806, \"iteration\": 440, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1967988908290863, \"iteration\": 441, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.16027608513832092, \"iteration\": 442, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18428823351860046, \"iteration\": 443, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.11598813533782959, \"iteration\": 444, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.1122245341539383, \"iteration\": 445, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1365458369255066, \"iteration\": 446, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.1564377248287201, \"iteration\": 447, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.09943033009767532, \"iteration\": 448, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19173112511634827, \"iteration\": 449, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.143645778298378, \"iteration\": 450, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1727811098098755, \"iteration\": 451, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.23184937238693237, \"iteration\": 452, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08398108184337616, \"iteration\": 453, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.18875400722026825, \"iteration\": 454, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.11395197361707687, \"iteration\": 455, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09149308502674103, \"iteration\": 456, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.16131116449832916, \"iteration\": 457, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22821743786334991, \"iteration\": 458, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.164209246635437, \"iteration\": 459, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.21125157177448273, \"iteration\": 460, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1302589625120163, \"iteration\": 461, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1701129525899887, \"iteration\": 462, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27218472957611084, \"iteration\": 463, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1715300977230072, \"iteration\": 464, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15594235062599182, \"iteration\": 465, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.13041937351226807, \"iteration\": 466, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1267269402742386, \"iteration\": 467, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1568443775177002, \"iteration\": 468, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.18807633221149445, \"iteration\": 469, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.12506279349327087, \"iteration\": 470, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17402803897857666, \"iteration\": 471, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.14786794781684875, \"iteration\": 472, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15099895000457764, \"iteration\": 473, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1541157215833664, \"iteration\": 474, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.14577427506446838, \"iteration\": 475, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17888039350509644, \"iteration\": 476, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1246982142329216, \"iteration\": 477, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15824192762374878, \"iteration\": 478, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1362924575805664, \"iteration\": 479, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.12181307375431061, \"iteration\": 480, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.10685615241527557, \"iteration\": 481, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.17494124174118042, \"iteration\": 482, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19217395782470703, \"iteration\": 483, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.14566689729690552, \"iteration\": 484, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1657153069972992, \"iteration\": 485, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19124910235404968, \"iteration\": 486, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1953308880329132, \"iteration\": 487, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09122602641582489, \"iteration\": 488, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.16916236281394958, \"iteration\": 489, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17945271730422974, \"iteration\": 490, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.17172735929489136, \"iteration\": 491, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18926364183425903, \"iteration\": 492, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20763884484767914, \"iteration\": 493, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.1900637447834015, \"iteration\": 494, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.12364911288022995, \"iteration\": 495, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20716485381126404, \"iteration\": 496, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1633211374282837, \"iteration\": 497, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2664855122566223, \"iteration\": 498, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21999114751815796, \"iteration\": 499, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1623189002275467, \"iteration\": 500, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12334679067134857, \"iteration\": 501, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.24104811251163483, \"iteration\": 502, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23204882442951202, \"iteration\": 503, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.12396851927042007, \"iteration\": 504, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20545294880867004, \"iteration\": 505, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1314590871334076, \"iteration\": 506, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.12444363534450531, \"iteration\": 507, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.19743110239505768, \"iteration\": 508, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.13235832750797272, \"iteration\": 509, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.14334344863891602, \"iteration\": 510, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2199954390525818, \"iteration\": 511, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.1291969269514084, \"iteration\": 512, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15322932600975037, \"iteration\": 513, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.11574412882328033, \"iteration\": 514, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.13328781723976135, \"iteration\": 515, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.18130917847156525, \"iteration\": 516, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.18854832649230957, \"iteration\": 517, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.12177577614784241, \"iteration\": 518, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.12248408794403076, \"iteration\": 519, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.12931591272354126, \"iteration\": 520, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.1963781714439392, \"iteration\": 521, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.19047978520393372, \"iteration\": 522, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15817052125930786, \"iteration\": 523, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.24597246944904327, \"iteration\": 524, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1669648438692093, \"iteration\": 525, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.12015985697507858, \"iteration\": 526, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1956194043159485, \"iteration\": 527, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17452117800712585, \"iteration\": 528, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.21965466439723969, \"iteration\": 529, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10823924839496613, \"iteration\": 530, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.14621447026729584, \"iteration\": 531, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.15717022120952606, \"iteration\": 532, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14288249611854553, \"iteration\": 533, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15735702216625214, \"iteration\": 534, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2430831640958786, \"iteration\": 535, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1805320382118225, \"iteration\": 536, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1436137855052948, \"iteration\": 537, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.15439726412296295, \"iteration\": 538, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.22301533818244934, \"iteration\": 539, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.16703486442565918, \"iteration\": 540, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15846285223960876, \"iteration\": 541, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19273297488689423, \"iteration\": 542, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1482289433479309, \"iteration\": 543, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1962260603904724, \"iteration\": 544, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20252031087875366, \"iteration\": 545, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20193475484848022, \"iteration\": 546, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17822593450546265, \"iteration\": 547, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.16238847374916077, \"iteration\": 548, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.30666959285736084, \"iteration\": 549, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.16520956158638, \"iteration\": 550, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.13550297915935516, \"iteration\": 551, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.16410472989082336, \"iteration\": 552, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18444883823394775, \"iteration\": 553, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.19186332821846008, \"iteration\": 554, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20182164013385773, \"iteration\": 555, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2077457159757614, \"iteration\": 556, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17011617124080658, \"iteration\": 557, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.14975453913211823, \"iteration\": 558, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.14948976039886475, \"iteration\": 559, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23582977056503296, \"iteration\": 560, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2549009621143341, \"iteration\": 561, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1504298895597458, \"iteration\": 562, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.17081904411315918, \"iteration\": 563, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2287934124469757, \"iteration\": 564, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1637173891067505, \"iteration\": 565, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2328910380601883, \"iteration\": 566, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2371283769607544, \"iteration\": 567, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18915817141532898, \"iteration\": 568, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.18961143493652344, \"iteration\": 569, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17348036170005798, \"iteration\": 570, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2183903157711029, \"iteration\": 571, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.25377312302589417, \"iteration\": 572, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09636126458644867, \"iteration\": 573, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.16231165826320648, \"iteration\": 574, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22583375871181488, \"iteration\": 575, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.17273084819316864, \"iteration\": 576, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.16051970422267914, \"iteration\": 577, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2561688721179962, \"iteration\": 578, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1595471203327179, \"iteration\": 579, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.11569654196500778, \"iteration\": 580, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23734141886234283, \"iteration\": 581, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22402966022491455, \"iteration\": 582, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.20029431581497192, \"iteration\": 583, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.1765238344669342, \"iteration\": 584, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26557233929634094, \"iteration\": 585, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.16384214162826538, \"iteration\": 586, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20536541938781738, \"iteration\": 587, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30289366841316223, \"iteration\": 588, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.15709364414215088, \"iteration\": 589, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16338753700256348, \"iteration\": 590, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.234983891248703, \"iteration\": 591, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18127189576625824, \"iteration\": 592, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.22383227944374084, \"iteration\": 593, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.16968421638011932, \"iteration\": 594, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19278636574745178, \"iteration\": 595, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19783198833465576, \"iteration\": 596, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.16234903037548065, \"iteration\": 597, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17357711493968964, \"iteration\": 598, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2397184818983078, \"iteration\": 599, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15064460039138794, \"iteration\": 600, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15346065163612366, \"iteration\": 601, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17244862020015717, \"iteration\": 602, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.18110308051109314, \"iteration\": 603, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2820027470588684, \"iteration\": 604, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20883777737617493, \"iteration\": 605, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18951693177223206, \"iteration\": 606, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.30609840154647827, \"iteration\": 607, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.16149422526359558, \"iteration\": 608, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2392922192811966, \"iteration\": 609, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1820191890001297, \"iteration\": 610, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1945400834083557, \"iteration\": 611, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18114027380943298, \"iteration\": 612, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.179389089345932, \"iteration\": 613, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2955441474914551, \"iteration\": 614, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15565550327301025, \"iteration\": 615, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1660192757844925, \"iteration\": 616, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1862579882144928, \"iteration\": 617, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1619311273097992, \"iteration\": 618, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.15099550783634186, \"iteration\": 619, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1726388931274414, \"iteration\": 620, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.17331361770629883, \"iteration\": 621, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18456555902957916, \"iteration\": 622, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.21409070491790771, \"iteration\": 623, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17150984704494476, \"iteration\": 624, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18209517002105713, \"iteration\": 625, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.16548886895179749, \"iteration\": 626, \"epoch\": 3}, {\"training_acc\": 1.0, \"training_loss\": 0.02837502583861351, \"iteration\": 627, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12404845654964447, \"iteration\": 628, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07768139243125916, \"iteration\": 629, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06941930204629898, \"iteration\": 630, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.05969470739364624, \"iteration\": 631, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06607353687286377, \"iteration\": 632, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.13475652039051056, \"iteration\": 633, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.05914872884750366, \"iteration\": 634, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09233053028583527, \"iteration\": 635, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06955504417419434, \"iteration\": 636, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11390725523233414, \"iteration\": 637, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06088583171367645, \"iteration\": 638, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07616910338401794, \"iteration\": 639, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06606312841176987, \"iteration\": 640, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09293301403522491, \"iteration\": 641, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04525379464030266, \"iteration\": 642, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09392476081848145, \"iteration\": 643, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08438242226839066, \"iteration\": 644, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07038228213787079, \"iteration\": 645, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.05609305948019028, \"iteration\": 646, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09723813831806183, \"iteration\": 647, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.12020702660083771, \"iteration\": 648, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07363972067832947, \"iteration\": 649, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04211008548736572, \"iteration\": 650, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06512688100337982, \"iteration\": 651, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09932121634483337, \"iteration\": 652, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11018338799476624, \"iteration\": 653, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.049284107983112335, \"iteration\": 654, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10201816260814667, \"iteration\": 655, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.052859317511320114, \"iteration\": 656, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.040268585085868835, \"iteration\": 657, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0598805733025074, \"iteration\": 658, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.03904655575752258, \"iteration\": 659, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07909108698368073, \"iteration\": 660, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0893278419971466, \"iteration\": 661, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06802977621555328, \"iteration\": 662, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.06296101212501526, \"iteration\": 663, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12956376373767853, \"iteration\": 664, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05076763778924942, \"iteration\": 665, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09226427972316742, \"iteration\": 666, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10295267403125763, \"iteration\": 667, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08663661032915115, \"iteration\": 668, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.061729371547698975, \"iteration\": 669, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08236218988895416, \"iteration\": 670, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1064542829990387, \"iteration\": 671, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0840969979763031, \"iteration\": 672, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10328475385904312, \"iteration\": 673, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1311028152704239, \"iteration\": 674, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06356531381607056, \"iteration\": 675, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10222280025482178, \"iteration\": 676, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.04832810163497925, \"iteration\": 677, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07300084084272385, \"iteration\": 678, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.03886125981807709, \"iteration\": 679, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10748837888240814, \"iteration\": 680, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09845209121704102, \"iteration\": 681, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.05679700896143913, \"iteration\": 682, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04687601327896118, \"iteration\": 683, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.043072301894426346, \"iteration\": 684, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07499487698078156, \"iteration\": 685, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09332697093486786, \"iteration\": 686, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04926454648375511, \"iteration\": 687, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09147137403488159, \"iteration\": 688, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.05588541179895401, \"iteration\": 689, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0854281485080719, \"iteration\": 690, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.06185973435640335, \"iteration\": 691, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1242307499051094, \"iteration\": 692, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15373960137367249, \"iteration\": 693, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06215440481901169, \"iteration\": 694, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10402537882328033, \"iteration\": 695, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.05159382149577141, \"iteration\": 696, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11973181366920471, \"iteration\": 697, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1167282685637474, \"iteration\": 698, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07261806726455688, \"iteration\": 699, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12093768268823624, \"iteration\": 700, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08467556536197662, \"iteration\": 701, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.05154196545481682, \"iteration\": 702, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08655025064945221, \"iteration\": 703, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.02959764003753662, \"iteration\": 704, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07651001960039139, \"iteration\": 705, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10124468058347702, \"iteration\": 706, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14132145047187805, \"iteration\": 707, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11190595477819443, \"iteration\": 708, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12988975644111633, \"iteration\": 709, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08354876190423965, \"iteration\": 710, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11753304302692413, \"iteration\": 711, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08132411539554596, \"iteration\": 712, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07934213429689407, \"iteration\": 713, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.045253247022628784, \"iteration\": 714, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10369289666414261, \"iteration\": 715, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07375110685825348, \"iteration\": 716, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.052626121789216995, \"iteration\": 717, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0505312941968441, \"iteration\": 718, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07216648012399673, \"iteration\": 719, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.061180491000413895, \"iteration\": 720, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0695706307888031, \"iteration\": 721, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10434412211179733, \"iteration\": 722, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05341778323054314, \"iteration\": 723, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0987522229552269, \"iteration\": 724, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1471480429172516, \"iteration\": 725, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.061949849128723145, \"iteration\": 726, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08496718108654022, \"iteration\": 727, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.05434329807758331, \"iteration\": 728, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10372012853622437, \"iteration\": 729, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.15371476113796234, \"iteration\": 730, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.13457629084587097, \"iteration\": 731, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06098336726427078, \"iteration\": 732, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08371854573488235, \"iteration\": 733, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07234051078557968, \"iteration\": 734, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10339135676622391, \"iteration\": 735, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.06375657021999359, \"iteration\": 736, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17407265305519104, \"iteration\": 737, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.13448932766914368, \"iteration\": 738, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0637132078409195, \"iteration\": 739, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06379177421331406, \"iteration\": 740, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04644718021154404, \"iteration\": 741, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05287718400359154, \"iteration\": 742, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0776139497756958, \"iteration\": 743, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.03804481029510498, \"iteration\": 744, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07245166599750519, \"iteration\": 745, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.06862770020961761, \"iteration\": 746, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.01913977973163128, \"iteration\": 747, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07716874778270721, \"iteration\": 748, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0904068648815155, \"iteration\": 749, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12476946413516998, \"iteration\": 750, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.15186695754528046, \"iteration\": 751, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10012466460466385, \"iteration\": 752, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.06169278547167778, \"iteration\": 753, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05301712825894356, \"iteration\": 754, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0749809741973877, \"iteration\": 755, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.06646318733692169, \"iteration\": 756, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09625466167926788, \"iteration\": 757, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04761943221092224, \"iteration\": 758, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07209602743387222, \"iteration\": 759, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06299740821123123, \"iteration\": 760, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08441398292779922, \"iteration\": 761, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0972370058298111, \"iteration\": 762, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11776015162467957, \"iteration\": 763, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.041143156588077545, \"iteration\": 764, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.15522027015686035, \"iteration\": 765, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11956683546304703, \"iteration\": 766, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.09955491125583649, \"iteration\": 767, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1556820273399353, \"iteration\": 768, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09847534447908401, \"iteration\": 769, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10979844629764557, \"iteration\": 770, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07517953217029572, \"iteration\": 771, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09800262749195099, \"iteration\": 772, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.030356280505657196, \"iteration\": 773, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.05828053504228592, \"iteration\": 774, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09272263944149017, \"iteration\": 775, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04751019924879074, \"iteration\": 776, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13964764773845673, \"iteration\": 777, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10853859782218933, \"iteration\": 778, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.05509299412369728, \"iteration\": 779, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10610617697238922, \"iteration\": 780, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11813519150018692, \"iteration\": 781, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1463671326637268, \"iteration\": 782, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04583258554339409, \"iteration\": 783, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08774000406265259, \"iteration\": 784, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12187354266643524, \"iteration\": 785, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.034238990396261215, \"iteration\": 786, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.04575499892234802, \"iteration\": 787, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11133662611246109, \"iteration\": 788, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0574009045958519, \"iteration\": 789, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10719238966703415, \"iteration\": 790, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06983748078346252, \"iteration\": 791, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10602819919586182, \"iteration\": 792, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1008332222700119, \"iteration\": 793, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0679212138056755, \"iteration\": 794, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.18556714057922363, \"iteration\": 795, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09382042288780212, \"iteration\": 796, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1292351931333542, \"iteration\": 797, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07619060575962067, \"iteration\": 798, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08896590769290924, \"iteration\": 799, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08106319606304169, \"iteration\": 800, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08225316554307938, \"iteration\": 801, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05170751363039017, \"iteration\": 802, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15745899081230164, \"iteration\": 803, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08743572980165482, \"iteration\": 804, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09944739192724228, \"iteration\": 805, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19396057724952698, \"iteration\": 806, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11774950474500656, \"iteration\": 807, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.13626611232757568, \"iteration\": 808, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.2040046900510788, \"iteration\": 809, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0785398781299591, \"iteration\": 810, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.08507411181926727, \"iteration\": 811, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11382575333118439, \"iteration\": 812, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08819685876369476, \"iteration\": 813, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10852250456809998, \"iteration\": 814, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10586243122816086, \"iteration\": 815, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.10959011316299438, \"iteration\": 816, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1053902730345726, \"iteration\": 817, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.13260367512702942, \"iteration\": 818, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.06843484938144684, \"iteration\": 819, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1050381064414978, \"iteration\": 820, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07077345252037048, \"iteration\": 821, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11403819173574448, \"iteration\": 822, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1068042442202568, \"iteration\": 823, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12910841405391693, \"iteration\": 824, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11981114000082016, \"iteration\": 825, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09345309436321259, \"iteration\": 826, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09832043200731277, \"iteration\": 827, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1296783685684204, \"iteration\": 828, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09657620638608932, \"iteration\": 829, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07705462723970413, \"iteration\": 830, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1042228490114212, \"iteration\": 831, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09942159056663513, \"iteration\": 832, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03572826832532883, \"iteration\": 833, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0776480957865715, \"iteration\": 834, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07060694694519043, \"iteration\": 835, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.03993307426571846, \"iteration\": 836, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.028929028660058975, \"iteration\": 837, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.019451014697551727, \"iteration\": 838, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.04051605612039566, \"iteration\": 839, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06226745992898941, \"iteration\": 840, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04663170501589775, \"iteration\": 841, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.032748423516750336, \"iteration\": 842, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.04307791590690613, \"iteration\": 843, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04156964644789696, \"iteration\": 844, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02000965178012848, \"iteration\": 845, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020192589610815048, \"iteration\": 846, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.030099382624030113, \"iteration\": 847, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.028401095420122147, \"iteration\": 848, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.043342314660549164, \"iteration\": 849, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0308049526065588, \"iteration\": 850, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.013874411582946777, \"iteration\": 851, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05900902673602104, \"iteration\": 852, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.038106195628643036, \"iteration\": 853, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.032553207129240036, \"iteration\": 854, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.04718443751335144, \"iteration\": 855, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03466698154807091, \"iteration\": 856, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.028592532500624657, \"iteration\": 857, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06106347218155861, \"iteration\": 858, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018107831478118896, \"iteration\": 859, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018419813364744186, \"iteration\": 860, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04329540580511093, \"iteration\": 861, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06621835380792618, \"iteration\": 862, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01837330311536789, \"iteration\": 863, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.021950943395495415, \"iteration\": 864, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02858607843518257, \"iteration\": 865, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023525111377239227, \"iteration\": 866, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.040106192231178284, \"iteration\": 867, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.052019089460372925, \"iteration\": 868, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.015247957780957222, \"iteration\": 869, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04988888278603554, \"iteration\": 870, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022066442295908928, \"iteration\": 871, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.012590231373906136, \"iteration\": 872, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03187292814254761, \"iteration\": 873, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06419769674539566, \"iteration\": 874, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017624765634536743, \"iteration\": 875, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04212424159049988, \"iteration\": 876, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05302167683839798, \"iteration\": 877, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05949955806136131, \"iteration\": 878, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01854853704571724, \"iteration\": 879, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04071052744984627, \"iteration\": 880, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.032068781554698944, \"iteration\": 881, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017794325947761536, \"iteration\": 882, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020414669066667557, \"iteration\": 883, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01028578169643879, \"iteration\": 884, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.030405402183532715, \"iteration\": 885, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06504034250974655, \"iteration\": 886, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.056839220225811005, \"iteration\": 887, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09365236014127731, \"iteration\": 888, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03366059809923172, \"iteration\": 889, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.027390852570533752, \"iteration\": 890, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06584599614143372, \"iteration\": 891, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02068403549492359, \"iteration\": 892, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02287081815302372, \"iteration\": 893, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.021489303559064865, \"iteration\": 894, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.041142918169498444, \"iteration\": 895, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03170178458094597, \"iteration\": 896, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.029333971440792084, \"iteration\": 897, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022817635908722878, \"iteration\": 898, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06337136775255203, \"iteration\": 899, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.033519577234983444, \"iteration\": 900, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0459783673286438, \"iteration\": 901, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02029505930840969, \"iteration\": 902, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01919391378760338, \"iteration\": 903, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023515164852142334, \"iteration\": 904, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03465821593999863, \"iteration\": 905, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.011457400396466255, \"iteration\": 906, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.024916063994169235, \"iteration\": 907, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05715147405862808, \"iteration\": 908, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01282692514359951, \"iteration\": 909, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02369821071624756, \"iteration\": 910, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02137334644794464, \"iteration\": 911, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06406054645776749, \"iteration\": 912, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.008858960121870041, \"iteration\": 913, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01203683391213417, \"iteration\": 914, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07059541344642639, \"iteration\": 915, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02497001364827156, \"iteration\": 916, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020647255703806877, \"iteration\": 917, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.04073809087276459, \"iteration\": 918, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08172422647476196, \"iteration\": 919, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02824321761727333, \"iteration\": 920, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.012687886133790016, \"iteration\": 921, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03086625412106514, \"iteration\": 922, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022993862628936768, \"iteration\": 923, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021295586600899696, \"iteration\": 924, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01979650929570198, \"iteration\": 925, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.024189915508031845, \"iteration\": 926, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.021378381177783012, \"iteration\": 927, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03200507164001465, \"iteration\": 928, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06510861217975616, \"iteration\": 929, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.04312392324209213, \"iteration\": 930, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.028439387679100037, \"iteration\": 931, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01731450855731964, \"iteration\": 932, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.010977236554026604, \"iteration\": 933, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.029909733682870865, \"iteration\": 934, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.008995840325951576, \"iteration\": 935, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02070688083767891, \"iteration\": 936, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.026416976004838943, \"iteration\": 937, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01298636756837368, \"iteration\": 938, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02237672358751297, \"iteration\": 939, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01979122683405876, \"iteration\": 940, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04568181559443474, \"iteration\": 941, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02489337883889675, \"iteration\": 942, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026668976992368698, \"iteration\": 943, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.009830165654420853, \"iteration\": 944, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0232803113758564, \"iteration\": 945, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027472425252199173, \"iteration\": 946, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.015617084689438343, \"iteration\": 947, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02018108405172825, \"iteration\": 948, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1166851669549942, \"iteration\": 949, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.021587252616882324, \"iteration\": 950, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03668273240327835, \"iteration\": 951, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0123855359852314, \"iteration\": 952, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00913443323224783, \"iteration\": 953, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020722612738609314, \"iteration\": 954, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08078721165657043, \"iteration\": 955, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03867878019809723, \"iteration\": 956, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.010228459723293781, \"iteration\": 957, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.010987362824380398, \"iteration\": 958, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06941203027963638, \"iteration\": 959, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07029016315937042, \"iteration\": 960, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03528592735528946, \"iteration\": 961, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0232992023229599, \"iteration\": 962, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.045184455811977386, \"iteration\": 963, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03538801521062851, \"iteration\": 964, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.040410254150629044, \"iteration\": 965, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020017221570014954, \"iteration\": 966, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018367599695920944, \"iteration\": 967, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.025598714128136635, \"iteration\": 968, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11241661012172699, \"iteration\": 969, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020532172173261642, \"iteration\": 970, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.024241063743829727, \"iteration\": 971, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.011557206511497498, \"iteration\": 972, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.051090046763420105, \"iteration\": 973, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01115415245294571, \"iteration\": 974, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020156845450401306, \"iteration\": 975, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07783813029527664, \"iteration\": 976, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04205162823200226, \"iteration\": 977, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.029712971299886703, \"iteration\": 978, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.014185645617544651, \"iteration\": 979, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05118056386709213, \"iteration\": 980, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020330261439085007, \"iteration\": 981, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04323398321866989, \"iteration\": 982, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03641059249639511, \"iteration\": 983, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.04896899685263634, \"iteration\": 984, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04700763523578644, \"iteration\": 985, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.015751317143440247, \"iteration\": 986, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027849815785884857, \"iteration\": 987, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03599496930837631, \"iteration\": 988, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09912890195846558, \"iteration\": 989, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.009183543734252453, \"iteration\": 990, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10416796058416367, \"iteration\": 991, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08526068180799484, \"iteration\": 992, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03236432746052742, \"iteration\": 993, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.12991322576999664, \"iteration\": 994, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.016984660178422928, \"iteration\": 995, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.043138567358255386, \"iteration\": 996, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04295159876346588, \"iteration\": 997, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06439708918333054, \"iteration\": 998, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.014333564788103104, \"iteration\": 999, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.00925871916115284, \"iteration\": 1000, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.044909536838531494, \"iteration\": 1001, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027422890067100525, \"iteration\": 1002, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.026874303817749023, \"iteration\": 1003, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07663512229919434, \"iteration\": 1004, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01868661679327488, \"iteration\": 1005, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.030559903010725975, \"iteration\": 1006, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.019723303616046906, \"iteration\": 1007, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02540937066078186, \"iteration\": 1008, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.011645229533314705, \"iteration\": 1009, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.010469071567058563, \"iteration\": 1010, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.060979343950748444, \"iteration\": 1011, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.056088823825120926, \"iteration\": 1012, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02740699052810669, \"iteration\": 1013, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05986039340496063, \"iteration\": 1014, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.030734833329916, \"iteration\": 1015, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02847064658999443, \"iteration\": 1016, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02826385200023651, \"iteration\": 1017, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.010883654467761517, \"iteration\": 1018, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.012131323106586933, \"iteration\": 1019, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03361198678612709, \"iteration\": 1020, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.10021564364433289, \"iteration\": 1021, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.009640943259000778, \"iteration\": 1022, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02333788387477398, \"iteration\": 1023, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01972406916320324, \"iteration\": 1024, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02306853048503399, \"iteration\": 1025, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.014557055197656155, \"iteration\": 1026, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.009644214063882828, \"iteration\": 1027, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.010976467281579971, \"iteration\": 1028, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03606041148304939, \"iteration\": 1029, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07091863453388214, \"iteration\": 1030, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04598163813352585, \"iteration\": 1031, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.12410871684551239, \"iteration\": 1032, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.04109717905521393, \"iteration\": 1033, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.032232772558927536, \"iteration\": 1034, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.025210842490196228, \"iteration\": 1035, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06896950304508209, \"iteration\": 1036, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.037250716239213943, \"iteration\": 1037, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03282781317830086, \"iteration\": 1038, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05499375984072685, \"iteration\": 1039, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.015836400911211967, \"iteration\": 1040, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04805905371904373, \"iteration\": 1041, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01252998597919941, \"iteration\": 1042, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03149152547121048, \"iteration\": 1043, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018239526078104973, \"iteration\": 1044, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.009897715412080288, \"iteration\": 1045, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.013417000882327557, \"iteration\": 1046, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005096648819744587, \"iteration\": 1047, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01634194329380989, \"iteration\": 1048, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006028197705745697, \"iteration\": 1049, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006314071826636791, \"iteration\": 1050, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012425328604876995, \"iteration\": 1051, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007233916781842709, \"iteration\": 1052, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.025686878710985184, \"iteration\": 1053, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004620077554136515, \"iteration\": 1054, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005697714630514383, \"iteration\": 1055, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006281438749283552, \"iteration\": 1056, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010863794013857841, \"iteration\": 1057, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00473074009642005, \"iteration\": 1058, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008960037492215633, \"iteration\": 1059, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005922623910009861, \"iteration\": 1060, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01104709692299366, \"iteration\": 1061, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012757707387208939, \"iteration\": 1062, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005483550019562244, \"iteration\": 1063, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01709059067070484, \"iteration\": 1064, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00620255945250392, \"iteration\": 1065, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013251418247818947, \"iteration\": 1066, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004023967310786247, \"iteration\": 1067, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004403415136039257, \"iteration\": 1068, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00929019134491682, \"iteration\": 1069, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005339704919606447, \"iteration\": 1070, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00475609814748168, \"iteration\": 1071, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.019832119345664978, \"iteration\": 1072, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003645416582003236, \"iteration\": 1073, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006261743605136871, \"iteration\": 1074, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007967249490320683, \"iteration\": 1075, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0076300958171486855, \"iteration\": 1076, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005076201166957617, \"iteration\": 1077, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0036643987987190485, \"iteration\": 1078, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004115079529583454, \"iteration\": 1079, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.015030751936137676, \"iteration\": 1080, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026259304955601692, \"iteration\": 1081, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007381487637758255, \"iteration\": 1082, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0023473426699638367, \"iteration\": 1083, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011534054763615131, \"iteration\": 1084, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011109994724392891, \"iteration\": 1085, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003191169584169984, \"iteration\": 1086, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003810822032392025, \"iteration\": 1087, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005799140315502882, \"iteration\": 1088, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08351690322160721, \"iteration\": 1089, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.013923774473369122, \"iteration\": 1090, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007702740374952555, \"iteration\": 1091, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006518290378153324, \"iteration\": 1092, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0062515875324606895, \"iteration\": 1093, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011907676234841347, \"iteration\": 1094, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004803110845386982, \"iteration\": 1095, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009809417650103569, \"iteration\": 1096, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010157553479075432, \"iteration\": 1097, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005490993149578571, \"iteration\": 1098, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00430052075535059, \"iteration\": 1099, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006388660054653883, \"iteration\": 1100, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013826831243932247, \"iteration\": 1101, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004279741086065769, \"iteration\": 1102, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005845386069267988, \"iteration\": 1103, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0051795439794659615, \"iteration\": 1104, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02923394739627838, \"iteration\": 1105, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003546641208231449, \"iteration\": 1106, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00690837949514389, \"iteration\": 1107, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00795161072164774, \"iteration\": 1108, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009283201768994331, \"iteration\": 1109, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03170999884605408, \"iteration\": 1110, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008034653030335903, \"iteration\": 1111, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005036002956330776, \"iteration\": 1112, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005837447475641966, \"iteration\": 1113, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.04073811322450638, \"iteration\": 1114, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0059016915038228035, \"iteration\": 1115, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009637009352445602, \"iteration\": 1116, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0068098208867013454, \"iteration\": 1117, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.010964499786496162, \"iteration\": 1118, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007173639722168446, \"iteration\": 1119, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005467650480568409, \"iteration\": 1120, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005546463653445244, \"iteration\": 1121, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007541893981397152, \"iteration\": 1122, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.046360813081264496, \"iteration\": 1123, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02641930617392063, \"iteration\": 1124, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007715713232755661, \"iteration\": 1125, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004755397792905569, \"iteration\": 1126, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01025758869946003, \"iteration\": 1127, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00677019776776433, \"iteration\": 1128, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005643986165523529, \"iteration\": 1129, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008748133666813374, \"iteration\": 1130, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011522786691784859, \"iteration\": 1131, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007923047058284283, \"iteration\": 1132, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008311075158417225, \"iteration\": 1133, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003973782528191805, \"iteration\": 1134, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0065543148666620255, \"iteration\": 1135, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005948876030743122, \"iteration\": 1136, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009919855743646622, \"iteration\": 1137, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00914023444056511, \"iteration\": 1138, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00388727942481637, \"iteration\": 1139, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0030740872025489807, \"iteration\": 1140, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008029064163565636, \"iteration\": 1141, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007286513224244118, \"iteration\": 1142, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005795399658381939, \"iteration\": 1143, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007174499798566103, \"iteration\": 1144, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011128495447337627, \"iteration\": 1145, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0062507446855306625, \"iteration\": 1146, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0028266524896025658, \"iteration\": 1147, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009013530798256397, \"iteration\": 1148, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0022809505462646484, \"iteration\": 1149, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004704270511865616, \"iteration\": 1150, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007526211440563202, \"iteration\": 1151, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02384469471871853, \"iteration\": 1152, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006650217808783054, \"iteration\": 1153, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006820849608629942, \"iteration\": 1154, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0030343348626047373, \"iteration\": 1155, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005760642234236002, \"iteration\": 1156, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0028702118434011936, \"iteration\": 1157, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0038883390370756388, \"iteration\": 1158, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0026407151017338037, \"iteration\": 1159, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013325296342372894, \"iteration\": 1160, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006781527306884527, \"iteration\": 1161, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.002289696829393506, \"iteration\": 1162, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003060216549783945, \"iteration\": 1163, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.038463715463876724, \"iteration\": 1164, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005490873008966446, \"iteration\": 1165, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05359792336821556, \"iteration\": 1166, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0029794597066938877, \"iteration\": 1167, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012664737179875374, \"iteration\": 1168, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004906428046524525, \"iteration\": 1169, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007748676463961601, \"iteration\": 1170, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.002888304879888892, \"iteration\": 1171, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012180143967270851, \"iteration\": 1172, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00969608686864376, \"iteration\": 1173, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.017186544835567474, \"iteration\": 1174, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010389858856797218, \"iteration\": 1175, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008380571380257607, \"iteration\": 1176, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004727951250970364, \"iteration\": 1177, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0041543832048773766, \"iteration\": 1178, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03034140355885029, \"iteration\": 1179, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0037873443216085434, \"iteration\": 1180, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021951911970973015, \"iteration\": 1181, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.019730940461158752, \"iteration\": 1182, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.002873414196074009, \"iteration\": 1183, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01595228537917137, \"iteration\": 1184, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014900755137205124, \"iteration\": 1185, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003087216755375266, \"iteration\": 1186, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005256107077002525, \"iteration\": 1187, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009742493741214275, \"iteration\": 1188, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006822699215263128, \"iteration\": 1189, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011920765042304993, \"iteration\": 1190, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006997809745371342, \"iteration\": 1191, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003867033403366804, \"iteration\": 1192, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009119325317442417, \"iteration\": 1193, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012511670589447021, \"iteration\": 1194, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05003857612609863, \"iteration\": 1195, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0088115269318223, \"iteration\": 1196, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003336419351398945, \"iteration\": 1197, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011807617731392384, \"iteration\": 1198, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0021338986698538065, \"iteration\": 1199, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006322960834950209, \"iteration\": 1200, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006961558014154434, \"iteration\": 1201, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04848884791135788, \"iteration\": 1202, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007287940941751003, \"iteration\": 1203, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006764762103557587, \"iteration\": 1204, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006431915797293186, \"iteration\": 1205, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.015481742098927498, \"iteration\": 1206, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013494059443473816, \"iteration\": 1207, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008485030382871628, \"iteration\": 1208, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005869817454367876, \"iteration\": 1209, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.04046515375375748, \"iteration\": 1210, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.049366533756256104, \"iteration\": 1211, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.003101817099377513, \"iteration\": 1212, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011643745936453342, \"iteration\": 1213, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004971218761056662, \"iteration\": 1214, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0040343934670090675, \"iteration\": 1215, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0038676017429679632, \"iteration\": 1216, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0056681158021092415, \"iteration\": 1217, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01032446138560772, \"iteration\": 1218, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006210534833371639, \"iteration\": 1219, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00736667774617672, \"iteration\": 1220, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0060033733025193214, \"iteration\": 1221, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005122875329107046, \"iteration\": 1222, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012071570381522179, \"iteration\": 1223, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0024721913505345583, \"iteration\": 1224, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00868027564138174, \"iteration\": 1225, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0042455121874809265, \"iteration\": 1226, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.002113024704158306, \"iteration\": 1227, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0021368633024394512, \"iteration\": 1228, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006053558550775051, \"iteration\": 1229, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0038277776911854744, \"iteration\": 1230, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005314286798238754, \"iteration\": 1231, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02466796711087227, \"iteration\": 1232, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010499902069568634, \"iteration\": 1233, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008756658993661404, \"iteration\": 1234, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010515382513403893, \"iteration\": 1235, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010470522567629814, \"iteration\": 1236, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010116560384631157, \"iteration\": 1237, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0037477773148566484, \"iteration\": 1238, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0034520223271101713, \"iteration\": 1239, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.002530063968151808, \"iteration\": 1240, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008145025931298733, \"iteration\": 1241, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00232146168127656, \"iteration\": 1242, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007300734519958496, \"iteration\": 1243, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006235108245164156, \"iteration\": 1244, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.002346999477595091, \"iteration\": 1245, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008060593158006668, \"iteration\": 1246, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010720962658524513, \"iteration\": 1247, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.002789249876514077, \"iteration\": 1248, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005340951960533857, \"iteration\": 1249, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004205406177788973, \"iteration\": 1250, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04742235317826271, \"iteration\": 1251, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006274513900279999, \"iteration\": 1252, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008986607193946838, \"iteration\": 1253, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0001980290253413841, \"iteration\": 1254, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0018673320300877094, \"iteration\": 1255, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00273833516985178, \"iteration\": 1256, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0038539962843060493, \"iteration\": 1257, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013788656797260046, \"iteration\": 1258, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016415845602750778, \"iteration\": 1259, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011134454980492592, \"iteration\": 1260, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001295760739594698, \"iteration\": 1261, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012653390876948833, \"iteration\": 1262, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00780220702290535, \"iteration\": 1263, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0026982994750142097, \"iteration\": 1264, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0032937312498688698, \"iteration\": 1265, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0037099039182066917, \"iteration\": 1266, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0035609910264611244, \"iteration\": 1267, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013545280089601874, \"iteration\": 1268, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018892724765464664, \"iteration\": 1269, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018934850813820958, \"iteration\": 1270, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026094375178217888, \"iteration\": 1271, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016153082251548767, \"iteration\": 1272, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003178053069859743, \"iteration\": 1273, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001611884217709303, \"iteration\": 1274, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012353250058367848, \"iteration\": 1275, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008281518239527941, \"iteration\": 1276, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017813025042414665, \"iteration\": 1277, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015250016003847122, \"iteration\": 1278, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017847687704488635, \"iteration\": 1279, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0019163147080689669, \"iteration\": 1280, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0029572653584182262, \"iteration\": 1281, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026549767702817917, \"iteration\": 1282, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003418406005948782, \"iteration\": 1283, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0024950106162577868, \"iteration\": 1284, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015108929947018623, \"iteration\": 1285, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0027894885279238224, \"iteration\": 1286, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014201038284227252, \"iteration\": 1287, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013868988025933504, \"iteration\": 1288, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014889591839164495, \"iteration\": 1289, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012042601592838764, \"iteration\": 1290, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0024289502762258053, \"iteration\": 1291, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016307549085468054, \"iteration\": 1292, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018633478321135044, \"iteration\": 1293, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012670860160142183, \"iteration\": 1294, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002612894866615534, \"iteration\": 1295, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015477774431928992, \"iteration\": 1296, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002614047611132264, \"iteration\": 1297, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0023052990436553955, \"iteration\": 1298, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013549732975661755, \"iteration\": 1299, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002105167368426919, \"iteration\": 1300, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002373808529227972, \"iteration\": 1301, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0038381468039005995, \"iteration\": 1302, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016987225972115993, \"iteration\": 1303, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001519672921858728, \"iteration\": 1304, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016788416542112827, \"iteration\": 1305, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0031365733593702316, \"iteration\": 1306, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0028705548029392958, \"iteration\": 1307, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.020281454548239708, \"iteration\": 1308, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001758688478730619, \"iteration\": 1309, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014058349188417196, \"iteration\": 1310, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014680090826004744, \"iteration\": 1311, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013949708081781864, \"iteration\": 1312, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009780724067240953, \"iteration\": 1313, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001521597383543849, \"iteration\": 1314, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001045520300976932, \"iteration\": 1315, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001727837254293263, \"iteration\": 1316, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0023320431355386972, \"iteration\": 1317, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017114675138145685, \"iteration\": 1318, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008035028586164117, \"iteration\": 1319, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009873774833977222, \"iteration\": 1320, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015411153435707092, \"iteration\": 1321, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0022933606524020433, \"iteration\": 1322, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002495784778147936, \"iteration\": 1323, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001187714864499867, \"iteration\": 1324, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015306504210457206, \"iteration\": 1325, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002355656586587429, \"iteration\": 1326, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0028267716988921165, \"iteration\": 1327, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010473434813320637, \"iteration\": 1328, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010341537417843938, \"iteration\": 1329, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002259506145492196, \"iteration\": 1330, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009094956330955029, \"iteration\": 1331, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012396045494824648, \"iteration\": 1332, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020685335621237755, \"iteration\": 1333, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015152234118431807, \"iteration\": 1334, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001824435661546886, \"iteration\": 1335, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0021936516277492046, \"iteration\": 1336, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007407101220451295, \"iteration\": 1337, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001006617210805416, \"iteration\": 1338, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04283039644360542, \"iteration\": 1339, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018106232164427638, \"iteration\": 1340, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001645728712901473, \"iteration\": 1341, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001628055702894926, \"iteration\": 1342, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013576690107584, \"iteration\": 1343, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009600763441994786, \"iteration\": 1344, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00210629147477448, \"iteration\": 1345, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001969078555703163, \"iteration\": 1346, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016365419141948223, \"iteration\": 1347, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002593275625258684, \"iteration\": 1348, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008440946694463491, \"iteration\": 1349, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010463661747053266, \"iteration\": 1350, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016256438102573156, \"iteration\": 1351, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.020420759916305542, \"iteration\": 1352, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001727850059978664, \"iteration\": 1353, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011275209253653884, \"iteration\": 1354, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011033866321668029, \"iteration\": 1355, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018647917313501239, \"iteration\": 1356, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012537747388705611, \"iteration\": 1357, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011451117461547256, \"iteration\": 1358, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003227365668863058, \"iteration\": 1359, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0019121121149510145, \"iteration\": 1360, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0024496533442288637, \"iteration\": 1361, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010652842465788126, \"iteration\": 1362, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016552514862269163, \"iteration\": 1363, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002297922968864441, \"iteration\": 1364, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012308841105550528, \"iteration\": 1365, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014249456580728292, \"iteration\": 1366, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009863676968961954, \"iteration\": 1367, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0032200440764427185, \"iteration\": 1368, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0038493790198117495, \"iteration\": 1369, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000985545921139419, \"iteration\": 1370, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013214920181781054, \"iteration\": 1371, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016528251580893993, \"iteration\": 1372, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016868363600224257, \"iteration\": 1373, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013299265410751104, \"iteration\": 1374, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001108692493289709, \"iteration\": 1375, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010055750608444214, \"iteration\": 1376, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001160831656306982, \"iteration\": 1377, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015724282711744308, \"iteration\": 1378, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010141412494704127, \"iteration\": 1379, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028473157435655594, \"iteration\": 1380, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001807288033887744, \"iteration\": 1381, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004915282130241394, \"iteration\": 1382, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0030207019299268723, \"iteration\": 1383, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006035197526216507, \"iteration\": 1384, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000846510287374258, \"iteration\": 1385, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010023785289376974, \"iteration\": 1386, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018054412212222815, \"iteration\": 1387, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020752211567014456, \"iteration\": 1388, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0021607920061796904, \"iteration\": 1389, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016498134937137365, \"iteration\": 1390, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002389144152402878, \"iteration\": 1391, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013058576732873917, \"iteration\": 1392, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008344849338755012, \"iteration\": 1393, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018458093982189894, \"iteration\": 1394, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015256059123203158, \"iteration\": 1395, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013028349494561553, \"iteration\": 1396, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0021390807814896107, \"iteration\": 1397, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003724956652149558, \"iteration\": 1398, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016324878670275211, \"iteration\": 1399, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009400822455063462, \"iteration\": 1400, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006960452301427722, \"iteration\": 1401, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0017120963893830776, \"iteration\": 1402, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001650524907745421, \"iteration\": 1403, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0038061526138335466, \"iteration\": 1404, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0027839993126690388, \"iteration\": 1405, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0030722690280526876, \"iteration\": 1406, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009761357796378434, \"iteration\": 1407, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00173293671105057, \"iteration\": 1408, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001008871360681951, \"iteration\": 1409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011256413999944925, \"iteration\": 1410, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001113931299187243, \"iteration\": 1411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011468566954135895, \"iteration\": 1412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018163924105465412, \"iteration\": 1413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001028158818371594, \"iteration\": 1414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016343491151928902, \"iteration\": 1415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001409544376656413, \"iteration\": 1416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009208827978000045, \"iteration\": 1417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001671595498919487, \"iteration\": 1418, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014903118135407567, \"iteration\": 1419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001511790556833148, \"iteration\": 1420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006369227543473244, \"iteration\": 1421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014236230636015534, \"iteration\": 1422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001672147074714303, \"iteration\": 1423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01129517424851656, \"iteration\": 1424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008523717988282442, \"iteration\": 1425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002447907580062747, \"iteration\": 1426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013187308795750141, \"iteration\": 1427, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011598924174904823, \"iteration\": 1428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014094810467213392, \"iteration\": 1429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0023539364337921143, \"iteration\": 1430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007548152934759855, \"iteration\": 1431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012573503190651536, \"iteration\": 1432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0016579922521486878, \"iteration\": 1433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001633867621421814, \"iteration\": 1434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015115104615688324, \"iteration\": 1435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0011425146367400885, \"iteration\": 1436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009043127065524459, \"iteration\": 1437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008699923055246472, \"iteration\": 1438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006421863799914718, \"iteration\": 1439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014941917033866048, \"iteration\": 1440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0010579123627394438, \"iteration\": 1441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001112809288315475, \"iteration\": 1442, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009520401945337653, \"iteration\": 1443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006339821266010404, \"iteration\": 1444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008481282275170088, \"iteration\": 1445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0014150659553706646, \"iteration\": 1446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0007051362190395594, \"iteration\": 1447, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002284856978803873, \"iteration\": 1448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005787677364423871, \"iteration\": 1449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0019462585914880037, \"iteration\": 1450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001229381188750267, \"iteration\": 1451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0015499265864491463, \"iteration\": 1452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001327848294749856, \"iteration\": 1453, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0025055587757378817, \"iteration\": 1454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0013349627843126655, \"iteration\": 1455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0006042125169187784, \"iteration\": 1456, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0005368837155401707, \"iteration\": 1457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00110257463529706, \"iteration\": 1458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0008658371516503394, \"iteration\": 1459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.000796727545093745, \"iteration\": 1460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0009504237677901983, \"iteration\": 1461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0012219897471368313, \"iteration\": 1462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00014939421089366078, \"iteration\": 1463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00041633323417045176, \"iteration\": 1464, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005955536034889519, \"iteration\": 1465, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004829808312933892, \"iteration\": 1466, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010110356379300356, \"iteration\": 1467, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007001041085459292, \"iteration\": 1468, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.019842898473143578, \"iteration\": 1469, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005876068025827408, \"iteration\": 1470, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005657735164277256, \"iteration\": 1471, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005924527067691088, \"iteration\": 1472, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005795152392238379, \"iteration\": 1473, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003836581890936941, \"iteration\": 1474, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008246928919106722, \"iteration\": 1475, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000831039622426033, \"iteration\": 1476, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008992085931822658, \"iteration\": 1477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000706828897818923, \"iteration\": 1478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008166002808138728, \"iteration\": 1479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001075412379577756, \"iteration\": 1480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006527016521431506, \"iteration\": 1481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001029210863634944, \"iteration\": 1482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000841133703943342, \"iteration\": 1483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007850840338505805, \"iteration\": 1484, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02357785403728485, \"iteration\": 1485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010079611092805862, \"iteration\": 1486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016285867895931005, \"iteration\": 1487, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05497992783784866, \"iteration\": 1488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012611187994480133, \"iteration\": 1489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004465954261831939, \"iteration\": 1490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004901493084616959, \"iteration\": 1491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005124765448272228, \"iteration\": 1492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00046584755182266235, \"iteration\": 1493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000490585109218955, \"iteration\": 1494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005663341726176441, \"iteration\": 1495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006325371796265244, \"iteration\": 1496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005903398850932717, \"iteration\": 1497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005393621395342052, \"iteration\": 1498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004817395529244095, \"iteration\": 1499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006840121932327747, \"iteration\": 1500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004899179330095649, \"iteration\": 1501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006385532906278968, \"iteration\": 1502, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.019149353727698326, \"iteration\": 1503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006076946156099439, \"iteration\": 1504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0026842677034437656, \"iteration\": 1505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005879398668184876, \"iteration\": 1506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009341944241896272, \"iteration\": 1507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010339676402509212, \"iteration\": 1508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002075023716315627, \"iteration\": 1509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007014931179583073, \"iteration\": 1510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010831455001607537, \"iteration\": 1511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0020117787644267082, \"iteration\": 1512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010626775911077857, \"iteration\": 1513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0011918165255337954, \"iteration\": 1514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005875637871213257, \"iteration\": 1515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007196944206953049, \"iteration\": 1516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001287324819713831, \"iteration\": 1517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016318075358867645, \"iteration\": 1518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00047677388647571206, \"iteration\": 1519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007786372443661094, \"iteration\": 1520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006363787106238306, \"iteration\": 1521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007434931467287242, \"iteration\": 1522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007062897202558815, \"iteration\": 1523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.013238376937806606, \"iteration\": 1524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009184315567836165, \"iteration\": 1525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007932213484309614, \"iteration\": 1526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006604406516999006, \"iteration\": 1527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00036474180524237454, \"iteration\": 1528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000987345352768898, \"iteration\": 1529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006175028393045068, \"iteration\": 1530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006927247159183025, \"iteration\": 1531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004374643322080374, \"iteration\": 1532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003478490689303726, \"iteration\": 1533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006307093426585197, \"iteration\": 1534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006276820786297321, \"iteration\": 1535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005824406398460269, \"iteration\": 1536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0036956516560167074, \"iteration\": 1537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004092944145668298, \"iteration\": 1538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005963692092336714, \"iteration\": 1539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005244847852736712, \"iteration\": 1540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006976955337449908, \"iteration\": 1541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006651693256571889, \"iteration\": 1542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006154289003461599, \"iteration\": 1543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000950333836954087, \"iteration\": 1544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005933059146627784, \"iteration\": 1545, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000693672220222652, \"iteration\": 1546, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00057021010434255, \"iteration\": 1547, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005572627414949238, \"iteration\": 1548, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004502680676523596, \"iteration\": 1549, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012742263497784734, \"iteration\": 1550, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005216710269451141, \"iteration\": 1551, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004745750338770449, \"iteration\": 1552, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002074513118714094, \"iteration\": 1553, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000742205127608031, \"iteration\": 1554, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008471194887533784, \"iteration\": 1555, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005123177543282509, \"iteration\": 1556, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004938685451634228, \"iteration\": 1557, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0011165875475853682, \"iteration\": 1558, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008802157826721668, \"iteration\": 1559, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007293432718142867, \"iteration\": 1560, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005201375461183488, \"iteration\": 1561, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00044459267519414425, \"iteration\": 1562, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004192594496998936, \"iteration\": 1563, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005518955294974148, \"iteration\": 1564, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00037411751691251993, \"iteration\": 1565, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005694511346518993, \"iteration\": 1566, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004371251561678946, \"iteration\": 1567, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008650022791698575, \"iteration\": 1568, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008005212876014411, \"iteration\": 1569, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005511235794983804, \"iteration\": 1570, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005086978198960423, \"iteration\": 1571, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006977184675633907, \"iteration\": 1572, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006036608829163015, \"iteration\": 1573, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000581001047976315, \"iteration\": 1574, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005655556451529264, \"iteration\": 1575, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000519957859069109, \"iteration\": 1576, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00039204099448397756, \"iteration\": 1577, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006248549907468259, \"iteration\": 1578, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007079895585775375, \"iteration\": 1579, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004550192388705909, \"iteration\": 1580, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006832209182903171, \"iteration\": 1581, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010099834762513638, \"iteration\": 1582, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004889567499049008, \"iteration\": 1583, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008896640501916409, \"iteration\": 1584, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000804424227681011, \"iteration\": 1585, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013983719982206821, \"iteration\": 1586, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0040549105033278465, \"iteration\": 1587, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00040768278995528817, \"iteration\": 1588, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00044744787737727165, \"iteration\": 1589, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006353228818625212, \"iteration\": 1590, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005156113184057176, \"iteration\": 1591, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005860881647095084, \"iteration\": 1592, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00042942905565723777, \"iteration\": 1593, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005758641636930406, \"iteration\": 1594, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005207434296607971, \"iteration\": 1595, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005111685022711754, \"iteration\": 1596, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.038705624639987946, \"iteration\": 1597, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006411193171516061, \"iteration\": 1598, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005810922593809664, \"iteration\": 1599, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008130470523610711, \"iteration\": 1600, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005326265236362815, \"iteration\": 1601, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005286437226459384, \"iteration\": 1602, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006721967947669327, \"iteration\": 1603, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007543668616563082, \"iteration\": 1604, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00042732511064969003, \"iteration\": 1605, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005644740303978324, \"iteration\": 1606, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009129428071901202, \"iteration\": 1607, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006635135505348444, \"iteration\": 1608, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005466240108944476, \"iteration\": 1609, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005246821674518287, \"iteration\": 1610, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008755886228755116, \"iteration\": 1611, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00045501350541599095, \"iteration\": 1612, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004246871394570917, \"iteration\": 1613, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005604976322501898, \"iteration\": 1614, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005606157938018441, \"iteration\": 1615, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00043291537440381944, \"iteration\": 1616, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009831080678850412, \"iteration\": 1617, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008145595202222466, \"iteration\": 1618, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003848990018013865, \"iteration\": 1619, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003657603811006993, \"iteration\": 1620, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00047926107072271407, \"iteration\": 1621, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009116126457229257, \"iteration\": 1622, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000528184580616653, \"iteration\": 1623, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00033846701262518764, \"iteration\": 1624, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006352928467094898, \"iteration\": 1625, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007644421420991421, \"iteration\": 1626, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003789594629779458, \"iteration\": 1627, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000613575684837997, \"iteration\": 1628, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005165226757526398, \"iteration\": 1629, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005021459655836225, \"iteration\": 1630, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005530398921109736, \"iteration\": 1631, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010775129776448011, \"iteration\": 1632, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005973904626443982, \"iteration\": 1633, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005356837064027786, \"iteration\": 1634, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006813699146732688, \"iteration\": 1635, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005308372201398015, \"iteration\": 1636, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00043086818186566234, \"iteration\": 1637, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004174778296146542, \"iteration\": 1638, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004615214711520821, \"iteration\": 1639, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004116118361707777, \"iteration\": 1640, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00032616511452943087, \"iteration\": 1641, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003705970011651516, \"iteration\": 1642, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006450990913435817, \"iteration\": 1643, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00035621115239337087, \"iteration\": 1644, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005450625321827829, \"iteration\": 1645, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004071151197422296, \"iteration\": 1646, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005827643908560276, \"iteration\": 1647, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011775113642215729, \"iteration\": 1648, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006137201562523842, \"iteration\": 1649, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009840315906330943, \"iteration\": 1650, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005620309384539723, \"iteration\": 1651, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005655240966007113, \"iteration\": 1652, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008111989591270685, \"iteration\": 1653, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007335718255490065, \"iteration\": 1654, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007895475719124079, \"iteration\": 1655, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0007486653630621731, \"iteration\": 1656, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.023708656430244446, \"iteration\": 1657, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0003942610928788781, \"iteration\": 1658, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004405801882967353, \"iteration\": 1659, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004183525452390313, \"iteration\": 1660, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012783457059413195, \"iteration\": 1661, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003690603654831648, \"iteration\": 1662, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009261057712137699, \"iteration\": 1663, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005455650971271098, \"iteration\": 1664, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005842198152095079, \"iteration\": 1665, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005937484675087035, \"iteration\": 1666, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008572207298129797, \"iteration\": 1667, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0005959213012829423, \"iteration\": 1668, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006027133204042912, \"iteration\": 1669, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0008996702963486314, \"iteration\": 1670, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0034409507643431425, \"iteration\": 1671, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0004365094064269215, \"iteration\": 1672, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0006056784186512232, \"iteration\": 1673, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004402080667205155, \"iteration\": 1674, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00043409166391938925, \"iteration\": 1675, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00036328405258245766, \"iteration\": 1676, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004335489356890321, \"iteration\": 1677, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00039395352359861135, \"iteration\": 1678, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00035685335751622915, \"iteration\": 1679, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00047827325761318207, \"iteration\": 1680, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008533768123015761, \"iteration\": 1681, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00043382629519328475, \"iteration\": 1682, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007764424663037062, \"iteration\": 1683, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00029025744879618287, \"iteration\": 1684, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004990371526218951, \"iteration\": 1685, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004894655430689454, \"iteration\": 1686, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005094482330605388, \"iteration\": 1687, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004129018052481115, \"iteration\": 1688, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001217717886902392, \"iteration\": 1689, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00028701365226879716, \"iteration\": 1690, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00048564403550699353, \"iteration\": 1691, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000544473878107965, \"iteration\": 1692, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010643077548593283, \"iteration\": 1693, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000479597772937268, \"iteration\": 1694, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003706276183947921, \"iteration\": 1695, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00046719491365365684, \"iteration\": 1696, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002164769684895873, \"iteration\": 1697, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03012143261730671, \"iteration\": 1698, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000391837558709085, \"iteration\": 1699, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00035119670792482793, \"iteration\": 1700, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005184755427762866, \"iteration\": 1701, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00038172260974533856, \"iteration\": 1702, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004956945776939392, \"iteration\": 1703, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.012118738144636154, \"iteration\": 1704, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00036647991510108113, \"iteration\": 1705, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00039076368557289243, \"iteration\": 1706, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003398287226445973, \"iteration\": 1707, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00038110220339149237, \"iteration\": 1708, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002567230840213597, \"iteration\": 1709, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005518697435036302, \"iteration\": 1710, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008093281066976488, \"iteration\": 1711, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005997564876452088, \"iteration\": 1712, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009027568157762289, \"iteration\": 1713, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003713516052812338, \"iteration\": 1714, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003724748967215419, \"iteration\": 1715, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003471347445156425, \"iteration\": 1716, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005473879864439368, \"iteration\": 1717, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00045717443572357297, \"iteration\": 1718, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00037573149893432856, \"iteration\": 1719, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.026980943977832794, \"iteration\": 1720, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009855915559455752, \"iteration\": 1721, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003362028510309756, \"iteration\": 1722, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00041057384805753827, \"iteration\": 1723, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007863856153562665, \"iteration\": 1724, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004073744348715991, \"iteration\": 1725, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00042257504537701607, \"iteration\": 1726, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003627289552241564, \"iteration\": 1727, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003455802798271179, \"iteration\": 1728, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005732165300287306, \"iteration\": 1729, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004157311050221324, \"iteration\": 1730, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00039126837509684265, \"iteration\": 1731, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00025726878084242344, \"iteration\": 1732, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003903278848156333, \"iteration\": 1733, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00027093145763501525, \"iteration\": 1734, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00036544809699989855, \"iteration\": 1735, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00039949722122401, \"iteration\": 1736, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006884869653731585, \"iteration\": 1737, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003599653718993068, \"iteration\": 1738, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004662545397877693, \"iteration\": 1739, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00036406348226591945, \"iteration\": 1740, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00033322779927402735, \"iteration\": 1741, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005233130068518221, \"iteration\": 1742, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00044787212391383946, \"iteration\": 1743, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005004577687941492, \"iteration\": 1744, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003828168846666813, \"iteration\": 1745, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004288399941287935, \"iteration\": 1746, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003402292204555124, \"iteration\": 1747, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00025110121350735426, \"iteration\": 1748, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005056083900853992, \"iteration\": 1749, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004437497991602868, \"iteration\": 1750, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006948442896828055, \"iteration\": 1751, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004126905696466565, \"iteration\": 1752, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00032342693884857, \"iteration\": 1753, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023709873494226485, \"iteration\": 1754, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002824332914315164, \"iteration\": 1755, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00038110511377453804, \"iteration\": 1756, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003660070651676506, \"iteration\": 1757, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00024349280283786356, \"iteration\": 1758, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00034552940633147955, \"iteration\": 1759, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05676022917032242, \"iteration\": 1760, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00030433517531491816, \"iteration\": 1761, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002995870599988848, \"iteration\": 1762, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023830484133213758, \"iteration\": 1763, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003617973707150668, \"iteration\": 1764, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002990149660035968, \"iteration\": 1765, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005189559306018054, \"iteration\": 1766, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005349162966012955, \"iteration\": 1767, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00047527722199447453, \"iteration\": 1768, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006340930704027414, \"iteration\": 1769, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003518200828693807, \"iteration\": 1770, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005571688525378704, \"iteration\": 1771, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005037604132667184, \"iteration\": 1772, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007932488224469125, \"iteration\": 1773, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005320594646036625, \"iteration\": 1774, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00039163685869425535, \"iteration\": 1775, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00046820082934573293, \"iteration\": 1776, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004468976112548262, \"iteration\": 1777, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00047635548980906606, \"iteration\": 1778, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000435022811871022, \"iteration\": 1779, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003546487423591316, \"iteration\": 1780, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001017831265926361, \"iteration\": 1781, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007568157743662596, \"iteration\": 1782, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00046693190233781934, \"iteration\": 1783, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00037011533277109265, \"iteration\": 1784, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005970615893602371, \"iteration\": 1785, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005121022695675492, \"iteration\": 1786, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00033636530861258507, \"iteration\": 1787, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05036509037017822, \"iteration\": 1788, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00032940515666268766, \"iteration\": 1789, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00023446365958079696, \"iteration\": 1790, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003952908155042678, \"iteration\": 1791, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000471422856207937, \"iteration\": 1792, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00029637874104082584, \"iteration\": 1793, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00041911439620889723, \"iteration\": 1794, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00026495600468479097, \"iteration\": 1795, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003509254311211407, \"iteration\": 1796, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00044577702647075057, \"iteration\": 1797, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006081509636715055, \"iteration\": 1798, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017842507222667336, \"iteration\": 1799, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02084079571068287, \"iteration\": 1800, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000565363559871912, \"iteration\": 1801, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004085061664227396, \"iteration\": 1802, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002070594346150756, \"iteration\": 1803, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00046592336730100214, \"iteration\": 1804, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000310536939650774, \"iteration\": 1805, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005743618821725249, \"iteration\": 1806, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00020731364202219993, \"iteration\": 1807, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004271790967322886, \"iteration\": 1808, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00041351321851834655, \"iteration\": 1809, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005292129935696721, \"iteration\": 1810, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005215768469497561, \"iteration\": 1811, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004329585935920477, \"iteration\": 1812, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00033722404623404145, \"iteration\": 1813, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002164963399991393, \"iteration\": 1814, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002238003653474152, \"iteration\": 1815, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003518568119034171, \"iteration\": 1816, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007368769147433341, \"iteration\": 1817, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002913466305471957, \"iteration\": 1818, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00042706835665740073, \"iteration\": 1819, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00029353745048865676, \"iteration\": 1820, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003539479512255639, \"iteration\": 1821, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006447324412874877, \"iteration\": 1822, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000283179891994223, \"iteration\": 1823, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020960280671715736, \"iteration\": 1824, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005051227635703981, \"iteration\": 1825, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002841016394086182, \"iteration\": 1826, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003696478670462966, \"iteration\": 1827, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00039979006396606565, \"iteration\": 1828, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002930313057731837, \"iteration\": 1829, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006813679938204587, \"iteration\": 1830, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004736723203677684, \"iteration\": 1831, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003591573622543365, \"iteration\": 1832, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00037205935223028064, \"iteration\": 1833, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006013495731167495, \"iteration\": 1834, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00041802634950727224, \"iteration\": 1835, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007086803670972586, \"iteration\": 1836, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002768750418908894, \"iteration\": 1837, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002369576832279563, \"iteration\": 1838, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004480322531890124, \"iteration\": 1839, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004052731383126229, \"iteration\": 1840, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002459308016113937, \"iteration\": 1841, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000437020295066759, \"iteration\": 1842, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00031552164000459015, \"iteration\": 1843, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003971111436840147, \"iteration\": 1844, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003445106849540025, \"iteration\": 1845, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002596802660264075, \"iteration\": 1846, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004957707715220749, \"iteration\": 1847, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00038412626599892974, \"iteration\": 1848, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00044574501225724816, \"iteration\": 1849, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007759654545225203, \"iteration\": 1850, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002598679857328534, \"iteration\": 1851, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06213828921318054, \"iteration\": 1852, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000391501234844327, \"iteration\": 1853, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004655624506995082, \"iteration\": 1854, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00037953592254780233, \"iteration\": 1855, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015022215666249394, \"iteration\": 1856, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004409126122482121, \"iteration\": 1857, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00030506093753501773, \"iteration\": 1858, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000328631023876369, \"iteration\": 1859, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003685011761263013, \"iteration\": 1860, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0003971040714532137, \"iteration\": 1861, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00043347052996978164, \"iteration\": 1862, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006036860286258161, \"iteration\": 1863, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004535387852229178, \"iteration\": 1864, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00041851424612104893, \"iteration\": 1865, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002719740732572973, \"iteration\": 1866, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004923014203086495, \"iteration\": 1867, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00043272756738588214, \"iteration\": 1868, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00044785713544115424, \"iteration\": 1869, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007630307809449732, \"iteration\": 1870, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00047106697456911206, \"iteration\": 1871, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002370486909057945, \"iteration\": 1872, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005301975179463625, \"iteration\": 1873, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000501392234582454, \"iteration\": 1874, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002768692502286285, \"iteration\": 1875, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002838408399838954, \"iteration\": 1876, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002861575339920819, \"iteration\": 1877, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00038227002369239926, \"iteration\": 1878, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0002494618820492178, \"iteration\": 1879, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00024369696620851755, \"iteration\": 1880, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004789622325915843, \"iteration\": 1881, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0004462719371076673, \"iteration\": 1882, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00026684108888730407, \"iteration\": 1883, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019181842799298465, \"iteration\": 1884, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00031242857221513987, \"iteration\": 1885, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003171624557580799, \"iteration\": 1886, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002653629635460675, \"iteration\": 1887, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00041442582732997835, \"iteration\": 1888, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00040416145930066705, \"iteration\": 1889, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002039547689491883, \"iteration\": 1890, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003295864735264331, \"iteration\": 1891, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00036409017047844827, \"iteration\": 1892, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020906739518977702, \"iteration\": 1893, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003088241792283952, \"iteration\": 1894, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002555489481892437, \"iteration\": 1895, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003148748946841806, \"iteration\": 1896, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00041583937127143145, \"iteration\": 1897, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027206208324059844, \"iteration\": 1898, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00031734455842524767, \"iteration\": 1899, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00040289771277457476, \"iteration\": 1900, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003229897702112794, \"iteration\": 1901, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022671639453619719, \"iteration\": 1902, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00042104837484657764, \"iteration\": 1903, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000192615480045788, \"iteration\": 1904, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003341141273267567, \"iteration\": 1905, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001855533046182245, \"iteration\": 1906, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003490956441964954, \"iteration\": 1907, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020253608818165958, \"iteration\": 1908, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00026076968060806394, \"iteration\": 1909, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000254191632848233, \"iteration\": 1910, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002343835076317191, \"iteration\": 1911, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002611704112496227, \"iteration\": 1912, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022121518850326538, \"iteration\": 1913, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024540044250898063, \"iteration\": 1914, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003753906348720193, \"iteration\": 1915, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021954526891931891, \"iteration\": 1916, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002118308620993048, \"iteration\": 1917, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002546531904954463, \"iteration\": 1918, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002983736922033131, \"iteration\": 1919, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021785199351143092, \"iteration\": 1920, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023842017981223762, \"iteration\": 1921, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00034329178743064404, \"iteration\": 1922, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024005227896850556, \"iteration\": 1923, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003178035840392113, \"iteration\": 1924, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002028320450335741, \"iteration\": 1925, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018522054597269744, \"iteration\": 1926, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00029424874810501933, \"iteration\": 1927, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00028863197076134384, \"iteration\": 1928, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016798233846202493, \"iteration\": 1929, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015982359764166176, \"iteration\": 1930, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00017292413394898176, \"iteration\": 1931, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002152096276404336, \"iteration\": 1932, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023088805028237402, \"iteration\": 1933, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002279813343193382, \"iteration\": 1934, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.008069416508078575, \"iteration\": 1935, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013932987349107862, \"iteration\": 1936, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002292058925377205, \"iteration\": 1937, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014032688923180103, \"iteration\": 1938, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00033984953188337386, \"iteration\": 1939, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003000306314788759, \"iteration\": 1940, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013124409597367048, \"iteration\": 1941, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00031210557790473104, \"iteration\": 1942, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003977732267230749, \"iteration\": 1943, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001896447065519169, \"iteration\": 1944, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002036849909927696, \"iteration\": 1945, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024927256163209677, \"iteration\": 1946, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00038199295522645116, \"iteration\": 1947, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002539308334235102, \"iteration\": 1948, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00044914899626746774, \"iteration\": 1949, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000228046890697442, \"iteration\": 1950, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021604992798529565, \"iteration\": 1951, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00018717171042226255, \"iteration\": 1952, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00029712310060858727, \"iteration\": 1953, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000353247974999249, \"iteration\": 1954, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002453361521475017, \"iteration\": 1955, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014752130664419383, \"iteration\": 1956, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027378741651773453, \"iteration\": 1957, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027043186128139496, \"iteration\": 1958, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000202577342861332, \"iteration\": 1959, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003177508187945932, \"iteration\": 1960, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002237411099486053, \"iteration\": 1961, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022156003979034722, \"iteration\": 1962, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00030344363767653704, \"iteration\": 1963, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024148909142240882, \"iteration\": 1964, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003401875146664679, \"iteration\": 1965, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016442271589767188, \"iteration\": 1966, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00028205872513353825, \"iteration\": 1967, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004732933593913913, \"iteration\": 1968, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003578211762942374, \"iteration\": 1969, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020795319869648665, \"iteration\": 1970, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021790248865727335, \"iteration\": 1971, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00025306970928795636, \"iteration\": 1972, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015677386545576155, \"iteration\": 1973, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002381281228736043, \"iteration\": 1974, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021202753123361617, \"iteration\": 1975, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00028575328178703785, \"iteration\": 1976, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00035705132177099586, \"iteration\": 1977, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022805857588537037, \"iteration\": 1978, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00033398636151105165, \"iteration\": 1979, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001942560775205493, \"iteration\": 1980, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002703022910282016, \"iteration\": 1981, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001446931710233912, \"iteration\": 1982, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003729465533979237, \"iteration\": 1983, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00038794224383309484, \"iteration\": 1984, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005453030578792095, \"iteration\": 1985, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003480105078779161, \"iteration\": 1986, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016762095037847757, \"iteration\": 1987, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00036204184289090335, \"iteration\": 1988, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024527980713173747, \"iteration\": 1989, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022476712183561176, \"iteration\": 1990, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021304843539837748, \"iteration\": 1991, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016546831466257572, \"iteration\": 1992, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001860486518125981, \"iteration\": 1993, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003353842184878886, \"iteration\": 1994, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003151330165565014, \"iteration\": 1995, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.025203723460435867, \"iteration\": 1996, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011217861901968718, \"iteration\": 1997, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002675294817890972, \"iteration\": 1998, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010398279409855604, \"iteration\": 1999, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023689091904088855, \"iteration\": 2000, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023484871780965477, \"iteration\": 2001, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002806193078868091, \"iteration\": 2002, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002939914120361209, \"iteration\": 2003, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003193288284819573, \"iteration\": 2004, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003634131280705333, \"iteration\": 2005, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003608532715588808, \"iteration\": 2006, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006618102779611945, \"iteration\": 2007, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00023017500643618405, \"iteration\": 2008, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003738247323781252, \"iteration\": 2009, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000329900678480044, \"iteration\": 2010, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00034729845356196165, \"iteration\": 2011, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004509183927439153, \"iteration\": 2012, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005321496282704175, \"iteration\": 2013, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.020074866712093353, \"iteration\": 2014, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00035347207449376583, \"iteration\": 2015, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002266779338242486, \"iteration\": 2016, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000525500625371933, \"iteration\": 2017, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000520220841281116, \"iteration\": 2018, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019517600594554096, \"iteration\": 2019, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027893419610336423, \"iteration\": 2020, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022069270198699087, \"iteration\": 2021, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003765659057535231, \"iteration\": 2022, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003593465080484748, \"iteration\": 2023, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002644920314196497, \"iteration\": 2024, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0030129780061542988, \"iteration\": 2025, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002431360917398706, \"iteration\": 2026, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016875572327990085, \"iteration\": 2027, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021132093388587236, \"iteration\": 2028, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00020998955005779862, \"iteration\": 2029, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00028674124041572213, \"iteration\": 2030, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005496604717336595, \"iteration\": 2031, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027162194601260126, \"iteration\": 2032, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002479644026607275, \"iteration\": 2033, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002996906405314803, \"iteration\": 2034, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022221461404114962, \"iteration\": 2035, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002835503255482763, \"iteration\": 2036, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00026332715060561895, \"iteration\": 2037, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000429795152740553, \"iteration\": 2038, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019906822126358747, \"iteration\": 2039, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00024666578974574804, \"iteration\": 2040, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003004920727107674, \"iteration\": 2041, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002803901443257928, \"iteration\": 2042, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00036382381222210824, \"iteration\": 2043, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00029389880364760756, \"iteration\": 2044, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001804452040232718, \"iteration\": 2045, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019754335517063737, \"iteration\": 2046, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00013616382784675807, \"iteration\": 2047, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04443608969449997, \"iteration\": 2048, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003231577284168452, \"iteration\": 2049, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002590350923128426, \"iteration\": 2050, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019357753626536578, \"iteration\": 2051, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00022973434533923864, \"iteration\": 2052, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004817117005586624, \"iteration\": 2053, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00028668087907135487, \"iteration\": 2054, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003082815674133599, \"iteration\": 2055, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004922764492221177, \"iteration\": 2056, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004134352784603834, \"iteration\": 2057, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001637690729694441, \"iteration\": 2058, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000264786824118346, \"iteration\": 2059, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006740051321685314, \"iteration\": 2060, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005116429529152811, \"iteration\": 2061, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003115393337793648, \"iteration\": 2062, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002566621406003833, \"iteration\": 2063, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002683107741177082, \"iteration\": 2064, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00014425592962652445, \"iteration\": 2065, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003592742723412812, \"iteration\": 2066, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00027582893380895257, \"iteration\": 2067, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003013663226738572, \"iteration\": 2068, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003154173609800637, \"iteration\": 2069, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00015672206063754857, \"iteration\": 2070, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016513298032805324, \"iteration\": 2071, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002488397294655442, \"iteration\": 2072, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000154108798597008, \"iteration\": 2073, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003011908265762031, \"iteration\": 2074, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003004226600751281, \"iteration\": 2075, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0023543003480881453, \"iteration\": 2076, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002847997529897839, \"iteration\": 2077, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002111506910296157, \"iteration\": 2078, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00031876543653197587, \"iteration\": 2079, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004211857740301639, \"iteration\": 2080, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00019107403932139277, \"iteration\": 2081, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002641252358444035, \"iteration\": 2082, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00016792707901913673, \"iteration\": 2083, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000253533071372658, \"iteration\": 2084, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00021675789321307093, \"iteration\": 2085, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0001805659558158368, \"iteration\": 2086, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00034460824099369347, \"iteration\": 2087, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003136612940579653, \"iteration\": 2088, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00039831805042922497, \"iteration\": 2089, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00012462364975363016, \"iteration\": 2090, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Prepare data...\")\n",
    "train_nn_words = encode_data(train_raw, words_encoder)\n",
    "test_nn_words = encode_data(test_raw, words_encoder)\n",
    "\n",
    "print(\"Train model\")\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    "    \n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_nn_words, batch_size=128, shuffle=True)\n",
    "\n",
    "USE_CACHE = True\n",
    "\n",
    "model_nn_words = NeuralNetwork(\n",
    "    input_size=len(words_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn_words.pt').exists() and USE_CACHE:\n",
    "    model_nn_words = load_model(model_nn_words, models_dir, 'model_nn_words')\n",
    "else:\n",
    "    model_nn_words.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn_words, models_dir, \"model_nn_words\")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    X_test_nn = torch.stack([test[0] for test in test_nn_words]).cpu()\n",
    "    y_test_nn = torch.stack([test[1] for test in test_nn_words]).cpu()\n",
    "    y_pred_nn_words = model_nn_words.predict(X_test_nn)\n",
    "    logits_nn_words = model_nn_words.forward(X_test_nn)\n",
    "\n",
    "result_nn_words = evaluate(y_test_nn.cpu(), y_pred_nn_words.cpu(), logits_nn_words.cpu())\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn_words, train_config, dataloader)\n",
    "\n",
    "model_nn_words.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data...\n",
      "Train model\n",
      "Accuracy: 0.7466, Precision: 0.7354, Recall: 0.8533, F1: 0.7900, AUC: 0.8188\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-3d0e9ae2ba4442e78e39f6359c74b2d8.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-3d0e9ae2ba4442e78e39f6359c74b2d8.vega-embed details,\n",
       "  #altair-viz-3d0e9ae2ba4442e78e39f6359c74b2d8.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-3d0e9ae2ba4442e78e39f6359c74b2d8\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-3d0e9ae2ba4442e78e39f6359c74b2d8\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-3d0e9ae2ba4442e78e39f6359c74b2d8\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-9b834dae7e21590029b0324ecaf27b7a\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-9b834dae7e21590029b0324ecaf27b7a\": [{\"training_acc\": 0.4609375, \"training_loss\": 0.6960036754608154, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6976890563964844, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6918635368347168, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6884666681289673, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 0.6881596446037292, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.4921875, \"training_loss\": 0.6933454275131226, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6762093305587769, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.678402841091156, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6648179292678833, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6500144600868225, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6780773997306824, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6783601641654968, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6297419667243958, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 0.6978911757469177, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6402086615562439, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6967746019363403, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6798203587532043, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6606041193008423, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6872287392616272, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 0.6781901717185974, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6729367971420288, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 0.6826649904251099, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6740131378173828, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6573017239570618, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6578900814056396, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6659612655639648, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6690858602523804, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6598923802375793, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6597543954849243, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6485233306884766, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6380587816238403, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6490845680236816, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6232748031616211, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6229013204574585, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6391350030899048, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6731100082397461, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.644431471824646, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6126406192779541, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6266060471534729, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6379738450050354, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.6176855564117432, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.59075927734375, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.6156914234161377, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.6168454885482788, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.6198155879974365, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.6104500889778137, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.6019227504730225, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.6047071218490601, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5946630239486694, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6178974509239197, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6003321409225464, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5701820850372314, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5749713182449341, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5528954267501831, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5316415429115295, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5262306928634644, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5568329691886902, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5449333190917969, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5263029336929321, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5581983923912048, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5278351902961731, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5316052436828613, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5352615118026733, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5311915874481201, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5177023410797119, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.4767863154411316, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5014679431915283, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5358693599700928, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5560715198516846, \"iteration\": 69, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5549415349960327, \"iteration\": 70, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.48564353585243225, \"iteration\": 71, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.531305730342865, \"iteration\": 72, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5009064674377441, \"iteration\": 73, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5848934650421143, \"iteration\": 74, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.46545496582984924, \"iteration\": 75, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5167514085769653, \"iteration\": 76, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.6148757934570312, \"iteration\": 77, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.46530723571777344, \"iteration\": 78, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4774579405784607, \"iteration\": 79, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6119869947433472, \"iteration\": 80, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5284937620162964, \"iteration\": 81, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4712725281715393, \"iteration\": 82, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.44816648960113525, \"iteration\": 83, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.46205073595046997, \"iteration\": 84, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4821986258029938, \"iteration\": 85, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.48984938859939575, \"iteration\": 86, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.44043928384780884, \"iteration\": 87, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.49923989176750183, \"iteration\": 88, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5311990976333618, \"iteration\": 89, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.47513318061828613, \"iteration\": 90, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5348689556121826, \"iteration\": 91, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4243617653846741, \"iteration\": 92, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.43504223227500916, \"iteration\": 93, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.45328062772750854, \"iteration\": 94, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4991154670715332, \"iteration\": 95, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4861550033092499, \"iteration\": 96, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5407699346542358, \"iteration\": 97, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.47185006737709045, \"iteration\": 98, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.46904346346855164, \"iteration\": 99, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5634877681732178, \"iteration\": 100, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.511829137802124, \"iteration\": 101, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4569595754146576, \"iteration\": 102, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.4971978962421417, \"iteration\": 103, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4434082508087158, \"iteration\": 104, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4677232503890991, \"iteration\": 105, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.46810364723205566, \"iteration\": 106, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.44203904271125793, \"iteration\": 107, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5187393426895142, \"iteration\": 108, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.43745777010917664, \"iteration\": 109, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.4028172492980957, \"iteration\": 110, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.48903411626815796, \"iteration\": 111, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.49476563930511475, \"iteration\": 112, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.4933270812034607, \"iteration\": 113, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4365213215351105, \"iteration\": 114, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4370534121990204, \"iteration\": 115, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.4590756893157959, \"iteration\": 116, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4366418421268463, \"iteration\": 117, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4763216972351074, \"iteration\": 118, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5031570196151733, \"iteration\": 119, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.48476624488830566, \"iteration\": 120, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4670117199420929, \"iteration\": 121, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5566273927688599, \"iteration\": 122, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5387698411941528, \"iteration\": 123, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.5476683378219604, \"iteration\": 124, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.45181524753570557, \"iteration\": 125, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.39507752656936646, \"iteration\": 126, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.48608046770095825, \"iteration\": 127, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.44974109530448914, \"iteration\": 128, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.536462664604187, \"iteration\": 129, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4917081892490387, \"iteration\": 130, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5213459730148315, \"iteration\": 131, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4385709762573242, \"iteration\": 132, \"epoch\": 1}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3807130455970764, \"iteration\": 133, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.41511479020118713, \"iteration\": 134, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5272324681282043, \"iteration\": 135, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4662579894065857, \"iteration\": 136, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.4983237683773041, \"iteration\": 137, \"epoch\": 1}, {\"training_acc\": 0.734375, \"training_loss\": 0.5087183713912964, \"iteration\": 138, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5122575759887695, \"iteration\": 139, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4249737858772278, \"iteration\": 140, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.46541956067085266, \"iteration\": 141, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.43919289112091064, \"iteration\": 142, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4335933029651642, \"iteration\": 143, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.462225079536438, \"iteration\": 144, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.554714560508728, \"iteration\": 145, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.3782908320426941, \"iteration\": 146, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4911044239997864, \"iteration\": 147, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.4234919250011444, \"iteration\": 148, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.562064528465271, \"iteration\": 149, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.48321548104286194, \"iteration\": 150, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4341058135032654, \"iteration\": 151, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5262865424156189, \"iteration\": 152, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4404178857803345, \"iteration\": 153, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.45535677671432495, \"iteration\": 154, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.4479473829269409, \"iteration\": 155, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.5132743120193481, \"iteration\": 156, \"epoch\": 1}, {\"training_acc\": 0.875, \"training_loss\": 0.3752899765968323, \"iteration\": 157, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.46497642993927, \"iteration\": 158, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.46942442655563354, \"iteration\": 159, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.3832329511642456, \"iteration\": 160, \"epoch\": 1}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3719254732131958, \"iteration\": 161, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5137426257133484, \"iteration\": 162, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.39073216915130615, \"iteration\": 163, \"epoch\": 1}, {\"training_acc\": 0.8515625, \"training_loss\": 0.42088404297828674, \"iteration\": 164, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4465203881263733, \"iteration\": 165, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.41129064559936523, \"iteration\": 166, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.5067845582962036, \"iteration\": 167, \"epoch\": 1}, {\"training_acc\": 0.7890625, \"training_loss\": 0.44553524255752563, \"iteration\": 168, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4056611657142639, \"iteration\": 169, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.6237231492996216, \"iteration\": 170, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.49159055948257446, \"iteration\": 171, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.43104851245880127, \"iteration\": 172, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4414616823196411, \"iteration\": 173, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.40449273586273193, \"iteration\": 174, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4524754285812378, \"iteration\": 175, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.4959279000759125, \"iteration\": 176, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.515745997428894, \"iteration\": 177, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.4163561761379242, \"iteration\": 178, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.567231297492981, \"iteration\": 179, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5103345513343811, \"iteration\": 180, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.492978572845459, \"iteration\": 181, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4435248374938965, \"iteration\": 182, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.428256630897522, \"iteration\": 183, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.4608425498008728, \"iteration\": 184, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5421750545501709, \"iteration\": 185, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.47963523864746094, \"iteration\": 186, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4187818169593811, \"iteration\": 187, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4217246174812317, \"iteration\": 188, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.5001380443572998, \"iteration\": 189, \"epoch\": 1}, {\"training_acc\": 0.828125, \"training_loss\": 0.4025290310382843, \"iteration\": 190, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.451870322227478, \"iteration\": 191, \"epoch\": 1}, {\"training_acc\": 0.796875, \"training_loss\": 0.5528372526168823, \"iteration\": 192, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.586313009262085, \"iteration\": 193, \"epoch\": 1}, {\"training_acc\": 0.8359375, \"training_loss\": 0.42866089940071106, \"iteration\": 194, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.48346468806266785, \"iteration\": 195, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.4853815734386444, \"iteration\": 196, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.42771175503730774, \"iteration\": 197, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4770038425922394, \"iteration\": 198, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4028012454509735, \"iteration\": 199, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.3673364520072937, \"iteration\": 200, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4404585063457489, \"iteration\": 201, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.41258639097213745, \"iteration\": 202, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4168309271335602, \"iteration\": 203, \"epoch\": 1}, {\"training_acc\": 0.84375, \"training_loss\": 0.40037792921066284, \"iteration\": 204, \"epoch\": 1}, {\"training_acc\": 0.78125, \"training_loss\": 0.550528347492218, \"iteration\": 205, \"epoch\": 1}, {\"training_acc\": 0.8203125, \"training_loss\": 0.41358932852745056, \"iteration\": 206, \"epoch\": 1}, {\"training_acc\": 0.765625, \"training_loss\": 0.4802664816379547, \"iteration\": 207, \"epoch\": 1}, {\"training_acc\": 0.8125, \"training_loss\": 0.4129802882671356, \"iteration\": 208, \"epoch\": 1}, {\"training_acc\": 1.0, \"training_loss\": 0.4294842481613159, \"iteration\": 209, \"epoch\": 1}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3100142478942871, \"iteration\": 210, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3453250229358673, \"iteration\": 211, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3238442540168762, \"iteration\": 212, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2974982261657715, \"iteration\": 213, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.29784607887268066, \"iteration\": 214, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.31016260385513306, \"iteration\": 215, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3016563951969147, \"iteration\": 216, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.34011024236679077, \"iteration\": 217, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4492134153842926, \"iteration\": 218, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2663766145706177, \"iteration\": 219, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3144596517086029, \"iteration\": 220, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3242087662220001, \"iteration\": 221, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.26628172397613525, \"iteration\": 222, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.33340248465538025, \"iteration\": 223, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3481936752796173, \"iteration\": 224, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3229164481163025, \"iteration\": 225, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2721940875053406, \"iteration\": 226, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.34744793176651, \"iteration\": 227, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.30445006489753723, \"iteration\": 228, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.35204067826271057, \"iteration\": 229, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2871097922325134, \"iteration\": 230, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4072039723396301, \"iteration\": 231, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.37437060475349426, \"iteration\": 232, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.40417981147766113, \"iteration\": 233, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.39031219482421875, \"iteration\": 234, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4131399691104889, \"iteration\": 235, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2989004850387573, \"iteration\": 236, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.33987969160079956, \"iteration\": 237, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.36785757541656494, \"iteration\": 238, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3572167754173279, \"iteration\": 239, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3439560830593109, \"iteration\": 240, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.33778834342956543, \"iteration\": 241, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2668530344963074, \"iteration\": 242, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.20383574068546295, \"iteration\": 243, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3011159300804138, \"iteration\": 244, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.40416014194488525, \"iteration\": 245, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.36688780784606934, \"iteration\": 246, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3639058470726013, \"iteration\": 247, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.364342600107193, \"iteration\": 248, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3042559027671814, \"iteration\": 249, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.37834230065345764, \"iteration\": 250, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30395418405532837, \"iteration\": 251, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.36667653918266296, \"iteration\": 252, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4207618236541748, \"iteration\": 253, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2673034071922302, \"iteration\": 254, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2850516438484192, \"iteration\": 255, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.23915442824363708, \"iteration\": 256, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.33025333285331726, \"iteration\": 257, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.32614684104919434, \"iteration\": 258, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.34375959634780884, \"iteration\": 259, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4248112142086029, \"iteration\": 260, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.38173797726631165, \"iteration\": 261, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3763384222984314, \"iteration\": 262, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3574085831642151, \"iteration\": 263, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.21820718050003052, \"iteration\": 264, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30865538120269775, \"iteration\": 265, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.35620027780532837, \"iteration\": 266, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.36749035120010376, \"iteration\": 267, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.27808618545532227, \"iteration\": 268, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.2392696738243103, \"iteration\": 269, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3276427388191223, \"iteration\": 270, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3070725202560425, \"iteration\": 271, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2917978763580322, \"iteration\": 272, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.29756319522857666, \"iteration\": 273, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3272988200187683, \"iteration\": 274, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4014787971973419, \"iteration\": 275, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2585812211036682, \"iteration\": 276, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.36424463987350464, \"iteration\": 277, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3291509747505188, \"iteration\": 278, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.354414701461792, \"iteration\": 279, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.27893900871276855, \"iteration\": 280, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3759751319885254, \"iteration\": 281, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3526380658149719, \"iteration\": 282, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.2751336693763733, \"iteration\": 283, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3804357647895813, \"iteration\": 284, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.2640305757522583, \"iteration\": 285, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.38454923033714294, \"iteration\": 286, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3323713541030884, \"iteration\": 287, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3389495313167572, \"iteration\": 288, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.3556571304798126, \"iteration\": 289, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3423294723033905, \"iteration\": 290, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.34169667959213257, \"iteration\": 291, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.331810861825943, \"iteration\": 292, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.32133322954177856, \"iteration\": 293, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4147303104400635, \"iteration\": 294, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.37383368611335754, \"iteration\": 295, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.322540819644928, \"iteration\": 296, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.35766246914863586, \"iteration\": 297, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.2965301275253296, \"iteration\": 298, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4001031517982483, \"iteration\": 299, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3180094361305237, \"iteration\": 300, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.32851842045783997, \"iteration\": 301, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.30030250549316406, \"iteration\": 302, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.40257176756858826, \"iteration\": 303, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.36209529638290405, \"iteration\": 304, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3281450569629669, \"iteration\": 305, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3536112904548645, \"iteration\": 306, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.39453136920928955, \"iteration\": 307, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3172786235809326, \"iteration\": 308, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3625473976135254, \"iteration\": 309, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.40941786766052246, \"iteration\": 310, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3559805154800415, \"iteration\": 311, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3722514510154724, \"iteration\": 312, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.26172274351119995, \"iteration\": 313, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3298165202140808, \"iteration\": 314, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.3417491316795349, \"iteration\": 315, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3234591782093048, \"iteration\": 316, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3900163173675537, \"iteration\": 317, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.33989667892456055, \"iteration\": 318, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3070681095123291, \"iteration\": 319, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3526681661605835, \"iteration\": 320, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.35465699434280396, \"iteration\": 321, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3783624470233917, \"iteration\": 322, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3528505563735962, \"iteration\": 323, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.34455448389053345, \"iteration\": 324, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3242154121398926, \"iteration\": 325, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3700735569000244, \"iteration\": 326, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.38499677181243896, \"iteration\": 327, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.27034780383110046, \"iteration\": 328, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.36797836422920227, \"iteration\": 329, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.3910636901855469, \"iteration\": 330, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3958156704902649, \"iteration\": 331, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3857080340385437, \"iteration\": 332, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.32145118713378906, \"iteration\": 333, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3690546154975891, \"iteration\": 334, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4041604995727539, \"iteration\": 335, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3705992102622986, \"iteration\": 336, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.2438073307275772, \"iteration\": 337, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3768027722835541, \"iteration\": 338, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3426462709903717, \"iteration\": 339, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.34858274459838867, \"iteration\": 340, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.359147310256958, \"iteration\": 341, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.4245191514492035, \"iteration\": 342, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.345444917678833, \"iteration\": 343, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30824941396713257, \"iteration\": 344, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3300603926181793, \"iteration\": 345, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2789851725101471, \"iteration\": 346, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3955347537994385, \"iteration\": 347, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4731299579143524, \"iteration\": 348, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3190428912639618, \"iteration\": 349, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2932726740837097, \"iteration\": 350, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.377512127161026, \"iteration\": 351, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.33148688077926636, \"iteration\": 352, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30456236004829407, \"iteration\": 353, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3929169178009033, \"iteration\": 354, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.32774513959884644, \"iteration\": 355, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.32134243845939636, \"iteration\": 356, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3853968381881714, \"iteration\": 357, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3004284203052521, \"iteration\": 358, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.35271307826042175, \"iteration\": 359, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.33922192454338074, \"iteration\": 360, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.3955671191215515, \"iteration\": 361, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.313978910446167, \"iteration\": 362, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3672236204147339, \"iteration\": 363, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.38073259592056274, \"iteration\": 364, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.34734421968460083, \"iteration\": 365, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.47294315695762634, \"iteration\": 366, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3505222797393799, \"iteration\": 367, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3204653561115265, \"iteration\": 368, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.32114332914352417, \"iteration\": 369, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.33643385767936707, \"iteration\": 370, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3329629600048065, \"iteration\": 371, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3959655165672302, \"iteration\": 372, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.32029178738594055, \"iteration\": 373, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3332233726978302, \"iteration\": 374, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.3868318796157837, \"iteration\": 375, \"epoch\": 2}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23045040667057037, \"iteration\": 376, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3626788258552551, \"iteration\": 377, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.40324097871780396, \"iteration\": 378, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2650912404060364, \"iteration\": 379, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3311297595500946, \"iteration\": 380, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.2817394435405731, \"iteration\": 381, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3382297158241272, \"iteration\": 382, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.3854278326034546, \"iteration\": 383, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.39978790283203125, \"iteration\": 384, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.33865228295326233, \"iteration\": 385, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4046938419342041, \"iteration\": 386, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.33265572786331177, \"iteration\": 387, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.36161813139915466, \"iteration\": 388, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3215440511703491, \"iteration\": 389, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.33696800470352173, \"iteration\": 390, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.32947173714637756, \"iteration\": 391, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.32800620794296265, \"iteration\": 392, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.3472484350204468, \"iteration\": 393, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.41516393423080444, \"iteration\": 394, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.3487153649330139, \"iteration\": 395, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.42488640546798706, \"iteration\": 396, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.40645837783813477, \"iteration\": 397, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.40211308002471924, \"iteration\": 398, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.39677155017852783, \"iteration\": 399, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.39519476890563965, \"iteration\": 400, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.36715900897979736, \"iteration\": 401, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3387165665626526, \"iteration\": 402, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3806264400482178, \"iteration\": 403, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.29981252551078796, \"iteration\": 404, \"epoch\": 2}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27568483352661133, \"iteration\": 405, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.316325306892395, \"iteration\": 406, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.31632405519485474, \"iteration\": 407, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30502328276634216, \"iteration\": 408, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.32056957483291626, \"iteration\": 409, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4199814796447754, \"iteration\": 410, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3911488950252533, \"iteration\": 411, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.29549670219421387, \"iteration\": 412, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.421059787273407, \"iteration\": 413, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4113728702068329, \"iteration\": 414, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.37016183137893677, \"iteration\": 415, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.40086114406585693, \"iteration\": 416, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.44615018367767334, \"iteration\": 417, \"epoch\": 2}, {\"training_acc\": 1.0, \"training_loss\": 0.20826445519924164, \"iteration\": 418, \"epoch\": 2}, {\"training_acc\": 0.9375, \"training_loss\": 0.22177796065807343, \"iteration\": 419, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19961923360824585, \"iteration\": 420, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2262057363986969, \"iteration\": 421, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.19240956008434296, \"iteration\": 422, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.26442044973373413, \"iteration\": 423, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2534542679786682, \"iteration\": 424, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.25177985429763794, \"iteration\": 425, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20511773228645325, \"iteration\": 426, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.29213088750839233, \"iteration\": 427, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.20364320278167725, \"iteration\": 428, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.24253711104393005, \"iteration\": 429, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.2040029764175415, \"iteration\": 430, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.26854753494262695, \"iteration\": 431, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.24803291261196136, \"iteration\": 432, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.18449215590953827, \"iteration\": 433, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18187390267848969, \"iteration\": 434, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.1936047226190567, \"iteration\": 435, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23170876502990723, \"iteration\": 436, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30338770151138306, \"iteration\": 437, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.23336049914360046, \"iteration\": 438, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.27854466438293457, \"iteration\": 439, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.22959434986114502, \"iteration\": 440, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.24456442892551422, \"iteration\": 441, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2409353405237198, \"iteration\": 442, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1813177764415741, \"iteration\": 443, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2249811589717865, \"iteration\": 444, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2808082103729248, \"iteration\": 445, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.25487610697746277, \"iteration\": 446, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.2774617671966553, \"iteration\": 447, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.25055938959121704, \"iteration\": 448, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.23119139671325684, \"iteration\": 449, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1722862422466278, \"iteration\": 450, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.21119946241378784, \"iteration\": 451, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.20693866908550262, \"iteration\": 452, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21255014836788177, \"iteration\": 453, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.26360154151916504, \"iteration\": 454, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2850033938884735, \"iteration\": 455, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.22233566641807556, \"iteration\": 456, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.25194382667541504, \"iteration\": 457, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.24218274652957916, \"iteration\": 458, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2789943814277649, \"iteration\": 459, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.21658062934875488, \"iteration\": 460, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.19131916761398315, \"iteration\": 461, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21805405616760254, \"iteration\": 462, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2412548065185547, \"iteration\": 463, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.22082605957984924, \"iteration\": 464, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3239170014858246, \"iteration\": 465, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1971278190612793, \"iteration\": 466, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22405189275741577, \"iteration\": 467, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1724870502948761, \"iteration\": 468, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.24825149774551392, \"iteration\": 469, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2131110429763794, \"iteration\": 470, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3638925552368164, \"iteration\": 471, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2486414760351181, \"iteration\": 472, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.24744433164596558, \"iteration\": 473, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2578608989715576, \"iteration\": 474, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.24426952004432678, \"iteration\": 475, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.33493876457214355, \"iteration\": 476, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25147485733032227, \"iteration\": 477, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.29587656259536743, \"iteration\": 478, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3561546206474304, \"iteration\": 479, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2806248664855957, \"iteration\": 480, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.31035372614860535, \"iteration\": 481, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20729966461658478, \"iteration\": 482, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.29115790128707886, \"iteration\": 483, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.24146109819412231, \"iteration\": 484, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2894900441169739, \"iteration\": 485, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2669323682785034, \"iteration\": 486, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2256888598203659, \"iteration\": 487, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2421335130929947, \"iteration\": 488, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14069324731826782, \"iteration\": 489, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2325519323348999, \"iteration\": 490, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21037915349006653, \"iteration\": 491, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23470935225486755, \"iteration\": 492, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2512890100479126, \"iteration\": 493, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.210221529006958, \"iteration\": 494, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.26797014474868774, \"iteration\": 495, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.279166042804718, \"iteration\": 496, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20166105031967163, \"iteration\": 497, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20698298513889313, \"iteration\": 498, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.20789378881454468, \"iteration\": 499, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22867485880851746, \"iteration\": 500, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2657511234283447, \"iteration\": 501, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.25130513310432434, \"iteration\": 502, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2881039083003998, \"iteration\": 503, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2581680715084076, \"iteration\": 504, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3159485459327698, \"iteration\": 505, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.24572867155075073, \"iteration\": 506, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.20673862099647522, \"iteration\": 507, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2420693188905716, \"iteration\": 508, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.2793487310409546, \"iteration\": 509, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30585286021232605, \"iteration\": 510, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.25598108768463135, \"iteration\": 511, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.23801478743553162, \"iteration\": 512, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.1789437234401703, \"iteration\": 513, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3415266275405884, \"iteration\": 514, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.39371955394744873, \"iteration\": 515, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.20500613749027252, \"iteration\": 516, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2411959022283554, \"iteration\": 517, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3089165985584259, \"iteration\": 518, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.36573275923728943, \"iteration\": 519, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30270254611968994, \"iteration\": 520, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2706120014190674, \"iteration\": 521, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.27949994802474976, \"iteration\": 522, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3068224787712097, \"iteration\": 523, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2754133939743042, \"iteration\": 524, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3004302382469177, \"iteration\": 525, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.26633185148239136, \"iteration\": 526, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3249090313911438, \"iteration\": 527, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.26934826374053955, \"iteration\": 528, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.20120245218276978, \"iteration\": 529, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.24605116248130798, \"iteration\": 530, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3277919292449951, \"iteration\": 531, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22743196785449982, \"iteration\": 532, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.33326080441474915, \"iteration\": 533, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.29676496982574463, \"iteration\": 534, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22545860707759857, \"iteration\": 535, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19964927434921265, \"iteration\": 536, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.27695432305336, \"iteration\": 537, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.298555850982666, \"iteration\": 538, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20330044627189636, \"iteration\": 539, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.25956833362579346, \"iteration\": 540, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.31726378202438354, \"iteration\": 541, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3121958076953888, \"iteration\": 542, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2772984504699707, \"iteration\": 543, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30506467819213867, \"iteration\": 544, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.24467439949512482, \"iteration\": 545, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26727205514907837, \"iteration\": 546, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.23816978931427002, \"iteration\": 547, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2363397181034088, \"iteration\": 548, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2960721254348755, \"iteration\": 549, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.290079802274704, \"iteration\": 550, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.29687464237213135, \"iteration\": 551, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22480568289756775, \"iteration\": 552, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.22245706617832184, \"iteration\": 553, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.32861441373825073, \"iteration\": 554, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3242126703262329, \"iteration\": 555, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.1678943932056427, \"iteration\": 556, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26251718401908875, \"iteration\": 557, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.31534847617149353, \"iteration\": 558, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.27646130323410034, \"iteration\": 559, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 0.34912943840026855, \"iteration\": 560, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2676140069961548, \"iteration\": 561, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.352120578289032, \"iteration\": 562, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.25705671310424805, \"iteration\": 563, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2902541756629944, \"iteration\": 564, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.25246092677116394, \"iteration\": 565, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.31662097573280334, \"iteration\": 566, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2665961682796478, \"iteration\": 567, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3145756423473358, \"iteration\": 568, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.24589286744594574, \"iteration\": 569, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3146075904369354, \"iteration\": 570, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.24470163881778717, \"iteration\": 571, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.266225129365921, \"iteration\": 572, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.32619979977607727, \"iteration\": 573, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.31744301319122314, \"iteration\": 574, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22518078982830048, \"iteration\": 575, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2787705957889557, \"iteration\": 576, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3064878582954407, \"iteration\": 577, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2720189392566681, \"iteration\": 578, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.293478786945343, \"iteration\": 579, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2626968026161194, \"iteration\": 580, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.25117701292037964, \"iteration\": 581, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2675554156303406, \"iteration\": 582, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2790057957172394, \"iteration\": 583, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2852308750152588, \"iteration\": 584, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.24636688828468323, \"iteration\": 585, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.23258310556411743, \"iteration\": 586, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.29684484004974365, \"iteration\": 587, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.26941463351249695, \"iteration\": 588, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20520810782909393, \"iteration\": 589, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3465665280818939, \"iteration\": 590, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2772439122200012, \"iteration\": 591, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.2829996943473816, \"iteration\": 592, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 0.3802396059036255, \"iteration\": 593, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3642953038215637, \"iteration\": 594, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2825630307197571, \"iteration\": 595, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.2958923578262329, \"iteration\": 596, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.32695889472961426, \"iteration\": 597, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2884270250797272, \"iteration\": 598, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2658568322658539, \"iteration\": 599, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.23533250391483307, \"iteration\": 600, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.2859521210193634, \"iteration\": 601, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.261562317609787, \"iteration\": 602, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2731802761554718, \"iteration\": 603, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.28570976853370667, \"iteration\": 604, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.21685783565044403, \"iteration\": 605, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.24464169144630432, \"iteration\": 606, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3062838315963745, \"iteration\": 607, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3071609139442444, \"iteration\": 608, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22215232253074646, \"iteration\": 609, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.31539463996887207, \"iteration\": 610, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.22227539122104645, \"iteration\": 611, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.23820269107818604, \"iteration\": 612, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.2930981516838074, \"iteration\": 613, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2696078419685364, \"iteration\": 614, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.36831212043762207, \"iteration\": 615, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2660212218761444, \"iteration\": 616, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.294581800699234, \"iteration\": 617, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2687627077102661, \"iteration\": 618, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3395906090736389, \"iteration\": 619, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 0.3309575319290161, \"iteration\": 620, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.32896798849105835, \"iteration\": 621, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.23462948203086853, \"iteration\": 622, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.3469693958759308, \"iteration\": 623, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.25885170698165894, \"iteration\": 624, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2675214409828186, \"iteration\": 625, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.29872286319732666, \"iteration\": 626, \"epoch\": 3}, {\"training_acc\": 0.8571428571428571, \"training_loss\": 0.6343734860420227, \"iteration\": 627, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14536868035793304, \"iteration\": 628, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14540240168571472, \"iteration\": 629, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.15639770030975342, \"iteration\": 630, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1786661148071289, \"iteration\": 631, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.18964454531669617, \"iteration\": 632, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.156794935464859, \"iteration\": 633, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.1555958390235901, \"iteration\": 634, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17533990740776062, \"iteration\": 635, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1650271713733673, \"iteration\": 636, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.16994258761405945, \"iteration\": 637, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.15407860279083252, \"iteration\": 638, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11620377004146576, \"iteration\": 639, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19460691511631012, \"iteration\": 640, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.22165259718894958, \"iteration\": 641, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.15272995829582214, \"iteration\": 642, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22833439707756042, \"iteration\": 643, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.16438280045986176, \"iteration\": 644, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17653638124465942, \"iteration\": 645, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15105198323726654, \"iteration\": 646, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.12484100461006165, \"iteration\": 647, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13640713691711426, \"iteration\": 648, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17868107557296753, \"iteration\": 649, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13041162490844727, \"iteration\": 650, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.16963204741477966, \"iteration\": 651, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.20868340134620667, \"iteration\": 652, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.19332453608512878, \"iteration\": 653, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17367511987686157, \"iteration\": 654, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.16573145985603333, \"iteration\": 655, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17958858609199524, \"iteration\": 656, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.19184981286525726, \"iteration\": 657, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15734468400478363, \"iteration\": 658, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17160633206367493, \"iteration\": 659, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.156967431306839, \"iteration\": 660, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12938344478607178, \"iteration\": 661, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.166329026222229, \"iteration\": 662, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1474381387233734, \"iteration\": 663, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15653565526008606, \"iteration\": 664, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12526293098926544, \"iteration\": 665, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19914183020591736, \"iteration\": 666, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.16989800333976746, \"iteration\": 667, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1583513468503952, \"iteration\": 668, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.14975298941135406, \"iteration\": 669, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16533465683460236, \"iteration\": 670, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16275747120380402, \"iteration\": 671, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0807880312204361, \"iteration\": 672, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.1851336658000946, \"iteration\": 673, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16127482056617737, \"iteration\": 674, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1924895942211151, \"iteration\": 675, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.22407633066177368, \"iteration\": 676, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18066570162773132, \"iteration\": 677, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20117202401161194, \"iteration\": 678, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.195420503616333, \"iteration\": 679, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20784981548786163, \"iteration\": 680, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12236874550580978, \"iteration\": 681, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.18356573581695557, \"iteration\": 682, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18902133405208588, \"iteration\": 683, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14998647570610046, \"iteration\": 684, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09448173642158508, \"iteration\": 685, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1497800648212433, \"iteration\": 686, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12494954466819763, \"iteration\": 687, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.11279433220624924, \"iteration\": 688, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11825994402170181, \"iteration\": 689, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.28593549132347107, \"iteration\": 690, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22968360781669617, \"iteration\": 691, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1263119876384735, \"iteration\": 692, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.24937328696250916, \"iteration\": 693, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17326655983924866, \"iteration\": 694, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14672407507896423, \"iteration\": 695, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19142931699752808, \"iteration\": 696, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.16019397974014282, \"iteration\": 697, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09664635360240936, \"iteration\": 698, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.32069647312164307, \"iteration\": 699, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22750718891620636, \"iteration\": 700, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.18425124883651733, \"iteration\": 701, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2452264130115509, \"iteration\": 702, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1649821698665619, \"iteration\": 703, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.16316083073616028, \"iteration\": 704, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1754169762134552, \"iteration\": 705, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.23154906928539276, \"iteration\": 706, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.1893366128206253, \"iteration\": 707, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19195076823234558, \"iteration\": 708, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11085323989391327, \"iteration\": 709, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23296955227851868, \"iteration\": 710, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.21496421098709106, \"iteration\": 711, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.15213079750537872, \"iteration\": 712, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2352139949798584, \"iteration\": 713, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.2688666582107544, \"iteration\": 714, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2768820524215698, \"iteration\": 715, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.21626682579517365, \"iteration\": 716, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1423616111278534, \"iteration\": 717, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2179604470729828, \"iteration\": 718, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.21123409271240234, \"iteration\": 719, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1956462562084198, \"iteration\": 720, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.17574551701545715, \"iteration\": 721, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.16677328944206238, \"iteration\": 722, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19094324111938477, \"iteration\": 723, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.15357691049575806, \"iteration\": 724, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2618557810783386, \"iteration\": 725, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17665508389472961, \"iteration\": 726, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1840490698814392, \"iteration\": 727, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.12904725968837738, \"iteration\": 728, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22328728437423706, \"iteration\": 729, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17810258269309998, \"iteration\": 730, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.27667900919914246, \"iteration\": 731, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.23645853996276855, \"iteration\": 732, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.15008868277072906, \"iteration\": 733, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.1900859773159027, \"iteration\": 734, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.25959378480911255, \"iteration\": 735, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20947036147117615, \"iteration\": 736, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17294305562973022, \"iteration\": 737, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22258560359477997, \"iteration\": 738, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.22507885098457336, \"iteration\": 739, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15062934160232544, \"iteration\": 740, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1950078308582306, \"iteration\": 741, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19943399727344513, \"iteration\": 742, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.21485087275505066, \"iteration\": 743, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17867285013198853, \"iteration\": 744, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.242611825466156, \"iteration\": 745, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1581110805273056, \"iteration\": 746, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1576320230960846, \"iteration\": 747, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17458969354629517, \"iteration\": 748, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.12028482556343079, \"iteration\": 749, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.21740120649337769, \"iteration\": 750, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.14915035665035248, \"iteration\": 751, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15265268087387085, \"iteration\": 752, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.2917482852935791, \"iteration\": 753, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.22137898206710815, \"iteration\": 754, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14731192588806152, \"iteration\": 755, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2585008144378662, \"iteration\": 756, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18052104115486145, \"iteration\": 757, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20676550269126892, \"iteration\": 758, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.18580323457717896, \"iteration\": 759, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.1852913498878479, \"iteration\": 760, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1385187804698944, \"iteration\": 761, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1930726170539856, \"iteration\": 762, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.26216447353363037, \"iteration\": 763, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1984315812587738, \"iteration\": 764, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.18701329827308655, \"iteration\": 765, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.1787596344947815, \"iteration\": 766, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.14988212287425995, \"iteration\": 767, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.20760929584503174, \"iteration\": 768, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.23915159702301025, \"iteration\": 769, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2087630331516266, \"iteration\": 770, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.180486798286438, \"iteration\": 771, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22730395197868347, \"iteration\": 772, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.20718324184417725, \"iteration\": 773, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.13077928125858307, \"iteration\": 774, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13900543749332428, \"iteration\": 775, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1980493664741516, \"iteration\": 776, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.196649432182312, \"iteration\": 777, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23328691720962524, \"iteration\": 778, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2529785633087158, \"iteration\": 779, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3399437665939331, \"iteration\": 780, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1797371357679367, \"iteration\": 781, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.180921271443367, \"iteration\": 782, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2549237310886383, \"iteration\": 783, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18197208642959595, \"iteration\": 784, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.1031995564699173, \"iteration\": 785, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1836930811405182, \"iteration\": 786, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.14996640384197235, \"iteration\": 787, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.2878110408782959, \"iteration\": 788, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13440649211406708, \"iteration\": 789, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.20727673172950745, \"iteration\": 790, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19485804438591003, \"iteration\": 791, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.23260237276554108, \"iteration\": 792, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1663457751274109, \"iteration\": 793, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.1889919936656952, \"iteration\": 794, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.20270563662052155, \"iteration\": 795, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1417452096939087, \"iteration\": 796, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1713462471961975, \"iteration\": 797, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20668043196201324, \"iteration\": 798, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17254123091697693, \"iteration\": 799, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19647251069545746, \"iteration\": 800, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.23025071620941162, \"iteration\": 801, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.166407972574234, \"iteration\": 802, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.23711663484573364, \"iteration\": 803, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.19503751397132874, \"iteration\": 804, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.1731393039226532, \"iteration\": 805, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12085550278425217, \"iteration\": 806, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.16812284290790558, \"iteration\": 807, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.2968870997428894, \"iteration\": 808, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.29337456822395325, \"iteration\": 809, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.2748202085494995, \"iteration\": 810, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22379477322101593, \"iteration\": 811, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.27924028038978577, \"iteration\": 812, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2281944453716278, \"iteration\": 813, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20923905074596405, \"iteration\": 814, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.2155674695968628, \"iteration\": 815, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2804234027862549, \"iteration\": 816, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.26135170459747314, \"iteration\": 817, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23320257663726807, \"iteration\": 818, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.18606844544410706, \"iteration\": 819, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2537495493888855, \"iteration\": 820, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20014768838882446, \"iteration\": 821, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.3121502995491028, \"iteration\": 822, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.172329843044281, \"iteration\": 823, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2092597931623459, \"iteration\": 824, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.30013036727905273, \"iteration\": 825, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3063201904296875, \"iteration\": 826, \"epoch\": 4}, {\"training_acc\": 0.875, \"training_loss\": 0.2712342143058777, \"iteration\": 827, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.21682855486869812, \"iteration\": 828, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.24393382668495178, \"iteration\": 829, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.17747515439987183, \"iteration\": 830, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14577049016952515, \"iteration\": 831, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.29839712381362915, \"iteration\": 832, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.21618297696113586, \"iteration\": 833, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.2112087607383728, \"iteration\": 834, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.24540041387081146, \"iteration\": 835, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.20208145678043365, \"iteration\": 836, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0859987810254097, \"iteration\": 837, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.11505737155675888, \"iteration\": 838, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.14371202886104584, \"iteration\": 839, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.051839739084243774, \"iteration\": 840, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1037864238023758, \"iteration\": 841, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10616227984428406, \"iteration\": 842, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.12849515676498413, \"iteration\": 843, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07160340249538422, \"iteration\": 844, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.1541777104139328, \"iteration\": 845, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12720297276973724, \"iteration\": 846, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12135826051235199, \"iteration\": 847, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09000049531459808, \"iteration\": 848, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.21026933193206787, \"iteration\": 849, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0676359087228775, \"iteration\": 850, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.16030701994895935, \"iteration\": 851, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.1333835870027542, \"iteration\": 852, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11269212514162064, \"iteration\": 853, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.0802055299282074, \"iteration\": 854, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08054350316524506, \"iteration\": 855, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.13460683822631836, \"iteration\": 856, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1117096096277237, \"iteration\": 857, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06429089605808258, \"iteration\": 858, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1310957968235016, \"iteration\": 859, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.12375155836343765, \"iteration\": 860, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12592950463294983, \"iteration\": 861, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09952442348003387, \"iteration\": 862, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11717724800109863, \"iteration\": 863, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.13783934712409973, \"iteration\": 864, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09295260906219482, \"iteration\": 865, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06435912847518921, \"iteration\": 866, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08285966515541077, \"iteration\": 867, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09219491481781006, \"iteration\": 868, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.10259108990430832, \"iteration\": 869, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.10866738110780716, \"iteration\": 870, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.11952844262123108, \"iteration\": 871, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09419950097799301, \"iteration\": 872, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10091294348239899, \"iteration\": 873, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07049814611673355, \"iteration\": 874, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08660507202148438, \"iteration\": 875, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09594367444515228, \"iteration\": 876, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.13794296979904175, \"iteration\": 877, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.12849023938179016, \"iteration\": 878, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11142779886722565, \"iteration\": 879, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.14935949444770813, \"iteration\": 880, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19772830605506897, \"iteration\": 881, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08140493929386139, \"iteration\": 882, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12121301889419556, \"iteration\": 883, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.11788943409919739, \"iteration\": 884, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12341571599245071, \"iteration\": 885, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.10071451216936111, \"iteration\": 886, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08626454323530197, \"iteration\": 887, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08632005751132965, \"iteration\": 888, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.099473737180233, \"iteration\": 889, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.12999847531318665, \"iteration\": 890, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11423879861831665, \"iteration\": 891, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.10574375092983246, \"iteration\": 892, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07706575095653534, \"iteration\": 893, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1955510675907135, \"iteration\": 894, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10947225242853165, \"iteration\": 895, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08926811814308167, \"iteration\": 896, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10267731547355652, \"iteration\": 897, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08451753109693527, \"iteration\": 898, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.18486714363098145, \"iteration\": 899, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.10669992119073868, \"iteration\": 900, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10147225856781006, \"iteration\": 901, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.055017340928316116, \"iteration\": 902, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12798795104026794, \"iteration\": 903, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.11489996314048767, \"iteration\": 904, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12958991527557373, \"iteration\": 905, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08400128036737442, \"iteration\": 906, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.15837261080741882, \"iteration\": 907, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.13744278252124786, \"iteration\": 908, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08680090308189392, \"iteration\": 909, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09814027696847916, \"iteration\": 910, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06726115942001343, \"iteration\": 911, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1137114018201828, \"iteration\": 912, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.060465242713689804, \"iteration\": 913, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06758087128400803, \"iteration\": 914, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.12024343758821487, \"iteration\": 915, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.09525586664676666, \"iteration\": 916, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07962754368782043, \"iteration\": 917, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09681223332881927, \"iteration\": 918, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10576397180557251, \"iteration\": 919, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.18578191101551056, \"iteration\": 920, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10365559160709381, \"iteration\": 921, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16557908058166504, \"iteration\": 922, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06336050480604172, \"iteration\": 923, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.076236292719841, \"iteration\": 924, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07585054636001587, \"iteration\": 925, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07003963738679886, \"iteration\": 926, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08519437909126282, \"iteration\": 927, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.2402055263519287, \"iteration\": 928, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.13145391643047333, \"iteration\": 929, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06069246679544449, \"iteration\": 930, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.12783142924308777, \"iteration\": 931, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08236520737409592, \"iteration\": 932, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15068888664245605, \"iteration\": 933, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08362068980932236, \"iteration\": 934, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14830461144447327, \"iteration\": 935, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.12750612199306488, \"iteration\": 936, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.16777929663658142, \"iteration\": 937, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08088648319244385, \"iteration\": 938, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19057966768741608, \"iteration\": 939, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13649682700634003, \"iteration\": 940, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08605504781007767, \"iteration\": 941, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.08277668058872223, \"iteration\": 942, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.20458121597766876, \"iteration\": 943, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09292584657669067, \"iteration\": 944, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.09231172502040863, \"iteration\": 945, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.16410963237285614, \"iteration\": 946, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.13640961050987244, \"iteration\": 947, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10493221879005432, \"iteration\": 948, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09033368527889252, \"iteration\": 949, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11754346638917923, \"iteration\": 950, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.14374712109565735, \"iteration\": 951, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.20355704426765442, \"iteration\": 952, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.10648301243782043, \"iteration\": 953, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.13382641971111298, \"iteration\": 954, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.1185687854886055, \"iteration\": 955, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.10101073980331421, \"iteration\": 956, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07861645519733429, \"iteration\": 957, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1544494777917862, \"iteration\": 958, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08908102661371231, \"iteration\": 959, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.13807177543640137, \"iteration\": 960, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.14874158799648285, \"iteration\": 961, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1391148567199707, \"iteration\": 962, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16999441385269165, \"iteration\": 963, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.14739762246608734, \"iteration\": 964, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07350939512252808, \"iteration\": 965, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12909628450870514, \"iteration\": 966, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11874953657388687, \"iteration\": 967, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16867703199386597, \"iteration\": 968, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.08934155106544495, \"iteration\": 969, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16563570499420166, \"iteration\": 970, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1581539809703827, \"iteration\": 971, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07668612897396088, \"iteration\": 972, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09248118847608566, \"iteration\": 973, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1175699457526207, \"iteration\": 974, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10328364372253418, \"iteration\": 975, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08336129039525986, \"iteration\": 976, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.155935138463974, \"iteration\": 977, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.26224249601364136, \"iteration\": 978, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08740199357271194, \"iteration\": 979, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15671119093894958, \"iteration\": 980, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06436337530612946, \"iteration\": 981, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09678332507610321, \"iteration\": 982, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1153147891163826, \"iteration\": 983, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2575478255748749, \"iteration\": 984, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.14242610335350037, \"iteration\": 985, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14165011048316956, \"iteration\": 986, \"epoch\": 5}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2079259306192398, \"iteration\": 987, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06753771007061005, \"iteration\": 988, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.10210371017456055, \"iteration\": 989, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1050056666135788, \"iteration\": 990, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2249603271484375, \"iteration\": 991, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.072325199842453, \"iteration\": 992, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13744215667247772, \"iteration\": 993, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08334001153707504, \"iteration\": 994, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.22645600140094757, \"iteration\": 995, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.15289169549942017, \"iteration\": 996, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.15175259113311768, \"iteration\": 997, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.08561451733112335, \"iteration\": 998, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19436153769493103, \"iteration\": 999, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.2402115911245346, \"iteration\": 1000, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11826075613498688, \"iteration\": 1001, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18465274572372437, \"iteration\": 1002, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.19134746491909027, \"iteration\": 1003, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1097659021615982, \"iteration\": 1004, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12071584910154343, \"iteration\": 1005, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08138445764780045, \"iteration\": 1006, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.15424837172031403, \"iteration\": 1007, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09658533334732056, \"iteration\": 1008, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.13860341906547546, \"iteration\": 1009, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1729450225830078, \"iteration\": 1010, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17807790637016296, \"iteration\": 1011, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.17618303000926971, \"iteration\": 1012, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.13178177177906036, \"iteration\": 1013, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.17160330712795258, \"iteration\": 1014, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.18230536580085754, \"iteration\": 1015, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12904781103134155, \"iteration\": 1016, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.2085300087928772, \"iteration\": 1017, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08954601734876633, \"iteration\": 1018, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1284829080104828, \"iteration\": 1019, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14687132835388184, \"iteration\": 1020, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.14420217275619507, \"iteration\": 1021, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1374296247959137, \"iteration\": 1022, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.09062319248914719, \"iteration\": 1023, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10387512296438217, \"iteration\": 1024, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11932023614645004, \"iteration\": 1025, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.16039156913757324, \"iteration\": 1026, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.1632026731967926, \"iteration\": 1027, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10022426396608353, \"iteration\": 1028, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11449982970952988, \"iteration\": 1029, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17010745406150818, \"iteration\": 1030, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1292131394147873, \"iteration\": 1031, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15013828873634338, \"iteration\": 1032, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.13201744854450226, \"iteration\": 1033, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1333739161491394, \"iteration\": 1034, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.11089824140071869, \"iteration\": 1035, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17592473328113556, \"iteration\": 1036, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10252561420202255, \"iteration\": 1037, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.13539008796215057, \"iteration\": 1038, \"epoch\": 5}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23002572357654572, \"iteration\": 1039, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.10723938792943954, \"iteration\": 1040, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1607426106929779, \"iteration\": 1041, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.10250887274742126, \"iteration\": 1042, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08889879286289215, \"iteration\": 1043, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.18195460736751556, \"iteration\": 1044, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02298489771783352, \"iteration\": 1045, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05336838588118553, \"iteration\": 1046, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06101856380701065, \"iteration\": 1047, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08730188012123108, \"iteration\": 1048, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.027842875570058823, \"iteration\": 1049, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05842201039195061, \"iteration\": 1050, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07372391223907471, \"iteration\": 1051, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06346313655376434, \"iteration\": 1052, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05363443121314049, \"iteration\": 1053, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07014276832342148, \"iteration\": 1054, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.0567823126912117, \"iteration\": 1055, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.046883367002010345, \"iteration\": 1056, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.0663633644580841, \"iteration\": 1057, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05285874009132385, \"iteration\": 1058, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.028684891760349274, \"iteration\": 1059, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02487023174762726, \"iteration\": 1060, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.09346425533294678, \"iteration\": 1061, \"epoch\": 6}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12488383054733276, \"iteration\": 1062, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.049533694982528687, \"iteration\": 1063, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.030682917684316635, \"iteration\": 1064, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0232061930000782, \"iteration\": 1065, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07348691672086716, \"iteration\": 1066, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04680643975734711, \"iteration\": 1067, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0271145086735487, \"iteration\": 1068, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05797988176345825, \"iteration\": 1069, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05361749976873398, \"iteration\": 1070, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05590926855802536, \"iteration\": 1071, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.039862051606178284, \"iteration\": 1072, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03373353183269501, \"iteration\": 1073, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04920697957277298, \"iteration\": 1074, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08227597177028656, \"iteration\": 1075, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10107077658176422, \"iteration\": 1076, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07219867408275604, \"iteration\": 1077, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.034458231180906296, \"iteration\": 1078, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0493147149682045, \"iteration\": 1079, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.057642631232738495, \"iteration\": 1080, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05862423777580261, \"iteration\": 1081, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04192288964986801, \"iteration\": 1082, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09065591543912888, \"iteration\": 1083, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07604009658098221, \"iteration\": 1084, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04287714511156082, \"iteration\": 1085, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03568042069673538, \"iteration\": 1086, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05393555387854576, \"iteration\": 1087, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02321372553706169, \"iteration\": 1088, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.06507119536399841, \"iteration\": 1089, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.06773865967988968, \"iteration\": 1090, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08458378165960312, \"iteration\": 1091, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07592420279979706, \"iteration\": 1092, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03349728509783745, \"iteration\": 1093, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04399631917476654, \"iteration\": 1094, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04456732049584389, \"iteration\": 1095, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04610832408070564, \"iteration\": 1096, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03260881453752518, \"iteration\": 1097, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.036793701350688934, \"iteration\": 1098, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.061713725328445435, \"iteration\": 1099, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.04479615017771721, \"iteration\": 1100, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06562862545251846, \"iteration\": 1101, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.030366452410817146, \"iteration\": 1102, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.019257519394159317, \"iteration\": 1103, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06329494714736938, \"iteration\": 1104, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01937907561659813, \"iteration\": 1105, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.037340618669986725, \"iteration\": 1106, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07174016535282135, \"iteration\": 1107, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03147612512111664, \"iteration\": 1108, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.042954541742801666, \"iteration\": 1109, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02030360698699951, \"iteration\": 1110, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04708302021026611, \"iteration\": 1111, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.046874456107616425, \"iteration\": 1112, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.036027275025844574, \"iteration\": 1113, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.026899922639131546, \"iteration\": 1114, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07355409115552902, \"iteration\": 1115, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.018671255558729172, \"iteration\": 1116, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05199310556054115, \"iteration\": 1117, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02121785655617714, \"iteration\": 1118, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11024709790945053, \"iteration\": 1119, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.10773938149213791, \"iteration\": 1120, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07498475164175034, \"iteration\": 1121, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014053679071366787, \"iteration\": 1122, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05864516645669937, \"iteration\": 1123, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03544202446937561, \"iteration\": 1124, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12644733488559723, \"iteration\": 1125, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05097635090351105, \"iteration\": 1126, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09304673969745636, \"iteration\": 1127, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07681676745414734, \"iteration\": 1128, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.023837143555283546, \"iteration\": 1129, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014189496636390686, \"iteration\": 1130, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05790798366069794, \"iteration\": 1131, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.022472355514764786, \"iteration\": 1132, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02987496368587017, \"iteration\": 1133, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04166346788406372, \"iteration\": 1134, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04873141646385193, \"iteration\": 1135, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02572775073349476, \"iteration\": 1136, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04539943486452103, \"iteration\": 1137, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07566018402576447, \"iteration\": 1138, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05293082818388939, \"iteration\": 1139, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.09281584620475769, \"iteration\": 1140, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0430108979344368, \"iteration\": 1141, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01594582572579384, \"iteration\": 1142, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07059642672538757, \"iteration\": 1143, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.026310153305530548, \"iteration\": 1144, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05077512934803963, \"iteration\": 1145, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07128061354160309, \"iteration\": 1146, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.052502065896987915, \"iteration\": 1147, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07901453971862793, \"iteration\": 1148, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08597010374069214, \"iteration\": 1149, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.054522838443517685, \"iteration\": 1150, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06326808035373688, \"iteration\": 1151, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.10243993997573853, \"iteration\": 1152, \"epoch\": 6}, {\"training_acc\": 0.9609375, \"training_loss\": 0.09567900747060776, \"iteration\": 1153, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08128933608531952, \"iteration\": 1154, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07383318245410919, \"iteration\": 1155, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09401825815439224, \"iteration\": 1156, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.039153993129730225, \"iteration\": 1157, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0443943589925766, \"iteration\": 1158, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04162559285759926, \"iteration\": 1159, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.06210777163505554, \"iteration\": 1160, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.11035260558128357, \"iteration\": 1161, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.057936981320381165, \"iteration\": 1162, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.10371033847332001, \"iteration\": 1163, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0714901015162468, \"iteration\": 1164, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03816339373588562, \"iteration\": 1165, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03622890263795853, \"iteration\": 1166, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.06497479230165482, \"iteration\": 1167, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03344684839248657, \"iteration\": 1168, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.06741463392972946, \"iteration\": 1169, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03546483814716339, \"iteration\": 1170, \"epoch\": 6}, {\"training_acc\": 0.96875, \"training_loss\": 0.09582053124904633, \"iteration\": 1171, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08986715227365494, \"iteration\": 1172, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.022348541766405106, \"iteration\": 1173, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02416205033659935, \"iteration\": 1174, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07357878983020782, \"iteration\": 1175, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.021030360832810402, \"iteration\": 1176, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09237372875213623, \"iteration\": 1177, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04995805770158768, \"iteration\": 1178, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07443404197692871, \"iteration\": 1179, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.018297748640179634, \"iteration\": 1180, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0505191832780838, \"iteration\": 1181, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05857311189174652, \"iteration\": 1182, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03393144905567169, \"iteration\": 1183, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.11876533925533295, \"iteration\": 1184, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.058024194091558456, \"iteration\": 1185, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.04634614288806915, \"iteration\": 1186, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009017039090394974, \"iteration\": 1187, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07306179404258728, \"iteration\": 1188, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.027417676523327827, \"iteration\": 1189, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04962741583585739, \"iteration\": 1190, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05917441472411156, \"iteration\": 1191, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04412643611431122, \"iteration\": 1192, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08447302877902985, \"iteration\": 1193, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04061760753393173, \"iteration\": 1194, \"epoch\": 6}, {\"training_acc\": 0.96875, \"training_loss\": 0.1697389781475067, \"iteration\": 1195, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.023577172309160233, \"iteration\": 1196, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.033054664731025696, \"iteration\": 1197, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08471572399139404, \"iteration\": 1198, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06430555880069733, \"iteration\": 1199, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.03324069082736969, \"iteration\": 1200, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10291688144207001, \"iteration\": 1201, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.047110266983509064, \"iteration\": 1202, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06908287107944489, \"iteration\": 1203, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.047306716442108154, \"iteration\": 1204, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.054767608642578125, \"iteration\": 1205, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.025082428008317947, \"iteration\": 1206, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05818946659564972, \"iteration\": 1207, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07894687354564667, \"iteration\": 1208, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05170518159866333, \"iteration\": 1209, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04591583460569382, \"iteration\": 1210, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0388195626437664, \"iteration\": 1211, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.057807557284832, \"iteration\": 1212, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.02509334310889244, \"iteration\": 1213, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0474114865064621, \"iteration\": 1214, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0498354509472847, \"iteration\": 1215, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04740569740533829, \"iteration\": 1216, \"epoch\": 6}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17301928997039795, \"iteration\": 1217, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09927380084991455, \"iteration\": 1218, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07219166308641434, \"iteration\": 1219, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07456479221582413, \"iteration\": 1220, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.023178981617093086, \"iteration\": 1221, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.1053282767534256, \"iteration\": 1222, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06197456270456314, \"iteration\": 1223, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.045363131910562515, \"iteration\": 1224, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05745448172092438, \"iteration\": 1225, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08794297277927399, \"iteration\": 1226, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.052242349833250046, \"iteration\": 1227, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.042850472033023834, \"iteration\": 1228, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04108311980962753, \"iteration\": 1229, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0386241152882576, \"iteration\": 1230, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.05124228447675705, \"iteration\": 1231, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10176436603069305, \"iteration\": 1232, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.13994063436985016, \"iteration\": 1233, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04021603986620903, \"iteration\": 1234, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13146016001701355, \"iteration\": 1235, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04510913044214249, \"iteration\": 1236, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.056206464767456055, \"iteration\": 1237, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.019980095326900482, \"iteration\": 1238, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07618933916091919, \"iteration\": 1239, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04847724735736847, \"iteration\": 1240, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.05857257544994354, \"iteration\": 1241, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.061011962592601776, \"iteration\": 1242, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.046914298087358475, \"iteration\": 1243, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05800514295697212, \"iteration\": 1244, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07414744794368744, \"iteration\": 1245, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09419947862625122, \"iteration\": 1246, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0447796955704689, \"iteration\": 1247, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09019032120704651, \"iteration\": 1248, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.05578332394361496, \"iteration\": 1249, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.054696980863809586, \"iteration\": 1250, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.09155264496803284, \"iteration\": 1251, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0799269750714302, \"iteration\": 1252, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.037713415920734406, \"iteration\": 1253, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.028657743707299232, \"iteration\": 1254, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08076179772615433, \"iteration\": 1255, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.023566976189613342, \"iteration\": 1256, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01627342775464058, \"iteration\": 1257, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.026505017653107643, \"iteration\": 1258, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013108290731906891, \"iteration\": 1259, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0192453283816576, \"iteration\": 1260, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06421342492103577, \"iteration\": 1261, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02062816545367241, \"iteration\": 1262, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010149321518838406, \"iteration\": 1263, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008934034034609795, \"iteration\": 1264, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.029889235273003578, \"iteration\": 1265, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.021881572902202606, \"iteration\": 1266, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.042639367282390594, \"iteration\": 1267, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0242849662899971, \"iteration\": 1268, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012223327532410622, \"iteration\": 1269, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07514829188585281, \"iteration\": 1270, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.020500807091593742, \"iteration\": 1271, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.024579927325248718, \"iteration\": 1272, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03485368564724922, \"iteration\": 1273, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016703620553016663, \"iteration\": 1274, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015405165031552315, \"iteration\": 1275, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.030735265463590622, \"iteration\": 1276, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.022570524364709854, \"iteration\": 1277, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01334400661289692, \"iteration\": 1278, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02354460582137108, \"iteration\": 1279, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03232865780591965, \"iteration\": 1280, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01728253811597824, \"iteration\": 1281, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010444404557347298, \"iteration\": 1282, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01166633889079094, \"iteration\": 1283, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014872772619128227, \"iteration\": 1284, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.016502799466252327, \"iteration\": 1285, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015466411598026752, \"iteration\": 1286, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.031074363738298416, \"iteration\": 1287, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02186017856001854, \"iteration\": 1288, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.04309447854757309, \"iteration\": 1289, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.019304748624563217, \"iteration\": 1290, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.024640053510665894, \"iteration\": 1291, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04666917771100998, \"iteration\": 1292, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03018125519156456, \"iteration\": 1293, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0330965630710125, \"iteration\": 1294, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01668725535273552, \"iteration\": 1295, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00629245862364769, \"iteration\": 1296, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010812236927449703, \"iteration\": 1297, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.018468016758561134, \"iteration\": 1298, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008805063553154469, \"iteration\": 1299, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014623500406742096, \"iteration\": 1300, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01991339772939682, \"iteration\": 1301, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.017113063484430313, \"iteration\": 1302, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008852698840200901, \"iteration\": 1303, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.028018269687891006, \"iteration\": 1304, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.030347604304552078, \"iteration\": 1305, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.028126198798418045, \"iteration\": 1306, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013119862414896488, \"iteration\": 1307, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02773091197013855, \"iteration\": 1308, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.018394727259874344, \"iteration\": 1309, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016009924933314323, \"iteration\": 1310, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0074187531135976315, \"iteration\": 1311, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.053041841834783554, \"iteration\": 1312, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02026521973311901, \"iteration\": 1313, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015849580988287926, \"iteration\": 1314, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01656930334866047, \"iteration\": 1315, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007490387186408043, \"iteration\": 1316, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.018108204007148743, \"iteration\": 1317, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0598631389439106, \"iteration\": 1318, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.033944275230169296, \"iteration\": 1319, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005225906148552895, \"iteration\": 1320, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008649112656712532, \"iteration\": 1321, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04445711895823479, \"iteration\": 1322, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05256146565079689, \"iteration\": 1323, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00971841812133789, \"iteration\": 1324, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03139188140630722, \"iteration\": 1325, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02442244626581669, \"iteration\": 1326, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016560442745685577, \"iteration\": 1327, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01229124702513218, \"iteration\": 1328, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010743874125182629, \"iteration\": 1329, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014807198196649551, \"iteration\": 1330, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008970234543085098, \"iteration\": 1331, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.054660409688949585, \"iteration\": 1332, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013037198223173618, \"iteration\": 1333, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010068937204778194, \"iteration\": 1334, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.018139788880944252, \"iteration\": 1335, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.018858319148421288, \"iteration\": 1336, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016493499279022217, \"iteration\": 1337, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01006295531988144, \"iteration\": 1338, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010448170825839043, \"iteration\": 1339, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.046995822340250015, \"iteration\": 1340, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00870165228843689, \"iteration\": 1341, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011182932183146477, \"iteration\": 1342, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01891236938536167, \"iteration\": 1343, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03717966750264168, \"iteration\": 1344, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01280011236667633, \"iteration\": 1345, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.025346076115965843, \"iteration\": 1346, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.015168995596468449, \"iteration\": 1347, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01241757720708847, \"iteration\": 1348, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.07416772097349167, \"iteration\": 1349, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.026350591331720352, \"iteration\": 1350, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.020206894725561142, \"iteration\": 1351, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01347306463867426, \"iteration\": 1352, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009376009926199913, \"iteration\": 1353, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.025628112256526947, \"iteration\": 1354, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03219548612833023, \"iteration\": 1355, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.040099985897541046, \"iteration\": 1356, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04055655002593994, \"iteration\": 1357, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02044466882944107, \"iteration\": 1358, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.028890864923596382, \"iteration\": 1359, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.042701058089733124, \"iteration\": 1360, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03535858914256096, \"iteration\": 1361, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03198216110467911, \"iteration\": 1362, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028133288025856018, \"iteration\": 1363, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013396167196333408, \"iteration\": 1364, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011811258271336555, \"iteration\": 1365, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0963798463344574, \"iteration\": 1366, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04359286651015282, \"iteration\": 1367, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.022916708141565323, \"iteration\": 1368, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.06870172917842865, \"iteration\": 1369, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.10101056098937988, \"iteration\": 1370, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.015771768987178802, \"iteration\": 1371, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04650019109249115, \"iteration\": 1372, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.033914823085069656, \"iteration\": 1373, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.043773166835308075, \"iteration\": 1374, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01522398553788662, \"iteration\": 1375, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.031187601387500763, \"iteration\": 1376, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011669386178255081, \"iteration\": 1377, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.019524648785591125, \"iteration\": 1378, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.07573521882295609, \"iteration\": 1379, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.055916883051395416, \"iteration\": 1380, \"epoch\": 7}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06296776235103607, \"iteration\": 1381, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.05590810999274254, \"iteration\": 1382, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.06901836395263672, \"iteration\": 1383, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04793458431959152, \"iteration\": 1384, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.040398597717285156, \"iteration\": 1385, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.02916569821536541, \"iteration\": 1386, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.11383907496929169, \"iteration\": 1387, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014196234755218029, \"iteration\": 1388, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016366517171263695, \"iteration\": 1389, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.06724213808774948, \"iteration\": 1390, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02703419327735901, \"iteration\": 1391, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016422616317868233, \"iteration\": 1392, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.059855569154024124, \"iteration\": 1393, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04602833092212677, \"iteration\": 1394, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01972825638949871, \"iteration\": 1395, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05667480081319809, \"iteration\": 1396, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.043244440108537674, \"iteration\": 1397, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.028366059064865112, \"iteration\": 1398, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03518429771065712, \"iteration\": 1399, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.017722824588418007, \"iteration\": 1400, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.018264420330524445, \"iteration\": 1401, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012533840723335743, \"iteration\": 1402, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02352575585246086, \"iteration\": 1403, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0109640434384346, \"iteration\": 1404, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011314880102872849, \"iteration\": 1405, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.020218491554260254, \"iteration\": 1406, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02034752070903778, \"iteration\": 1407, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.08361323177814484, \"iteration\": 1408, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01333066076040268, \"iteration\": 1409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.031009696424007416, \"iteration\": 1410, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.035243771970272064, \"iteration\": 1411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013858586549758911, \"iteration\": 1412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007143059279769659, \"iteration\": 1413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011844262480735779, \"iteration\": 1414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009826162829995155, \"iteration\": 1415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.024361243471503258, \"iteration\": 1416, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04506874829530716, \"iteration\": 1417, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03898048773407936, \"iteration\": 1418, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021308667957782745, \"iteration\": 1419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03139489144086838, \"iteration\": 1420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01976369507610798, \"iteration\": 1421, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03415379300713539, \"iteration\": 1422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016055312007665634, \"iteration\": 1423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0056197764351964, \"iteration\": 1424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.056332193315029144, \"iteration\": 1425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.026936475187540054, \"iteration\": 1426, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03381703794002533, \"iteration\": 1427, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.029966073110699654, \"iteration\": 1428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01677808165550232, \"iteration\": 1429, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.080718994140625, \"iteration\": 1430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.026351531967520714, \"iteration\": 1431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.031086569651961327, \"iteration\": 1432, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04121873900294304, \"iteration\": 1433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014041191898286343, \"iteration\": 1434, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07034654170274734, \"iteration\": 1435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02924976870417595, \"iteration\": 1436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.034655071794986725, \"iteration\": 1437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00492557929828763, \"iteration\": 1438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008008824661374092, \"iteration\": 1439, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04820316284894943, \"iteration\": 1440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.026908911764621735, \"iteration\": 1441, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.09298616647720337, \"iteration\": 1442, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05705398693680763, \"iteration\": 1443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.021492794156074524, \"iteration\": 1444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.014769373461604118, \"iteration\": 1445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016689661890268326, \"iteration\": 1446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02537817507982254, \"iteration\": 1447, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03282890096306801, \"iteration\": 1448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02223128080368042, \"iteration\": 1449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.026404350996017456, \"iteration\": 1450, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.049625277519226074, \"iteration\": 1451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.012291060760617256, \"iteration\": 1452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.022066539153456688, \"iteration\": 1453, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03179992735385895, \"iteration\": 1454, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05009027570486069, \"iteration\": 1455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00798178743571043, \"iteration\": 1456, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013265177607536316, \"iteration\": 1457, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04151415824890137, \"iteration\": 1458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.016103021800518036, \"iteration\": 1459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.01559123769402504, \"iteration\": 1460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.05433345586061478, \"iteration\": 1461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010244468227028847, \"iteration\": 1462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00243961438536644, \"iteration\": 1463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006105502136051655, \"iteration\": 1464, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008608529344201088, \"iteration\": 1465, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00637996569275856, \"iteration\": 1466, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.017747286707162857, \"iteration\": 1467, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0048813531175255775, \"iteration\": 1468, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014665749855339527, \"iteration\": 1469, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008419167250394821, \"iteration\": 1470, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011018919758498669, \"iteration\": 1471, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007623151410371065, \"iteration\": 1472, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.015354424715042114, \"iteration\": 1473, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.020222341641783714, \"iteration\": 1474, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00329678226262331, \"iteration\": 1475, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009743698872625828, \"iteration\": 1476, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003982129506766796, \"iteration\": 1477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00822792761027813, \"iteration\": 1478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0170136746019125, \"iteration\": 1479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010101601481437683, \"iteration\": 1480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006409233435988426, \"iteration\": 1481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003481094026938081, \"iteration\": 1482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011874975636601448, \"iteration\": 1483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014925627037882805, \"iteration\": 1484, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.023688631132245064, \"iteration\": 1485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005212629213929176, \"iteration\": 1486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009865875355899334, \"iteration\": 1487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002948499284684658, \"iteration\": 1488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.015269352123141289, \"iteration\": 1489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.015137537382543087, \"iteration\": 1490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006108779460191727, \"iteration\": 1491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.018114259466528893, \"iteration\": 1492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004146547056734562, \"iteration\": 1493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003448172938078642, \"iteration\": 1494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.02921082079410553, \"iteration\": 1495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004007440526038408, \"iteration\": 1496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01911480911076069, \"iteration\": 1497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.027493750676512718, \"iteration\": 1498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009357249364256859, \"iteration\": 1499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0063027385622262955, \"iteration\": 1500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014095507562160492, \"iteration\": 1501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.015553356148302555, \"iteration\": 1502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.021228661760687828, \"iteration\": 1503, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02988254837691784, \"iteration\": 1504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009670541621744633, \"iteration\": 1505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0073449499905109406, \"iteration\": 1506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.02210373617708683, \"iteration\": 1507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.022743957117199898, \"iteration\": 1508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.017542768269777298, \"iteration\": 1509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006531639955937862, \"iteration\": 1510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002778627909719944, \"iteration\": 1511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014068382792174816, \"iteration\": 1512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008839878253638744, \"iteration\": 1513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.019692186266183853, \"iteration\": 1514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014124395325779915, \"iteration\": 1515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006546948105096817, \"iteration\": 1516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012925383634865284, \"iteration\": 1517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004571541212499142, \"iteration\": 1518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.027274133637547493, \"iteration\": 1519, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04471919313073158, \"iteration\": 1520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004018585197627544, \"iteration\": 1521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008307015523314476, \"iteration\": 1522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.03304162994027138, \"iteration\": 1523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011348992586135864, \"iteration\": 1524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011201005429029465, \"iteration\": 1525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006166630890220404, \"iteration\": 1526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.028233477845788002, \"iteration\": 1527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0042016711086034775, \"iteration\": 1528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011266715824604034, \"iteration\": 1529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007471161894500256, \"iteration\": 1530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.047932349145412445, \"iteration\": 1531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007488394156098366, \"iteration\": 1532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.02444111928343773, \"iteration\": 1533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014853320084512234, \"iteration\": 1534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.03283367305994034, \"iteration\": 1535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01113059651106596, \"iteration\": 1536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011949253268539906, \"iteration\": 1537, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.029892031103372574, \"iteration\": 1538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0056589944288134575, \"iteration\": 1539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.034397780895233154, \"iteration\": 1540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.022710323333740234, \"iteration\": 1541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006821229122579098, \"iteration\": 1542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008812237530946732, \"iteration\": 1543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.020513642579317093, \"iteration\": 1544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.018525833263993263, \"iteration\": 1545, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014803924597799778, \"iteration\": 1546, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.013792155310511589, \"iteration\": 1547, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00881373230367899, \"iteration\": 1548, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012203389778733253, \"iteration\": 1549, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.015031697228550911, \"iteration\": 1550, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011826095171272755, \"iteration\": 1551, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005206596106290817, \"iteration\": 1552, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005477558821439743, \"iteration\": 1553, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009583983570337296, \"iteration\": 1554, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.016559168696403503, \"iteration\": 1555, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.017345545813441277, \"iteration\": 1556, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.02602086029946804, \"iteration\": 1557, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012251726351678371, \"iteration\": 1558, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010368111543357372, \"iteration\": 1559, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.015376295894384384, \"iteration\": 1560, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009113721549510956, \"iteration\": 1561, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.041757162660360336, \"iteration\": 1562, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.03327396884560585, \"iteration\": 1563, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005890249740332365, \"iteration\": 1564, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.017179526388645172, \"iteration\": 1565, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.015320321545004845, \"iteration\": 1566, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009771575219929218, \"iteration\": 1567, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004762754309922457, \"iteration\": 1568, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.025110239163041115, \"iteration\": 1569, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01722530648112297, \"iteration\": 1570, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009260340593755245, \"iteration\": 1571, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0048343888483941555, \"iteration\": 1572, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.013712286949157715, \"iteration\": 1573, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01217077486217022, \"iteration\": 1574, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.03715520352125168, \"iteration\": 1575, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0052607194520533085, \"iteration\": 1576, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.018618760630488396, \"iteration\": 1577, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01803692802786827, \"iteration\": 1578, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008556853048503399, \"iteration\": 1579, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008450949564576149, \"iteration\": 1580, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00542614609003067, \"iteration\": 1581, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014355899766087532, \"iteration\": 1582, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009574630297720432, \"iteration\": 1583, \"epoch\": 8}, {\"training_acc\": 0.984375, \"training_loss\": 0.08413746953010559, \"iteration\": 1584, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028188446536660194, \"iteration\": 1585, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00413643941283226, \"iteration\": 1586, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005532476585358381, \"iteration\": 1587, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009724374860525131, \"iteration\": 1588, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009075986221432686, \"iteration\": 1589, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007827993482351303, \"iteration\": 1590, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014037495478987694, \"iteration\": 1591, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0033535729162395, \"iteration\": 1592, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005902644246816635, \"iteration\": 1593, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004524996038526297, \"iteration\": 1594, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011357853189110756, \"iteration\": 1595, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007452093064785004, \"iteration\": 1596, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.016259800642728806, \"iteration\": 1597, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012834418565034866, \"iteration\": 1598, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01146000437438488, \"iteration\": 1599, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.024005837738513947, \"iteration\": 1600, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010667596012353897, \"iteration\": 1601, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009979939088225365, \"iteration\": 1602, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.019671078771352768, \"iteration\": 1603, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011021326296031475, \"iteration\": 1604, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012318301945924759, \"iteration\": 1605, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006802679970860481, \"iteration\": 1606, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0033485661260783672, \"iteration\": 1607, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01503969356417656, \"iteration\": 1608, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006481437012553215, \"iteration\": 1609, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010006293654441833, \"iteration\": 1610, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01025661826133728, \"iteration\": 1611, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007246197201311588, \"iteration\": 1612, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011851304210722446, \"iteration\": 1613, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01567365787923336, \"iteration\": 1614, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.029317839071154594, \"iteration\": 1615, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.03765133395791054, \"iteration\": 1616, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02658405900001526, \"iteration\": 1617, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0025852895341813564, \"iteration\": 1618, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003953919745981693, \"iteration\": 1619, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009077896364033222, \"iteration\": 1620, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010313570499420166, \"iteration\": 1621, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006969711743295193, \"iteration\": 1622, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.031867481768131256, \"iteration\": 1623, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.018904605880379677, \"iteration\": 1624, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01065774355083704, \"iteration\": 1625, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008879279717803001, \"iteration\": 1626, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021595727652311325, \"iteration\": 1627, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.030337173491716385, \"iteration\": 1628, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004999030381441116, \"iteration\": 1629, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006261598318815231, \"iteration\": 1630, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.038199469447135925, \"iteration\": 1631, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006862787529826164, \"iteration\": 1632, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.03145986422896385, \"iteration\": 1633, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01776813343167305, \"iteration\": 1634, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012414291501045227, \"iteration\": 1635, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009042369201779366, \"iteration\": 1636, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014862357638776302, \"iteration\": 1637, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00975480955094099, \"iteration\": 1638, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06102481484413147, \"iteration\": 1639, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014080900698900223, \"iteration\": 1640, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01817663572728634, \"iteration\": 1641, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.016532674431800842, \"iteration\": 1642, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07414856553077698, \"iteration\": 1643, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00605467613786459, \"iteration\": 1644, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007792152464389801, \"iteration\": 1645, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010231414809823036, \"iteration\": 1646, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01226472482085228, \"iteration\": 1647, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0032950423192232847, \"iteration\": 1648, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012857403606176376, \"iteration\": 1649, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011613992042839527, \"iteration\": 1650, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0025295394007116556, \"iteration\": 1651, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003561106976121664, \"iteration\": 1652, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011415093205869198, \"iteration\": 1653, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010904870927333832, \"iteration\": 1654, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.05258966237306595, \"iteration\": 1655, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.015540624037384987, \"iteration\": 1656, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006730708293616772, \"iteration\": 1657, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007875408977270126, \"iteration\": 1658, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.013098512776196003, \"iteration\": 1659, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006287815049290657, \"iteration\": 1660, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012029738165438175, \"iteration\": 1661, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.032807450741529465, \"iteration\": 1662, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011581909842789173, \"iteration\": 1663, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.016590632498264313, \"iteration\": 1664, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010983012616634369, \"iteration\": 1665, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01455787755548954, \"iteration\": 1666, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.027307020500302315, \"iteration\": 1667, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004207977093756199, \"iteration\": 1668, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004625524394214153, \"iteration\": 1669, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01709587872028351, \"iteration\": 1670, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010816077701747417, \"iteration\": 1671, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000531902420334518, \"iteration\": 1672, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.013055315241217613, \"iteration\": 1673, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.010110421106219292, \"iteration\": 1674, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00850333459675312, \"iteration\": 1675, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004270607139915228, \"iteration\": 1676, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0037425290793180466, \"iteration\": 1677, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005326339974999428, \"iteration\": 1678, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.021082710474729538, \"iteration\": 1679, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.009739212691783905, \"iteration\": 1680, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017813367303460836, \"iteration\": 1681, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0028078672476112843, \"iteration\": 1682, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0036683299113065004, \"iteration\": 1683, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0058209942653775215, \"iteration\": 1684, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004387687426060438, \"iteration\": 1685, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003414760809391737, \"iteration\": 1686, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0054415008053183556, \"iteration\": 1687, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0057667819783091545, \"iteration\": 1688, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003479316830635071, \"iteration\": 1689, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.011523224413394928, \"iteration\": 1690, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020020511001348495, \"iteration\": 1691, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00845243502408266, \"iteration\": 1692, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004244234412908554, \"iteration\": 1693, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018238489283248782, \"iteration\": 1694, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004932871088385582, \"iteration\": 1695, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.008085338398814201, \"iteration\": 1696, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0036832711193710566, \"iteration\": 1697, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0034324675798416138, \"iteration\": 1698, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0036309687420725822, \"iteration\": 1699, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003906602971255779, \"iteration\": 1700, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0051829456351697445, \"iteration\": 1701, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002308634342625737, \"iteration\": 1702, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.011998297646641731, \"iteration\": 1703, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0026963106356561184, \"iteration\": 1704, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006779825314879417, \"iteration\": 1705, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.007022523786872625, \"iteration\": 1706, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0027343586552888155, \"iteration\": 1707, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001563776284456253, \"iteration\": 1708, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013467301614582539, \"iteration\": 1709, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.014849554747343063, \"iteration\": 1710, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016287843463942409, \"iteration\": 1711, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013442509807646275, \"iteration\": 1712, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00127038371283561, \"iteration\": 1713, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.015492561273276806, \"iteration\": 1714, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.01750519685447216, \"iteration\": 1715, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.009105329401791096, \"iteration\": 1716, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004724600352346897, \"iteration\": 1717, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020292834378778934, \"iteration\": 1718, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04089668020606041, \"iteration\": 1719, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0046254429034888744, \"iteration\": 1720, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003971860744059086, \"iteration\": 1721, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0030880714766681194, \"iteration\": 1722, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002545114140957594, \"iteration\": 1723, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0040034642443060875, \"iteration\": 1724, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006596705876290798, \"iteration\": 1725, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0027884910814464092, \"iteration\": 1726, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003275776281952858, \"iteration\": 1727, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0054474445059895515, \"iteration\": 1728, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020126146264374256, \"iteration\": 1729, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015957471914589405, \"iteration\": 1730, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00942387804389, \"iteration\": 1731, \"epoch\": 9}, {\"training_acc\": 0.984375, \"training_loss\": 0.07021059095859528, \"iteration\": 1732, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004906116519123316, \"iteration\": 1733, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029146859887987375, \"iteration\": 1734, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.009773103520274162, \"iteration\": 1735, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004723565652966499, \"iteration\": 1736, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004178978502750397, \"iteration\": 1737, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003073928877711296, \"iteration\": 1738, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002185762394219637, \"iteration\": 1739, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.014099426567554474, \"iteration\": 1740, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00251797609962523, \"iteration\": 1741, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006494990549981594, \"iteration\": 1742, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005852199159562588, \"iteration\": 1743, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0030792229808866978, \"iteration\": 1744, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015090159140527248, \"iteration\": 1745, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016528794076293707, \"iteration\": 1746, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004035682883113623, \"iteration\": 1747, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003978008404374123, \"iteration\": 1748, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0034854046534746885, \"iteration\": 1749, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0019536016043275595, \"iteration\": 1750, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014053287450224161, \"iteration\": 1751, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003165070666000247, \"iteration\": 1752, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0019584991969168186, \"iteration\": 1753, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00382892694324255, \"iteration\": 1754, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0027293702587485313, \"iteration\": 1755, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0075338371098041534, \"iteration\": 1756, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014132740907371044, \"iteration\": 1757, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006216901354491711, \"iteration\": 1758, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.007020335178822279, \"iteration\": 1759, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.007459585089236498, \"iteration\": 1760, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003671785583719611, \"iteration\": 1761, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013939565978944302, \"iteration\": 1762, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029197735711932182, \"iteration\": 1763, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0033093849197030067, \"iteration\": 1764, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014386554248631, \"iteration\": 1765, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020347004756331444, \"iteration\": 1766, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003148556686937809, \"iteration\": 1767, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002043891465291381, \"iteration\": 1768, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001625388627871871, \"iteration\": 1769, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.036926668137311935, \"iteration\": 1770, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.011353121139109135, \"iteration\": 1771, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.010302286595106125, \"iteration\": 1772, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.007524906657636166, \"iteration\": 1773, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005820197053253651, \"iteration\": 1774, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00789685733616352, \"iteration\": 1775, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017768607940524817, \"iteration\": 1776, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005955423228442669, \"iteration\": 1777, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0030006093438714743, \"iteration\": 1778, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.014341485686600208, \"iteration\": 1779, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006455864757299423, \"iteration\": 1780, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003034416353330016, \"iteration\": 1781, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005136637017130852, \"iteration\": 1782, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0028637414798140526, \"iteration\": 1783, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001595623791217804, \"iteration\": 1784, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0049362825229763985, \"iteration\": 1785, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023958368692547083, \"iteration\": 1786, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003457373473793268, \"iteration\": 1787, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018437778344377875, \"iteration\": 1788, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006989516783505678, \"iteration\": 1789, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023067223373800516, \"iteration\": 1790, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.012225034646689892, \"iteration\": 1791, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013760966248810291, \"iteration\": 1792, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.01652931049466133, \"iteration\": 1793, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009224839741364121, \"iteration\": 1794, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001718316925689578, \"iteration\": 1795, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00527522899210453, \"iteration\": 1796, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002209217520430684, \"iteration\": 1797, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006682334467768669, \"iteration\": 1798, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004673405084758997, \"iteration\": 1799, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0022451786790043116, \"iteration\": 1800, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.007929494604468346, \"iteration\": 1801, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003005450125783682, \"iteration\": 1802, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016723205335438251, \"iteration\": 1803, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.007483516354113817, \"iteration\": 1804, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005889224819839001, \"iteration\": 1805, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005203681997954845, \"iteration\": 1806, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015799673274159431, \"iteration\": 1807, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004648820497095585, \"iteration\": 1808, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00650011794641614, \"iteration\": 1809, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002590382006019354, \"iteration\": 1810, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0040907906368374825, \"iteration\": 1811, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009261544910259545, \"iteration\": 1812, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0061669428832829, \"iteration\": 1813, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013656901428475976, \"iteration\": 1814, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001987494993954897, \"iteration\": 1815, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00474896002560854, \"iteration\": 1816, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001227703643962741, \"iteration\": 1817, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020106504671275616, \"iteration\": 1818, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0033215582370758057, \"iteration\": 1819, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006922129541635513, \"iteration\": 1820, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0027123079635202885, \"iteration\": 1821, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002897687489166856, \"iteration\": 1822, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012953553814440966, \"iteration\": 1823, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002876673825085163, \"iteration\": 1824, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023051511961966753, \"iteration\": 1825, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004430025815963745, \"iteration\": 1826, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0027440113481134176, \"iteration\": 1827, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004436210263520479, \"iteration\": 1828, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014975881204009056, \"iteration\": 1829, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017811832949519157, \"iteration\": 1830, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004418864846229553, \"iteration\": 1831, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0052186716347932816, \"iteration\": 1832, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015583352651447058, \"iteration\": 1833, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00406216224655509, \"iteration\": 1834, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002582008484750986, \"iteration\": 1835, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002089678542688489, \"iteration\": 1836, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0032231570221483707, \"iteration\": 1837, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003657747758552432, \"iteration\": 1838, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0035808768589049578, \"iteration\": 1839, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013723232550546527, \"iteration\": 1840, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0037023189943283796, \"iteration\": 1841, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005472095683217049, \"iteration\": 1842, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0042744898237288, \"iteration\": 1843, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0024865265004336834, \"iteration\": 1844, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.025604957714676857, \"iteration\": 1845, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0028350315988063812, \"iteration\": 1846, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.011269282549619675, \"iteration\": 1847, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004480944015085697, \"iteration\": 1848, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0019850232638418674, \"iteration\": 1849, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004109945148229599, \"iteration\": 1850, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0027763880789279938, \"iteration\": 1851, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023428569547832012, \"iteration\": 1852, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0032824957743287086, \"iteration\": 1853, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.012755295261740685, \"iteration\": 1854, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.009253278374671936, \"iteration\": 1855, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001985037000849843, \"iteration\": 1856, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.010994430631399155, \"iteration\": 1857, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.020674053579568863, \"iteration\": 1858, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002969174413010478, \"iteration\": 1859, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009236825862899423, \"iteration\": 1860, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.015190651640295982, \"iteration\": 1861, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00391725730150938, \"iteration\": 1862, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0026074228808283806, \"iteration\": 1863, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.010914902202785015, \"iteration\": 1864, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012813981156796217, \"iteration\": 1865, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023258053697645664, \"iteration\": 1866, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0015349802561104298, \"iteration\": 1867, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.009724980220198631, \"iteration\": 1868, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.010905265808105469, \"iteration\": 1869, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0034997498150914907, \"iteration\": 1870, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0031423016916960478, \"iteration\": 1871, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008522146381437778, \"iteration\": 1872, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012387672904878855, \"iteration\": 1873, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005488668568432331, \"iteration\": 1874, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005184594541788101, \"iteration\": 1875, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003245184663683176, \"iteration\": 1876, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.017647361382842064, \"iteration\": 1877, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.008716831915080547, \"iteration\": 1878, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005760697647929192, \"iteration\": 1879, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07862401008605957, \"iteration\": 1880, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00010593688057269901, \"iteration\": 1881, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004412406589835882, \"iteration\": 1882, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.006116025149822235, \"iteration\": 1883, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005752256140112877, \"iteration\": 1884, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007284448365680873, \"iteration\": 1885, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002634127624332905, \"iteration\": 1886, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007740780711174011, \"iteration\": 1887, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000651516136713326, \"iteration\": 1888, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00820864923298359, \"iteration\": 1889, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014794713351875544, \"iteration\": 1890, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005966252647340298, \"iteration\": 1891, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001838329597376287, \"iteration\": 1892, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017950977198779583, \"iteration\": 1893, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007008559186942875, \"iteration\": 1894, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002169826766476035, \"iteration\": 1895, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0020817324984818697, \"iteration\": 1896, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009010434150695801, \"iteration\": 1897, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0126925278455019, \"iteration\": 1898, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014497442170977592, \"iteration\": 1899, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004018344450742006, \"iteration\": 1900, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003586333245038986, \"iteration\": 1901, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011398021597415209, \"iteration\": 1902, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006336637306958437, \"iteration\": 1903, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002177849877625704, \"iteration\": 1904, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000889923598151654, \"iteration\": 1905, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000861446897033602, \"iteration\": 1906, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018969390075653791, \"iteration\": 1907, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007465903181582689, \"iteration\": 1908, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0021690470166504383, \"iteration\": 1909, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.017327245324850082, \"iteration\": 1910, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.009123509749770164, \"iteration\": 1911, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004422657657414675, \"iteration\": 1912, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010601325193420053, \"iteration\": 1913, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00113106076605618, \"iteration\": 1914, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018421649001538754, \"iteration\": 1915, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001551526365801692, \"iteration\": 1916, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010999159421771765, \"iteration\": 1917, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011495028156787157, \"iteration\": 1918, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015170684782788157, \"iteration\": 1919, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0046225921250879765, \"iteration\": 1920, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013246203307062387, \"iteration\": 1921, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0024962814059108496, \"iteration\": 1922, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015984519850462675, \"iteration\": 1923, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005398192442953587, \"iteration\": 1924, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018407555762678385, \"iteration\": 1925, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001363828545436263, \"iteration\": 1926, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011625399347394705, \"iteration\": 1927, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006782650016248226, \"iteration\": 1928, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011818456696346402, \"iteration\": 1929, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003143864218145609, \"iteration\": 1930, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.025020143017172813, \"iteration\": 1931, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0020189008209854364, \"iteration\": 1932, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021409234032034874, \"iteration\": 1933, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.010453621856868267, \"iteration\": 1934, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004787712823599577, \"iteration\": 1935, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004951824899762869, \"iteration\": 1936, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004190312698483467, \"iteration\": 1937, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00783686712384224, \"iteration\": 1938, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0026660363655537367, \"iteration\": 1939, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001281745615415275, \"iteration\": 1940, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00048613722901791334, \"iteration\": 1941, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008896142826415598, \"iteration\": 1942, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004890772979706526, \"iteration\": 1943, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.008774664252996445, \"iteration\": 1944, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.022231807932257652, \"iteration\": 1945, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0033068531192839146, \"iteration\": 1946, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004464070312678814, \"iteration\": 1947, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016411119140684605, \"iteration\": 1948, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004040366970002651, \"iteration\": 1949, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018863240256905556, \"iteration\": 1950, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.008907871320843697, \"iteration\": 1951, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017383175436407328, \"iteration\": 1952, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002169704530388117, \"iteration\": 1953, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017606006003916264, \"iteration\": 1954, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010164480190724134, \"iteration\": 1955, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000933646282646805, \"iteration\": 1956, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010549339931458235, \"iteration\": 1957, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007558454526588321, \"iteration\": 1958, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007534103351645172, \"iteration\": 1959, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015928829088807106, \"iteration\": 1960, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013946935068815947, \"iteration\": 1961, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001591833308339119, \"iteration\": 1962, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009321927791461349, \"iteration\": 1963, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011188205098733306, \"iteration\": 1964, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0020676101557910442, \"iteration\": 1965, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017973268404603004, \"iteration\": 1966, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009322005789726973, \"iteration\": 1967, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009835833916440606, \"iteration\": 1968, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0027543483301997185, \"iteration\": 1969, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001132710138335824, \"iteration\": 1970, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003912204410880804, \"iteration\": 1971, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004145610146224499, \"iteration\": 1972, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0023900754749774933, \"iteration\": 1973, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003265892155468464, \"iteration\": 1974, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007717898115515709, \"iteration\": 1975, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007616493385285139, \"iteration\": 1976, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0021707508713006973, \"iteration\": 1977, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015177603345364332, \"iteration\": 1978, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.010189059190452099, \"iteration\": 1979, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010741651058197021, \"iteration\": 1980, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0021282376255840063, \"iteration\": 1981, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017657446442171931, \"iteration\": 1982, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006865382310934365, \"iteration\": 1983, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.007577541284263134, \"iteration\": 1984, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010599980596452951, \"iteration\": 1985, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0022610239684581757, \"iteration\": 1986, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001354022417217493, \"iteration\": 1987, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017068672459572554, \"iteration\": 1988, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005488424561917782, \"iteration\": 1989, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013424137141555548, \"iteration\": 1990, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017438629874959588, \"iteration\": 1991, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003019513562321663, \"iteration\": 1992, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004997721873223782, \"iteration\": 1993, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003985493676736951, \"iteration\": 1994, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007895015296526253, \"iteration\": 1995, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001654869643971324, \"iteration\": 1996, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009290028247050941, \"iteration\": 1997, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00174569187220186, \"iteration\": 1998, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005995848216116428, \"iteration\": 1999, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0025509577244520187, \"iteration\": 2000, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0026960663963109255, \"iteration\": 2001, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018010676139965653, \"iteration\": 2002, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006110346876084805, \"iteration\": 2003, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004745716229081154, \"iteration\": 2004, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010787167120724916, \"iteration\": 2005, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000579881714656949, \"iteration\": 2006, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0038642841391265392, \"iteration\": 2007, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004302567336708307, \"iteration\": 2008, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0012324426788836718, \"iteration\": 2009, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005070046056061983, \"iteration\": 2010, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017513753846287727, \"iteration\": 2011, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028089137747883797, \"iteration\": 2012, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0012503652833402157, \"iteration\": 2013, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013397173024713993, \"iteration\": 2014, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004824374802410603, \"iteration\": 2015, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002023364184424281, \"iteration\": 2016, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001295731752179563, \"iteration\": 2017, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004931614268571138, \"iteration\": 2018, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016381582245230675, \"iteration\": 2019, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005444420385174453, \"iteration\": 2020, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009447736665606499, \"iteration\": 2021, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017823640955612063, \"iteration\": 2022, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007499490166082978, \"iteration\": 2023, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002062796615064144, \"iteration\": 2024, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003384524490684271, \"iteration\": 2025, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018425234593451023, \"iteration\": 2026, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011424770345911384, \"iteration\": 2027, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008652795222587883, \"iteration\": 2028, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007312471861951053, \"iteration\": 2029, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007324013276956975, \"iteration\": 2030, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009375750669278204, \"iteration\": 2031, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009146180236712098, \"iteration\": 2032, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009717307984828949, \"iteration\": 2033, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009629693813621998, \"iteration\": 2034, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015371767804026604, \"iteration\": 2035, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001566014252603054, \"iteration\": 2036, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015345783904194832, \"iteration\": 2037, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016711753560230136, \"iteration\": 2038, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010066282702609897, \"iteration\": 2039, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018428361508995295, \"iteration\": 2040, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00527397682890296, \"iteration\": 2041, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.007175103295594454, \"iteration\": 2042, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.006436836440116167, \"iteration\": 2043, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011150315403938293, \"iteration\": 2044, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014810555148869753, \"iteration\": 2045, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006963019259274006, \"iteration\": 2046, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00864126905798912, \"iteration\": 2047, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008714926661923528, \"iteration\": 2048, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0035107661969959736, \"iteration\": 2049, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005619263392873108, \"iteration\": 2050, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.007424249313771725, \"iteration\": 2051, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010627356823533773, \"iteration\": 2052, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010088267736136913, \"iteration\": 2053, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0027862885035574436, \"iteration\": 2054, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001182173378765583, \"iteration\": 2055, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009064283804036677, \"iteration\": 2056, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0026380931958556175, \"iteration\": 2057, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001251747366040945, \"iteration\": 2058, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0020856508053839207, \"iteration\": 2059, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014239936135709286, \"iteration\": 2060, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016884575597941875, \"iteration\": 2061, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004173936787992716, \"iteration\": 2062, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0029444315005093813, \"iteration\": 2063, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015545128844678402, \"iteration\": 2064, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005309700034558773, \"iteration\": 2065, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010787340579554439, \"iteration\": 2066, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004650110378861427, \"iteration\": 2067, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003859481308609247, \"iteration\": 2068, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0025765420868992805, \"iteration\": 2069, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019725405145436525, \"iteration\": 2070, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001472561270929873, \"iteration\": 2071, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.027098190039396286, \"iteration\": 2072, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000636612472590059, \"iteration\": 2073, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002672947011888027, \"iteration\": 2074, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011098384857177734, \"iteration\": 2075, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003812897251918912, \"iteration\": 2076, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.015325861051678658, \"iteration\": 2077, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003977376967668533, \"iteration\": 2078, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0020001789089292288, \"iteration\": 2079, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010529085993766785, \"iteration\": 2080, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015356652438640594, \"iteration\": 2081, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.029529932886362076, \"iteration\": 2082, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.010022019036114216, \"iteration\": 2083, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016885374207049608, \"iteration\": 2084, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015919215511530638, \"iteration\": 2085, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.011929645203053951, \"iteration\": 2086, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.02212543413043022, \"iteration\": 2087, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019344498869031668, \"iteration\": 2088, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014109602198004723, \"iteration\": 2089, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005107776378281415, \"iteration\": 2090, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Prepare data...\")\n",
    "test_nn_chars = encode_data(test_raw, chars_encoder)\n",
    "\n",
    "print(\"Train model\")\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "USE_CACHE = True\n",
    "\n",
    "model_nn_chars = NeuralNetwork(\n",
    "    input_size=len(chars_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn_chars.pt').exists() and USE_CACHE:\n",
    "    model_nn_chars = load_model(model_nn_chars, models_dir, 'model_nn_chars')\n",
    "else:\n",
    "    train_nn_chars = encode_data(train_raw, chars_encoder)\n",
    "    dataloader = DataLoader(train_nn_chars, batch_size=128, shuffle=True)\n",
    "\n",
    "    model_nn_chars.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn_chars, models_dir, \"model_nn_chars\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_test = torch.stack([test[0] for test in test_nn_chars]).to(model_nn_chars.device)\n",
    "    y_test = torch.stack([test[1] for test in test_nn_chars]).to(model_nn_chars.device)\n",
    "    y_pred = model_nn_chars.predict(X_test)\n",
    "    logits = model_nn_chars.forward(X_test)\n",
    "\n",
    "result_nn_chars = evaluate(y_test.cpu(), y_pred.cpu(), logits.cpu())\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn_chars, train_config, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "### Word feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/209 [00:00<?, ?batch/s]/media/hapham/Work/study/power-identification/utils.py:94: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  tokens_sparse = torch.sparse_csr_tensor(crow, col, token_val, size=mat_size, dtype=torch.long)\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:33<00:00,  2.23batch/s, batch_accuracy=0.714, loss=9.66]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:33<00:00,  2.24batch/s, batch_accuracy=0.714, loss=7.41]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:33<00:00,  2.23batch/s, batch_accuracy=0.714, loss=2.86]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:33<00:00,  2.23batch/s, batch_accuracy=1, loss=11.1]   \n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:33<00:00,  2.24batch/s, batch_accuracy=1, loss=10.9]   \n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:34<00:00,  2.22batch/s, batch_accuracy=1, loss=5.56]   \n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:34<00:00,  2.22batch/s, batch_accuracy=0.571, loss=2.64]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:34<00:00,  2.22batch/s, batch_accuracy=0.857, loss=4.01]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:34<00:00,  2.21batch/s, batch_accuracy=0.857, loss=4.43]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:36<00:00,  2.17batch/s, batch_accuracy=1, loss=3.33]   \n",
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [00:31<00:00,  1.63batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7255, Precision: 0.7036, Recall: 0.8787, F1: 0.7814, AUC: 0.7682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prepare data encoder...\")\n",
    "rnn_words_encoder = PositionalEncoder()\n",
    "rnn_words_encoder.fit(train_raw.texts)\n",
    "\n",
    "train_dataloader = DataLoader(train_raw, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_raw, batch_size=128, shuffle=False)\n",
    "\n",
    "# Prepare baseline config\n",
    "train_config = TrainConfig(\n",
    "    optimizer_params = {'lr': 0.01},\n",
    "    num_epochs       = 10,\n",
    "    early_stop       = False,\n",
    "    violation_limit  = 5\n",
    ")\n",
    "\n",
    "# Train baseline model\n",
    "model_lstm_words = RNNClassifier(\n",
    "    rnn_network         = nn.LSTM,\n",
    "    word_embedding_dim  = 32,\n",
    "    hidden_dim          = 64,\n",
    "    bidirectional       = False,\n",
    "    dropout             = 0,\n",
    "    encoder             = rnn_words_encoder,\n",
    "    device              = 'cuda'\n",
    ")\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "if (models_dir / 'model_lstm_words.pt').exists() and USE_CACHE:\n",
    "    model_lstm_words = load_model(model_lstm_words, 'model_lstm_words')\n",
    "else:\n",
    "    model_lstm_words.fit(train_dataloader, train_config, no_progress_bar=False)\n",
    "    save_model(model_lstm_words, models_dir, \"model_lstm_words\")\n",
    "\n",
    "test_dataloader = DataLoader(test_raw, batch_size=128, shuffle=False)\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    model_lstm_words.device = \"cpu\"\n",
    "    model_lstm_words.cpu()\n",
    "\n",
    "    pred_LSTM_words_lst = []\n",
    "    probs_LSTM_words_lst = []\n",
    "\n",
    "    for _, _, raw_inputs, raw_targets in tqdm(test_dataloader, unit=\"batch\", desc=\"Predicting\"):\n",
    "        batch_encoder = PositionalEncoder(vocabulary=rnn_words_encoder.vocabulary)\n",
    "        test_inputs = batch_encoder.fit_transform(raw_inputs).cpu()\n",
    "        test_targets = torch.as_tensor(raw_targets, dtype=torch.float).cpu()\n",
    "        \n",
    "        pred_LSTM_words_lst.append(model_lstm_words.predict(test_inputs))\n",
    "        probs_LSTM_words_lst.append(model_lstm_words._sigmoid(model_lstm_words.forward(test_inputs)).squeeze())\n",
    "\n",
    "\n",
    "pred_LSTM_words = torch.cat(pred_LSTM_words_lst).long().numpy()\n",
    "probs_LSTM_words = torch.concat(probs_LSTM_words_lst).numpy()\n",
    "\n",
    "model_lstm_words_result = evaluate(test_raw.labels, pred_LSTM_words, probs_LSTM_words)\n",
    "\n",
    "np.save(models_dir / 'model_lstm_words_results.npy', model_lstm_words_result)\n",
    "\n",
    "model_lstm_words.cpu()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:34<00:00,  2.20batch/s, batch_accuracy=0.571, loss=7.78]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:35<00:00,  2.20batch/s, batch_accuracy=0.571, loss=8.04]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:34<00:00,  2.22batch/s, batch_accuracy=0.571, loss=4.85]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:36<00:00,  2.17batch/s, batch_accuracy=0.857, loss=11.3]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:35<00:00,  2.18batch/s, batch_accuracy=0.857, loss=6.47]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:35<00:00,  2.20batch/s, batch_accuracy=0.714, loss=6.7]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:35<00:00,  2.19batch/s, batch_accuracy=1, loss=10.8]   \n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:35<00:00,  2.19batch/s, batch_accuracy=1, loss=1.44]   \n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:36<00:00,  2.16batch/s, batch_accuracy=1, loss=10.8]   \n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 209/209 [01:36<00:00,  2.17batch/s, batch_accuracy=1, loss=5.61]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5314, Precision: 0.5581, Recall: 0.7733, F1: 0.6483, AUC: 0.4975\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prepare data encoder...\")\n",
    "rnn_chars_encoder = PositionalEncoder(tokenizer=chars_encoder.build_tokenizer())\n",
    "rnn_chars_encoder.fit(train_raw.texts)\n",
    "\n",
    "train_dataloader = DataLoader(train_raw, batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(test_raw, batch_size=128, shuffle=False)\n",
    "\n",
    "test_inputs = rnn_chars_encoder.transform(test_raw.texts)\n",
    "\n",
    "# Prepare baseline config\n",
    "train_config = TrainConfig(\n",
    "    optimizer_params = {'lr': 0.01},\n",
    "    num_epochs       = 10,\n",
    "    early_stop       = False,\n",
    "    violation_limit  = 5\n",
    ")\n",
    "\n",
    "# Train baseline model\n",
    "model_lstm_chars = RNNClassifier(\n",
    "    rnn_network         = nn.LSTM,\n",
    "    word_embedding_dim  = 32,\n",
    "    hidden_dim          = 64,\n",
    "    bidirectional       = False,\n",
    "    dropout             = 0,\n",
    "    encoder             = rnn_chars_encoder,\n",
    "    device              = 'cuda'\n",
    ")\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "if (models_dir / 'model_lstm_chars.pt').exists() and USE_CACHE:\n",
    "    model_lstm_chars = load_model(model_lstm_chars, 'model_lstm_chars')\n",
    "else:\n",
    "    model_lstm_chars.fit(train_dataloader, train_config, no_progress_bar=False)\n",
    "    save_model(model_lstm_chars, models_dir, \"model_lstm_chars\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_lstm_chars.device = \"cpu\"\n",
    "    model_lstm_chars.cpu()\n",
    "\n",
    "    pred_LSTM_chars = []\n",
    "    logits_LSTM_chars = []\n",
    "\n",
    "    for _, _, raw_inputs, raw_targets in tqdm(test_dataloader, unit=\"batch\", desc=\"Predicting\"):\n",
    "        batch_encoder = PositionalEncoder(vocabulary=rnn_chars_encoder.vocabulary)\n",
    "        test_inputs = batch_encoder.fit_transform(raw_inputs).cpu()\n",
    "        test_targets = torch.as_tensor(raw_targets, dtype=torch.float).cpu()\n",
    "\n",
    "        pred_LSTM_chars.append(model_lstm_chars.predict(test_inputs))\n",
    "        logits_LSTM_chars.append(model_lstm_chars.forward(test_inputs))\n",
    "\n",
    "pred_LSTM_chars = torch.concat(pred_LSTM_chars).numpy()\n",
    "logits_LSTM_chars = torch.concat(logits_LSTM_chars).numpy()\n",
    "\n",
    "model_lstm_chars_result = evaluate(test_raw.labels, pred_LSTM_chars, logits_LSTM_chars)\n",
    "# print(model_lstm_chars_result)\n",
    "\n",
    "np.save(models_dir / 'model_lstm_chars_results.npy', model_lstm_chars_result)\n",
    "model_lstm_words.cpu()\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other classifiers from sklearn\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "\"Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train & test set\n",
    "X_train_skl_words = words_encoder.transform(train_raw.texts)\n",
    "X_test_skl_words = words_encoder.transform(test_raw.texts)\n",
    "\n",
    "X_train_skl_chars = chars_encoder.transform(train_raw.texts)\n",
    "X_test_skl_chars = chars_encoder.transform(test_raw.texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC\n",
    "Effective in high dimensional spaces.\n",
    "\n",
    "Still effective in cases where number of dimensions is greater than the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC with TfIdf did good on balanced English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7468, Precision: 0.7452, Recall: 0.8306, F1: 0.7856, AUC: 0.8251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# LinearSVC, word\n",
    "\n",
    "\n",
    "print(\"Fit model\")\n",
    "base_svc = LinearSVC()\n",
    "model_LinearSVC_words = CalibratedClassifierCV(estimator=base_svc, cv=5)\n",
    "model_LinearSVC_words.fit(X_train_skl_words, train_raw.labels)\n",
    "\n",
    "pred_LinearSVC_words = model_LinearSVC_words.predict(X_test_skl_words)\n",
    "logits_linearSVC_words = model_LinearSVC_words.predict_proba(X_test_skl_words)\n",
    "\n",
    "result_linearSVC_words =  evaluate(test_raw.labels, pred_LinearSVC_words, logits_linearSVC_words[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7757, Precision: 0.7731, Recall: 0.8471, F1: 0.8084, AUC: 0.8514\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "print(\"Fit model\")\n",
    "base_svc = LinearSVC()\n",
    "model_LinearSVC_chars = CalibratedClassifierCV(estimator=base_svc, cv=5)\n",
    "model_LinearSVC_chars.fit(X_train_skl_chars, train_raw.labels)\n",
    "\n",
    "pred_LinearSVC_chars = model_LinearSVC_chars.predict(X_test_skl_chars)\n",
    "logits_linearSVC_chars = model_LinearSVC_chars.predict_proba(X_test_skl_chars)\n",
    "\n",
    "result_linearSVC_chars =  evaluate(test_raw.labels, pred_LinearSVC_chars, logits_linearSVC_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7628, Precision: 0.7515, Recall: 0.8595, F1: 0.8019, AUC: 0.8357\n"
     ]
    }
   ],
   "source": [
    "# Word features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_logreg_words = LogisticRegression()\n",
    "model_logreg_words.fit(X_train_skl_words, train_raw.labels)\n",
    "\n",
    "pred_logreg_words = model_logreg_words.predict(X_test_skl_words)\n",
    "logits_logreg_words = model_logreg_words.predict_proba(X_test_skl_words)\n",
    "\n",
    "result_linearlogreg_words =  evaluate(test_raw.labels, pred_logreg_words, logits_logreg_words[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7800, Precision: 0.7694, Recall: 0.8654, F1: 0.8146, AUC: 0.8573\n"
     ]
    }
   ],
   "source": [
    "# char features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_logreg_chars = LogisticRegression()\n",
    "model_logreg_chars.fit(X_train_skl_chars, train_raw.labels)\n",
    "\n",
    "pred_logreg_chars = model_logreg_chars.predict(X_test_skl_chars)\n",
    "logits_logreg_chars = model_logreg_chars.predict_proba(X_test_skl_chars)\n",
    "\n",
    "result_linearlogreg_chars =  evaluate(test_raw.labels, pred_logreg_chars, logits_logreg_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier\n",
    "SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "\n",
    "SGD is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7552, Precision: 0.7364, Recall: 0.8749, F1: 0.7997, AUC: 0.8299\n"
     ]
    }
   ],
   "source": [
    "# word features\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_sgd_words = SGDClassifier(loss='log_loss')\n",
    "model_sgd_words.fit(X_train_skl_words, train_raw.labels)\n",
    "\n",
    "pred_sgd_words = model_sgd_words.predict(X_test_skl_words)\n",
    "logits_sgd_words = model_sgd_words.predict_proba(X_test_skl_words)\n",
    "\n",
    "result_linearsgd_words =  evaluate(test_raw.labels, pred_sgd_words, logits_sgd_words[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7760, Precision: 0.7663, Recall: 0.8619, F1: 0.8113, AUC: 0.8516\n"
     ]
    }
   ],
   "source": [
    "# chars features\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_sgd_chars = SGDClassifier(loss='log_loss')\n",
    "model_sgd_chars.fit(X_train_skl_chars, train_raw.labels)\n",
    "\n",
    "pred_sgd_chars = model_sgd_chars.predict(X_test_skl_chars)\n",
    "logits_sgd_chars = model_sgd_chars.predict_proba(X_test_skl_chars)\n",
    "\n",
    "result_linearsgd_chars =  evaluate(test_raw.labels, pred_sgd_chars, logits_sgd_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Overall bad performance, not worth pursuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.5340, Precision: 0.6202, Recall: 0.4272, F1: 0.5059, AUC: 0.5490\n"
     ]
    }
   ],
   "source": [
    "# words features\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_gnb_words = GaussianNB()\n",
    "model_gnb_words.fit(X_train_skl_words.toarray(), train_raw.labels)\n",
    "\n",
    "pred_gnb_words = model_gnb_words.predict(X_test_skl_words.toarray())\n",
    "logits_gnb_words = model_gnb_words.predict_proba(X_test_skl_words.toarray())\n",
    "\n",
    "result_lineargnb_words =  evaluate(test_raw.labels, pred_gnb_words, logits_gnb_words[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Accuracy: 0.7037, Precision: 0.7322, Recall: 0.7403, F1: 0.7363, AUC: 0.7258\n"
     ]
    }
   ],
   "source": [
    "# chars features\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_gnb_chars = GaussianNB()\n",
    "model_gnb_chars.fit(X_train_skl_chars.toarray(), train_raw.labels)\n",
    "\n",
    "pred_gnb_chars = model_gnb_chars.predict(X_test_skl_chars.toarray())\n",
    "logits_gnb_chars = model_gnb_chars.predict_proba(X_test_skl_chars.toarray())\n",
    "\n",
    "result_lineargnb_chars =  evaluate(test_raw.labels, pred_gnb_chars, logits_gnb_chars[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data for xgboost\n",
    "train_dmat_words = xgb.DMatrix(X_train_skl_words, pd.array(train_raw.labels).astype('category'))\n",
    "test_dmat_words = xgb.DMatrix(X_test_skl_words, pd.array(test_raw.labels).astype('category'))\n",
    "\n",
    "train_dmat_chars = xgb.DMatrix(X_train_skl_chars, pd.array(train_raw.labels).astype('category'))\n",
    "test_dmat_chars = xgb.DMatrix(X_test_skl_chars, pd.array(test_raw.labels).astype('category'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.62296\n",
      "[100]\ttrain-logloss:0.26590\n",
      "[200]\ttrain-logloss:0.18824\n",
      "[300]\ttrain-logloss:0.13465\n",
      "[400]\ttrain-logloss:0.09987\n",
      "[500]\ttrain-logloss:0.07312\n",
      "[600]\ttrain-logloss:0.05486\n",
      "[700]\ttrain-logloss:0.04140\n",
      "[800]\ttrain-logloss:0.03124\n",
      "[900]\ttrain-logloss:0.02381\n",
      "[1000]\ttrain-logloss:0.01857\n",
      "[1100]\ttrain-logloss:0.01495\n",
      "[1200]\ttrain-logloss:0.01230\n",
      "[1300]\ttrain-logloss:0.01008\n",
      "[1400]\ttrain-logloss:0.00856\n",
      "[1500]\ttrain-logloss:0.00722\n",
      "[1600]\ttrain-logloss:0.00625\n",
      "[1700]\ttrain-logloss:0.00558\n",
      "[1800]\ttrain-logloss:0.00483\n",
      "[1900]\ttrain-logloss:0.00438\n",
      "[1999]\ttrain-logloss:0.00391\n",
      "Accuracy: 0.7661, Precision: 0.7630, Recall: 0.8430, F1: 0.8010, AUC: 0.8420\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"device\": \"cpu\",\n",
    "    \"objective\": \"binary:logistic\",  # there is also binary:hinge but hinge does not output probability\n",
    "    \"tree_method\": \"hist\",  # default to hist\n",
    "    \"device\": \"cuda\",\n",
    "\n",
    "    # Params for tree booster\n",
    "    \"eta\": 0.3,\n",
    "    \"gamma\": 0.0,  # Min loss achieved to split the tree\n",
    "    \"max_depth\": 6,\n",
    "    \"reg_alpha\": 0,\n",
    "    \"reg_lambda\": 1,\n",
    "\n",
    "}\n",
    "evals_words = [(train_dmat_words, \"train\")]\n",
    "iterations = 2000\n",
    "\n",
    "model_xgb_words = xgb.train(\n",
    "    params = params,\n",
    "    dtrain = train_dmat_words,\n",
    "    num_boost_round = iterations,\n",
    "    evals = evals_words,\n",
    "    verbose_eval = 100\n",
    ")\n",
    "\n",
    "pred_xgb_words_probs = model_xgb_words.predict(test_dmat_words)\n",
    "result_xgb_words = evaluate(test_raw.labels, pred_xgb_words_probs > 0.5, pred_xgb_words_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.62296\n",
      "[100]\ttrain-logloss:0.26590\n",
      "[200]\ttrain-logloss:0.18824\n",
      "[300]\ttrain-logloss:0.13465\n",
      "[400]\ttrain-logloss:0.09987\n",
      "[500]\ttrain-logloss:0.07312\n",
      "[600]\ttrain-logloss:0.05486\n",
      "[700]\ttrain-logloss:0.04140\n",
      "[800]\ttrain-logloss:0.03124\n",
      "[900]\ttrain-logloss:0.02381\n",
      "[1000]\ttrain-logloss:0.01857\n",
      "[1100]\ttrain-logloss:0.01495\n",
      "[1200]\ttrain-logloss:0.01230\n",
      "[1300]\ttrain-logloss:0.01008\n",
      "[1400]\ttrain-logloss:0.00856\n",
      "[1500]\ttrain-logloss:0.00722\n",
      "[1600]\ttrain-logloss:0.00625\n",
      "[1700]\ttrain-logloss:0.00558\n",
      "[1800]\ttrain-logloss:0.00483\n",
      "[1900]\ttrain-logloss:0.00438\n",
      "[1999]\ttrain-logloss:0.00391\n",
      "Accuracy: 0.7661, Precision: 0.7630, Recall: 0.8430, F1: 0.8010, AUC: 0.8420\n"
     ]
    }
   ],
   "source": [
    "# Can use only half of the original max features\n",
    "xgb_chars_encoder = TfidfVectorizer(max_features=20000, analyzer=\"char\", ngram_range=(3,5), use_idf=True, sublinear_tf=True)\n",
    "xgb_chars_encoder.fit(train_raw.texts)\n",
    "\n",
    "# Prepare train & test set\n",
    "X_train_xgb_chars = xgb_chars_encoder.transform(train_raw.texts)\n",
    "X_test_xgb_chars = xgb_chars_encoder.transform(test_raw.texts)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "train_dmat_chars = xgb.DMatrix(X_train_xgb_chars, pd.array(train_raw.labels).astype('category'))\n",
    "test_dmat_chars = xgb.DMatrix(X_test_xgb_chars, pd.array(test_raw.labels).astype('category'))\n",
    "\n",
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"device\": \"cpu\",\n",
    "    \"objective\": \"binary:logistic\",  # there is also binary:hinge but hinge does not output probability\n",
    "    \"tree_method\": \"hist\",  # default to hist\n",
    "    \"device\": \"cuda\",\n",
    "\n",
    "    # Params for tree booster\n",
    "    \"eta\": 0.3,\n",
    "    \"gamma\": 0.0,  # Min loss achieved to split the tree\n",
    "    \"max_depth\": 6,\n",
    "    \"reg_alpha\": 0,\n",
    "    \"reg_lambda\": 1,\n",
    "\n",
    "}\n",
    "evals_chars = [(train_dmat_chars, \"train\")]\n",
    "iterations = 2000\n",
    "\n",
    "model_xgb_chars = xgb.train(\n",
    "    params = params,\n",
    "    dtrain = train_dmat_chars,\n",
    "    num_boost_round = iterations,\n",
    "    evals = evals_chars,\n",
    "    verbose_eval = 100\n",
    ")\n",
    "\n",
    "pred_xgb_chars_probs = model_xgb_chars.predict(test_dmat_chars)\n",
    "result_xgb_chars = evaluate(test_raw.labels, pred_xgb_chars_probs > 0.5, pred_xgb_chars_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-identification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

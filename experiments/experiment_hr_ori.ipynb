{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "\n",
    "from models import NeuralNetwork, TrainConfig, evaluate_nn_model, save_model, load_model, plot_results\n",
    "from utils import load_data, split_data, encode_data, mapping_dict\n",
    "from pathlib import Path\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(\"Device: cuda\")\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    print(\"Device: cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(folder_path=\"data/train/power/\", file_list=['power-hr-train.tsv'],text_head='text')\n",
    "train_raw, test_raw = split_data(data, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data encoder...\n",
      "Prepare data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prepare data encoder...\")\n",
    "tfidf_encoder = TfidfVectorizer(max_features=50000)\n",
    "tfidf_encoder.fit(train_raw.texts)\n",
    "\n",
    "print(\"Prepare data...\")\n",
    "train_data_nn = encode_data(train_raw, tfidf_encoder)\n",
    "test_data_nn = encode_data(test_raw, tfidf_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/68 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 68/68 [00:00<00:00, 81.29batch/s, batch_accuracy=0.675, loss=0.602]\n",
      "Epoch 2: 100%|██████████| 68/68 [00:00<00:00, 83.85batch/s, batch_accuracy=0.818, loss=0.51] \n",
      "Epoch 3: 100%|██████████| 68/68 [00:00<00:00, 89.02batch/s, batch_accuracy=0.935, loss=0.281]\n",
      "Epoch 4: 100%|██████████| 68/68 [00:00<00:00, 84.95batch/s, batch_accuracy=0.987, loss=0.123] \n",
      "Epoch 5: 100%|██████████| 68/68 [00:00<00:00, 88.68batch/s, batch_accuracy=0.974, loss=0.0692]\n",
      "Epoch 6: 100%|██████████| 68/68 [00:00<00:00, 78.36batch/s, batch_accuracy=1, loss=0.00678]   \n",
      "Epoch 7: 100%|██████████| 68/68 [00:00<00:00, 72.39batch/s, batch_accuracy=1, loss=0.00374]   \n",
      "Epoch 8: 100%|██████████| 68/68 [00:01<00:00, 65.91batch/s, batch_accuracy=1, loss=0.00127]    \n",
      "Epoch 9: 100%|██████████| 68/68 [00:00<00:00, 73.32batch/s, batch_accuracy=1, loss=0.000529]\n",
      "Epoch 10: 100%|██████████| 68/68 [00:00<00:00, 79.38batch/s, batch_accuracy=1, loss=0.000966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5334207077326344, 0.5145385587863464, 0.5238095238095238, None)\n",
      "AUC 0.6200294952759797\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-684e227ff19142daa6719a8d62a960cf.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-684e227ff19142daa6719a8d62a960cf.vega-embed details,\n",
       "  #altair-viz-684e227ff19142daa6719a8d62a960cf.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-684e227ff19142daa6719a8d62a960cf\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-684e227ff19142daa6719a8d62a960cf\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-684e227ff19142daa6719a8d62a960cf\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-6ba9581e1d3caed4d599d4e19a5f4480\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-6ba9581e1d3caed4d599d4e19a5f4480\": [{\"training_acc\": 0.625, \"training_loss\": 0.6903343200683594, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6909722089767456, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.685139536857605, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 0.6918458938598633, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6802948713302612, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6808996796607971, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6827007532119751, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6824434399604797, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6706165075302124, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6783038377761841, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 0.6973037123680115, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 0.6894251704216003, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6882959604263306, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6736505031585693, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6763061285018921, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6517360210418701, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.5, \"training_loss\": 0.7048043012619019, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6809593439102173, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6793280243873596, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6646914482116699, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6659722924232483, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6617559194564819, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6461666226387024, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6788259148597717, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.679409384727478, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.659471333026886, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6319507956504822, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6673734188079834, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6439790725708008, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6642922759056091, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6591367125511169, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6481307744979858, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6586334705352783, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6288121342658997, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6680198311805725, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6856027841567993, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6520538926124573, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.626565158367157, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6731595993041992, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6282652616500854, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.644844651222229, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6899802684783936, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.666530966758728, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6355390548706055, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6472100019454956, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6621633172035217, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6467878818511963, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6399873495101929, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6674879193305969, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.628359317779541, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.664182186126709, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 0.6528619527816772, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6289329528808594, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6628713607788086, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6465953588485718, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.617212176322937, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6182777285575867, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6370986104011536, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6763602495193481, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.629612922668457, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.5737394690513611, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6396181583404541, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6089904308319092, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6315255165100098, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.592843234539032, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6272711753845215, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6230990290641785, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.6753246753246753, \"training_loss\": 0.6021231412887573, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5467986464500427, \"iteration\": 69, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5453937649726868, \"iteration\": 70, \"epoch\": 2}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5392787456512451, \"iteration\": 71, \"epoch\": 2}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6088378429412842, \"iteration\": 72, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5571355223655701, \"iteration\": 73, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.5692039728164673, \"iteration\": 74, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5730238556861877, \"iteration\": 75, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5145491361618042, \"iteration\": 76, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5287479162216187, \"iteration\": 77, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.5450795292854309, \"iteration\": 78, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5284378528594971, \"iteration\": 79, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5314007997512817, \"iteration\": 80, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 0.5374653339385986, \"iteration\": 81, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.5117226839065552, \"iteration\": 82, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.49557721614837646, \"iteration\": 83, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5695945024490356, \"iteration\": 84, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.48453807830810547, \"iteration\": 85, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.47549888491630554, \"iteration\": 86, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5326581597328186, \"iteration\": 87, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5221537351608276, \"iteration\": 88, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.5040348768234253, \"iteration\": 89, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.5077630281448364, \"iteration\": 90, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.5017690658569336, \"iteration\": 91, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.500559389591217, \"iteration\": 92, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.4880906641483307, \"iteration\": 93, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4717698395252228, \"iteration\": 94, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4641777575016022, \"iteration\": 95, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.46606263518333435, \"iteration\": 96, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.46280810236930847, \"iteration\": 97, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4422149956226349, \"iteration\": 98, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4366716742515564, \"iteration\": 99, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4050423502922058, \"iteration\": 100, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.45337140560150146, \"iteration\": 101, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4346531927585602, \"iteration\": 102, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.42407381534576416, \"iteration\": 103, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.45944303274154663, \"iteration\": 104, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4518570601940155, \"iteration\": 105, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4371298849582672, \"iteration\": 106, \"epoch\": 2}, {\"training_acc\": 0.8984375, \"training_loss\": 0.40845006704330444, \"iteration\": 107, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3883828818798065, \"iteration\": 108, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.4155638813972473, \"iteration\": 109, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.4497160315513611, \"iteration\": 110, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.42303362488746643, \"iteration\": 111, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.45279228687286377, \"iteration\": 112, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4571165144443512, \"iteration\": 113, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.44993072748184204, \"iteration\": 114, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.46631118655204773, \"iteration\": 115, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.43255069851875305, \"iteration\": 116, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.40501171350479126, \"iteration\": 117, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.38723117113113403, \"iteration\": 118, \"epoch\": 2}, {\"training_acc\": 0.921875, \"training_loss\": 0.3194693624973297, \"iteration\": 119, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.37196531891822815, \"iteration\": 120, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.42511171102523804, \"iteration\": 121, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.4829118549823761, \"iteration\": 122, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.39182817935943604, \"iteration\": 123, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4392656981945038, \"iteration\": 124, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.38680243492126465, \"iteration\": 125, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.40284234285354614, \"iteration\": 126, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4690850079059601, \"iteration\": 127, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.33511823415756226, \"iteration\": 128, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4477246105670929, \"iteration\": 129, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.42865654826164246, \"iteration\": 130, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4218125641345978, \"iteration\": 131, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3880421817302704, \"iteration\": 132, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.41173338890075684, \"iteration\": 133, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.41016092896461487, \"iteration\": 134, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4196692705154419, \"iteration\": 135, \"epoch\": 2}, {\"training_acc\": 0.8181818181818182, \"training_loss\": 0.510239839553833, \"iteration\": 136, \"epoch\": 2}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1932620406150818, \"iteration\": 137, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.24396944046020508, \"iteration\": 138, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.22567227482795715, \"iteration\": 139, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.16660434007644653, \"iteration\": 140, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.3047201931476593, \"iteration\": 141, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2799017131328583, \"iteration\": 142, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.20623400807380676, \"iteration\": 143, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2686621844768524, \"iteration\": 144, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.17762085795402527, \"iteration\": 145, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20682509243488312, \"iteration\": 146, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.2596071660518646, \"iteration\": 147, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22960570454597473, \"iteration\": 148, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.23689337074756622, \"iteration\": 149, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18370628356933594, \"iteration\": 150, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.17485105991363525, \"iteration\": 151, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.22115558385849, \"iteration\": 152, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21900610625743866, \"iteration\": 153, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2241429090499878, \"iteration\": 154, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22078663110733032, \"iteration\": 155, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.16196995973587036, \"iteration\": 156, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18458494544029236, \"iteration\": 157, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22794829308986664, \"iteration\": 158, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.1974419802427292, \"iteration\": 159, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2607246935367584, \"iteration\": 160, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2576260566711426, \"iteration\": 161, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20219795405864716, \"iteration\": 162, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2274750918149948, \"iteration\": 163, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20192646980285645, \"iteration\": 164, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.29505470395088196, \"iteration\": 165, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17159488797187805, \"iteration\": 166, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17471536993980408, \"iteration\": 167, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22881171107292175, \"iteration\": 168, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17736361920833588, \"iteration\": 169, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.17752134799957275, \"iteration\": 170, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2098952978849411, \"iteration\": 171, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.18611633777618408, \"iteration\": 172, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23419927060604095, \"iteration\": 173, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3383694589138031, \"iteration\": 174, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17741771042346954, \"iteration\": 175, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18898601830005646, \"iteration\": 176, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.19652026891708374, \"iteration\": 177, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17761853337287903, \"iteration\": 178, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.17276088893413544, \"iteration\": 179, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15942907333374023, \"iteration\": 180, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20497426390647888, \"iteration\": 181, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.17630262672901154, \"iteration\": 182, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2318369299173355, \"iteration\": 183, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.31306740641593933, \"iteration\": 184, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.25227636098861694, \"iteration\": 185, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.20750093460083008, \"iteration\": 186, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.26536092162132263, \"iteration\": 187, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2892133593559265, \"iteration\": 188, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.18181896209716797, \"iteration\": 189, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2482714056968689, \"iteration\": 190, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.24933172762393951, \"iteration\": 191, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3047198951244354, \"iteration\": 192, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2569238245487213, \"iteration\": 193, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.1751507818698883, \"iteration\": 194, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.24248160421848297, \"iteration\": 195, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2951774597167969, \"iteration\": 196, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22501163184642792, \"iteration\": 197, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2698494493961334, \"iteration\": 198, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.29582345485687256, \"iteration\": 199, \"epoch\": 3}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1705612689256668, \"iteration\": 200, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2149123251438141, \"iteration\": 201, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.3034680485725403, \"iteration\": 202, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.24139386415481567, \"iteration\": 203, \"epoch\": 3}, {\"training_acc\": 0.935064935064935, \"training_loss\": 0.28062742948532104, \"iteration\": 204, \"epoch\": 3}, {\"training_acc\": 0.984375, \"training_loss\": 0.0906674712896347, \"iteration\": 205, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09793777018785477, \"iteration\": 206, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09237334877252579, \"iteration\": 207, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.06523286551237106, \"iteration\": 208, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.07504350692033768, \"iteration\": 209, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08893978595733643, \"iteration\": 210, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.07372216880321503, \"iteration\": 211, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.058643639087677, \"iteration\": 212, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10570104420185089, \"iteration\": 213, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0879519060254097, \"iteration\": 214, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1219637393951416, \"iteration\": 215, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09154446423053741, \"iteration\": 216, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10587379336357117, \"iteration\": 217, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10187342762947083, \"iteration\": 218, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.09675251692533493, \"iteration\": 219, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.077545665204525, \"iteration\": 220, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0641375482082367, \"iteration\": 221, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11249038577079773, \"iteration\": 222, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09030026942491531, \"iteration\": 223, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08324114233255386, \"iteration\": 224, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0653940811753273, \"iteration\": 225, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12347256392240524, \"iteration\": 226, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06629598140716553, \"iteration\": 227, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05781807377934456, \"iteration\": 228, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05566227808594704, \"iteration\": 229, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0877244621515274, \"iteration\": 230, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09115777164697647, \"iteration\": 231, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08024577796459198, \"iteration\": 232, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11363314092159271, \"iteration\": 233, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08571547269821167, \"iteration\": 234, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1022072434425354, \"iteration\": 235, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06698305904865265, \"iteration\": 236, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11501992493867874, \"iteration\": 237, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.05452030897140503, \"iteration\": 238, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12374351173639297, \"iteration\": 239, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09255408495664597, \"iteration\": 240, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13042399287223816, \"iteration\": 241, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06814954429864883, \"iteration\": 242, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09974561631679535, \"iteration\": 243, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07057309150695801, \"iteration\": 244, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10064929723739624, \"iteration\": 245, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07764574140310287, \"iteration\": 246, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12746809422969818, \"iteration\": 247, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08713024854660034, \"iteration\": 248, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14778263866901398, \"iteration\": 249, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08398432284593582, \"iteration\": 250, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14093010127544403, \"iteration\": 251, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07887497544288635, \"iteration\": 252, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11185434460639954, \"iteration\": 253, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09176293015480042, \"iteration\": 254, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.07205073535442352, \"iteration\": 255, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.06744121015071869, \"iteration\": 256, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.06348441541194916, \"iteration\": 257, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10942509770393372, \"iteration\": 258, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08716534823179245, \"iteration\": 259, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1044272631406784, \"iteration\": 260, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.14590279757976532, \"iteration\": 261, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10832501947879791, \"iteration\": 262, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11250291019678116, \"iteration\": 263, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07920403778553009, \"iteration\": 264, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0973522812128067, \"iteration\": 265, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1514458805322647, \"iteration\": 266, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08102680742740631, \"iteration\": 267, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10354120284318924, \"iteration\": 268, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.1169876903295517, \"iteration\": 269, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08031854778528214, \"iteration\": 270, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1701490879058838, \"iteration\": 271, \"epoch\": 4}, {\"training_acc\": 0.987012987012987, \"training_loss\": 0.12314944714307785, \"iteration\": 272, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.0305088609457016, \"iteration\": 273, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03642946854233742, \"iteration\": 274, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.058613620698451996, \"iteration\": 275, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04644250497221947, \"iteration\": 276, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02045954018831253, \"iteration\": 277, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04108291491866112, \"iteration\": 278, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.049212630838155746, \"iteration\": 279, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.031188586726784706, \"iteration\": 280, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03787403926253319, \"iteration\": 281, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027181852608919144, \"iteration\": 282, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03287258371710777, \"iteration\": 283, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.044650208204984665, \"iteration\": 284, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03497830405831337, \"iteration\": 285, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.022141939029097557, \"iteration\": 286, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027370011433959007, \"iteration\": 287, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03477701544761658, \"iteration\": 288, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.039786193519830704, \"iteration\": 289, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.029881229624152184, \"iteration\": 290, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.025701425969600677, \"iteration\": 291, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02499905787408352, \"iteration\": 292, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0522414855659008, \"iteration\": 293, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.036459144204854965, \"iteration\": 294, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03134586662054062, \"iteration\": 295, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.051580075174570084, \"iteration\": 296, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023339781910181046, \"iteration\": 297, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.034568168222904205, \"iteration\": 298, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07436836510896683, \"iteration\": 299, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05025957152247429, \"iteration\": 300, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04134742170572281, \"iteration\": 301, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023017819970846176, \"iteration\": 302, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.016441991552710533, \"iteration\": 303, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02104109898209572, \"iteration\": 304, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.030721722170710564, \"iteration\": 305, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07284506410360336, \"iteration\": 306, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04423406347632408, \"iteration\": 307, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08630640059709549, \"iteration\": 308, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0536491796374321, \"iteration\": 309, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01783607341349125, \"iteration\": 310, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.043629374355077744, \"iteration\": 311, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.05562209337949753, \"iteration\": 312, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03505879268050194, \"iteration\": 313, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04437233880162239, \"iteration\": 314, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04374263435602188, \"iteration\": 315, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017109233886003494, \"iteration\": 316, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05452560633420944, \"iteration\": 317, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.035675760358572006, \"iteration\": 318, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023987678810954094, \"iteration\": 319, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04053637757897377, \"iteration\": 320, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.024717390537261963, \"iteration\": 321, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.064167320728302, \"iteration\": 322, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06163497641682625, \"iteration\": 323, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06510629504919052, \"iteration\": 324, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04797486960887909, \"iteration\": 325, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.049201883375644684, \"iteration\": 326, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04345031455159187, \"iteration\": 327, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027326200157403946, \"iteration\": 328, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02756119705736637, \"iteration\": 329, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03391255810856819, \"iteration\": 330, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.044329721480607986, \"iteration\": 331, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02157283015549183, \"iteration\": 332, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.032215457409620285, \"iteration\": 333, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018071746453642845, \"iteration\": 334, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04507255554199219, \"iteration\": 335, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.014076398685574532, \"iteration\": 336, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018778221681714058, \"iteration\": 337, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.015737053006887436, \"iteration\": 338, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03683925420045853, \"iteration\": 339, \"epoch\": 5}, {\"training_acc\": 0.974025974025974, \"training_loss\": 0.06924869865179062, \"iteration\": 340, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0104363476857543, \"iteration\": 341, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01828291453421116, \"iteration\": 342, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007874093018472195, \"iteration\": 343, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009910308755934238, \"iteration\": 344, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009371308609843254, \"iteration\": 345, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028288567438721657, \"iteration\": 346, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009068217128515244, \"iteration\": 347, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03226253390312195, \"iteration\": 348, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008352619595825672, \"iteration\": 349, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03629716858267784, \"iteration\": 350, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007994077168405056, \"iteration\": 351, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008439327590167522, \"iteration\": 352, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007870842702686787, \"iteration\": 353, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013529852032661438, \"iteration\": 354, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009421423077583313, \"iteration\": 355, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01077184546738863, \"iteration\": 356, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008582789450883865, \"iteration\": 357, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00766548840329051, \"iteration\": 358, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009040245786309242, \"iteration\": 359, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009034228511154652, \"iteration\": 360, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011856798082590103, \"iteration\": 361, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00798868853598833, \"iteration\": 362, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010322708636522293, \"iteration\": 363, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007253221236169338, \"iteration\": 364, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010550851002335548, \"iteration\": 365, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00955558754503727, \"iteration\": 366, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.014755272306501865, \"iteration\": 367, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006272461730986834, \"iteration\": 368, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005524364300072193, \"iteration\": 369, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008344699628651142, \"iteration\": 370, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.05136207491159439, \"iteration\": 371, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006363692693412304, \"iteration\": 372, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006472090259194374, \"iteration\": 373, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006122987251728773, \"iteration\": 374, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.016630150377750397, \"iteration\": 375, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006725517101585865, \"iteration\": 376, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005515224765986204, \"iteration\": 377, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006234618369489908, \"iteration\": 378, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004826450254768133, \"iteration\": 379, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021449493244290352, \"iteration\": 380, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006382101681083441, \"iteration\": 381, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008690373972058296, \"iteration\": 382, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00589546374976635, \"iteration\": 383, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00783128198236227, \"iteration\": 384, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008117099292576313, \"iteration\": 385, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006271373946219683, \"iteration\": 386, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04624728113412857, \"iteration\": 387, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01929737813770771, \"iteration\": 388, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.017813976854085922, \"iteration\": 389, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007294566836208105, \"iteration\": 390, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004929648712277412, \"iteration\": 391, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01049870066344738, \"iteration\": 392, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.029372472316026688, \"iteration\": 393, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.035125505179166794, \"iteration\": 394, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.015221855603158474, \"iteration\": 395, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.004559674765914679, \"iteration\": 396, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.038382235914468765, \"iteration\": 397, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.018662672489881516, \"iteration\": 398, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.05404511094093323, \"iteration\": 399, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007515160366892815, \"iteration\": 400, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01118469052016735, \"iteration\": 401, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007359121460467577, \"iteration\": 402, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.017423231154680252, \"iteration\": 403, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.018350601196289062, \"iteration\": 404, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005395228508859873, \"iteration\": 405, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012410338036715984, \"iteration\": 406, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005151368677616119, \"iteration\": 407, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0067838383838534355, \"iteration\": 408, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005816884338855743, \"iteration\": 409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00698408018797636, \"iteration\": 410, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004302759654819965, \"iteration\": 411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004216454923152924, \"iteration\": 412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003737472230568528, \"iteration\": 413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006406386848539114, \"iteration\": 414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004148862324655056, \"iteration\": 415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004357744008302689, \"iteration\": 416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003231312148272991, \"iteration\": 417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004113784059882164, \"iteration\": 418, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003930001053959131, \"iteration\": 419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0045592593960464, \"iteration\": 420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0028659384697675705, \"iteration\": 421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003891033586114645, \"iteration\": 422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0038911057636141777, \"iteration\": 423, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02409687638282776, \"iteration\": 424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0030890535563230515, \"iteration\": 425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003504944732412696, \"iteration\": 426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005247144959867001, \"iteration\": 427, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0036289524286985397, \"iteration\": 428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004023029934614897, \"iteration\": 429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005230392795056105, \"iteration\": 430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005150168668478727, \"iteration\": 431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006068918388336897, \"iteration\": 432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002985680243000388, \"iteration\": 433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0030727137345820665, \"iteration\": 434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0075898244976997375, \"iteration\": 435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0024927197955548763, \"iteration\": 436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005719587206840515, \"iteration\": 437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010692553594708443, \"iteration\": 438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003622058779001236, \"iteration\": 439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002169570652768016, \"iteration\": 440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004130803979933262, \"iteration\": 441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002828582189977169, \"iteration\": 442, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00331264385022223, \"iteration\": 443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0023329465184360743, \"iteration\": 444, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.023703349754214287, \"iteration\": 445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0025982954539358616, \"iteration\": 446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001922087394632399, \"iteration\": 447, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0037933490239083767, \"iteration\": 448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002260709647089243, \"iteration\": 449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0026412808801978827, \"iteration\": 450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0023612414952367544, \"iteration\": 451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00342670944519341, \"iteration\": 452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002723082434386015, \"iteration\": 453, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002926204353570938, \"iteration\": 454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002676833188161254, \"iteration\": 455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002034825971350074, \"iteration\": 456, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002462233416736126, \"iteration\": 457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.001767661771737039, \"iteration\": 458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002272314392030239, \"iteration\": 459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0026156827807426453, \"iteration\": 460, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.010423925705254078, \"iteration\": 461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004052603617310524, \"iteration\": 462, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.027928117662668228, \"iteration\": 463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002237548353150487, \"iteration\": 464, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0028472424019128084, \"iteration\": 465, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0028489429969340563, \"iteration\": 466, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002466668142005801, \"iteration\": 467, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0018939365400001407, \"iteration\": 468, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0020585786551237106, \"iteration\": 469, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.013934782706201077, \"iteration\": 470, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.025132838636636734, \"iteration\": 471, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002234433311969042, \"iteration\": 472, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0024097063578665257, \"iteration\": 473, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004344678483903408, \"iteration\": 474, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0024051459040492773, \"iteration\": 475, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0037363055162131786, \"iteration\": 476, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0021960141602903605, \"iteration\": 477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0024120407178997993, \"iteration\": 478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016071283025667071, \"iteration\": 479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0024603577330708504, \"iteration\": 480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001728598028421402, \"iteration\": 481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002735936315730214, \"iteration\": 482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016700157430022955, \"iteration\": 483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002018705941736698, \"iteration\": 484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0022302349098026752, \"iteration\": 485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001867959974333644, \"iteration\": 486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0018802689155563712, \"iteration\": 487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0019877818413078785, \"iteration\": 488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0021146819926798344, \"iteration\": 489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0009392246138304472, \"iteration\": 490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002426601480692625, \"iteration\": 491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0014922661939635873, \"iteration\": 492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016373207326978445, \"iteration\": 493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002330588409677148, \"iteration\": 494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017382253427058458, \"iteration\": 495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012902611633762717, \"iteration\": 496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005977057386189699, \"iteration\": 497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016787394415587187, \"iteration\": 498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015865117311477661, \"iteration\": 499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015898472629487514, \"iteration\": 500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002327292924746871, \"iteration\": 501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0018586987862363458, \"iteration\": 502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0019223522394895554, \"iteration\": 503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00254616467282176, \"iteration\": 504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001192889059893787, \"iteration\": 505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0019601278472691774, \"iteration\": 506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0020577411632984877, \"iteration\": 507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017552904319018126, \"iteration\": 508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0014795501483604312, \"iteration\": 509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002057810081169009, \"iteration\": 510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015574009157717228, \"iteration\": 511, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.00907000619918108, \"iteration\": 512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013626840664073825, \"iteration\": 513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016090745339170098, \"iteration\": 514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016042946372181177, \"iteration\": 515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003570509608834982, \"iteration\": 516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015119996387511492, \"iteration\": 517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015145981451496482, \"iteration\": 518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00171891157515347, \"iteration\": 519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015415315283462405, \"iteration\": 520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016416979487985373, \"iteration\": 521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013757145497947931, \"iteration\": 522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0014965464361011982, \"iteration\": 523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.000913021678570658, \"iteration\": 524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013780134031549096, \"iteration\": 525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001552494941279292, \"iteration\": 526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001295785536058247, \"iteration\": 527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016266657039523125, \"iteration\": 528, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01239544153213501, \"iteration\": 529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001265588216483593, \"iteration\": 530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012265630066394806, \"iteration\": 531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013927300460636616, \"iteration\": 532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017956100637093186, \"iteration\": 533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010808423394337296, \"iteration\": 534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013517957413569093, \"iteration\": 535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013973491732031107, \"iteration\": 536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013936611358076334, \"iteration\": 537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0018908162601292133, \"iteration\": 538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0020747296512126923, \"iteration\": 539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001372615690343082, \"iteration\": 540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017513221828266978, \"iteration\": 541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0014624695759266615, \"iteration\": 542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001650848425924778, \"iteration\": 543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012687835842370987, \"iteration\": 544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010002286871895194, \"iteration\": 545, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001276028691790998, \"iteration\": 546, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012030396610498428, \"iteration\": 547, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010779682779684663, \"iteration\": 548, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009486231137998402, \"iteration\": 549, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007923110388219357, \"iteration\": 550, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007098438800312579, \"iteration\": 551, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009371188352815807, \"iteration\": 552, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010782211320474744, \"iteration\": 553, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009376283269375563, \"iteration\": 554, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010896793100982904, \"iteration\": 555, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007821278413757682, \"iteration\": 556, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009874894749373198, \"iteration\": 557, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010358529398217797, \"iteration\": 558, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016042619245126843, \"iteration\": 559, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009274332551285625, \"iteration\": 560, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001089298166334629, \"iteration\": 561, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006950662937015295, \"iteration\": 562, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008098394609987736, \"iteration\": 563, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011385841062292457, \"iteration\": 564, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001250704750418663, \"iteration\": 565, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008110060007311404, \"iteration\": 566, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008116142707876861, \"iteration\": 567, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007249598274938762, \"iteration\": 568, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001030531246215105, \"iteration\": 569, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005895504145883024, \"iteration\": 570, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008276468724943697, \"iteration\": 571, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006597593310289085, \"iteration\": 572, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005841661477461457, \"iteration\": 573, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006650664145126939, \"iteration\": 574, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017415601760149002, \"iteration\": 575, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014694476267322898, \"iteration\": 576, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000948765897192061, \"iteration\": 577, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006572964484803379, \"iteration\": 578, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010211837943643332, \"iteration\": 579, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011539345141500235, \"iteration\": 580, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005687212687917054, \"iteration\": 581, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013155925553292036, \"iteration\": 582, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010658848332241178, \"iteration\": 583, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002622931497171521, \"iteration\": 584, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013372618705034256, \"iteration\": 585, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012714742915704846, \"iteration\": 586, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000844645663164556, \"iteration\": 587, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008086448069661856, \"iteration\": 588, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006729752058163285, \"iteration\": 589, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008611290832050145, \"iteration\": 590, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007237995741888881, \"iteration\": 591, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007220926345326006, \"iteration\": 592, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007743239984847605, \"iteration\": 593, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000945407198742032, \"iteration\": 594, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013277308316901326, \"iteration\": 595, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011354410089552402, \"iteration\": 596, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008950051851570606, \"iteration\": 597, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001046571065671742, \"iteration\": 598, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005995452520437539, \"iteration\": 599, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006263650138862431, \"iteration\": 600, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006315229111351073, \"iteration\": 601, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009985320502892137, \"iteration\": 602, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0030999311711639166, \"iteration\": 603, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0021264594979584217, \"iteration\": 604, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007836357108317316, \"iteration\": 605, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007349577499553561, \"iteration\": 606, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006132345297373831, \"iteration\": 607, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00048811492160893977, \"iteration\": 608, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008989314665086567, \"iteration\": 609, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010511329164728522, \"iteration\": 610, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000727918348275125, \"iteration\": 611, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000529334822203964, \"iteration\": 612, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00045359236537478864, \"iteration\": 613, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007490802672691643, \"iteration\": 614, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000633841787930578, \"iteration\": 615, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00048304678057320416, \"iteration\": 616, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005887994193471968, \"iteration\": 617, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006884176982566714, \"iteration\": 618, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005961308488622308, \"iteration\": 619, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000615251250565052, \"iteration\": 620, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000569160794839263, \"iteration\": 621, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004993676557205617, \"iteration\": 622, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006477111601270735, \"iteration\": 623, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005757732433266938, \"iteration\": 624, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013450951082631946, \"iteration\": 625, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000561281165573746, \"iteration\": 626, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005412590689957142, \"iteration\": 627, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007117416244000196, \"iteration\": 628, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006826930912211537, \"iteration\": 629, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005225593922659755, \"iteration\": 630, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005522820865735412, \"iteration\": 631, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004524462856352329, \"iteration\": 632, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00047640755656175315, \"iteration\": 633, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006033614045009017, \"iteration\": 634, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005954482476226985, \"iteration\": 635, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000631942821200937, \"iteration\": 636, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00047970691230148077, \"iteration\": 637, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006546403164975345, \"iteration\": 638, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005805697874166071, \"iteration\": 639, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005108686164021492, \"iteration\": 640, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006171898567117751, \"iteration\": 641, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009012830560095608, \"iteration\": 642, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008133994997479022, \"iteration\": 643, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00040759824332781136, \"iteration\": 644, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004287787014618516, \"iteration\": 645, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000690094952005893, \"iteration\": 646, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006142898346297443, \"iteration\": 647, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006108550587669015, \"iteration\": 648, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005441805114969611, \"iteration\": 649, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005936027155257761, \"iteration\": 650, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005360227078199387, \"iteration\": 651, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00047373471898026764, \"iteration\": 652, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004197733069304377, \"iteration\": 653, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005734795704483986, \"iteration\": 654, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00043628920684568584, \"iteration\": 655, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004849905672017485, \"iteration\": 656, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004754580440931022, \"iteration\": 657, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005034224595874548, \"iteration\": 658, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005549919442273676, \"iteration\": 659, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007499238126911223, \"iteration\": 660, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004572273464873433, \"iteration\": 661, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005789794377051294, \"iteration\": 662, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004285039904061705, \"iteration\": 663, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004944820539094508, \"iteration\": 664, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005924428114667535, \"iteration\": 665, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0013899528421461582, \"iteration\": 666, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004838237655349076, \"iteration\": 667, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005260987672954798, \"iteration\": 668, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004088931600563228, \"iteration\": 669, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00039669490070082247, \"iteration\": 670, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005218534497544169, \"iteration\": 671, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007762821041978896, \"iteration\": 672, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00048381276428699493, \"iteration\": 673, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0010050998535007238, \"iteration\": 674, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00034554259036667645, \"iteration\": 675, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004101979429833591, \"iteration\": 676, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005128281773068011, \"iteration\": 677, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00038069102447479963, \"iteration\": 678, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005031550535932183, \"iteration\": 679, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009656942565925419, \"iteration\": 680, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# POC\n",
    "\n",
    "print(\"Train model\")\n",
    "models_dir = Path('models/hr')\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    "    \n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_data_nn, batch_size=128, shuffle=True)\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "model_nn = NeuralNetwork(\n",
    "    input_size=len(tfidf_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn.pt').exists() and USE_CACHE:\n",
    "    model_nn = load_model(model_nn, models_dir, 'model_nn')\n",
    "else:\n",
    "    model_nn.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn, models_dir, \"model_nn\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # X_test = torch.stack([dta[0] for dta in test])\n",
    "    X_test = torch.stack([test[0] for test in test_data_nn]).to(model_nn.device)\n",
    "    y_test = torch.stack([test[1] for test in test_data_nn]).to(model_nn.device)\n",
    "    y_pred = model_nn.predict(X_test)\n",
    "\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, y_pred, average='binary'))\n",
    "print(\"AUC\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn, train_config, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 68/68 [00:00<00:00, 83.39batch/s, batch_accuracy=0.766, loss=0.574]\n",
      "Epoch 2: 100%|██████████| 68/68 [00:00<00:00, 87.64batch/s, batch_accuracy=0.831, loss=0.482]\n",
      "Epoch 3: 100%|██████████| 68/68 [00:00<00:00, 77.28batch/s, batch_accuracy=0.935, loss=0.228]\n",
      "Epoch 4: 100%|██████████| 68/68 [00:00<00:00, 80.56batch/s, batch_accuracy=0.987, loss=0.0843]\n",
      "Epoch 5: 100%|██████████| 68/68 [00:00<00:00, 75.62batch/s, batch_accuracy=1, loss=0.0257]    \n",
      "Epoch 6: 100%|██████████| 68/68 [00:00<00:00, 78.07batch/s, batch_accuracy=1, loss=0.0077]    \n",
      "Epoch 7: 100%|██████████| 68/68 [00:00<00:00, 83.77batch/s, batch_accuracy=1, loss=0.00416]   \n",
      "Epoch 8: 100%|██████████| 68/68 [00:00<00:00, 79.57batch/s, batch_accuracy=1, loss=0.00167]   \n",
      "Epoch 9: 100%|██████████| 68/68 [00:00<00:00, 71.88batch/s, batch_accuracy=1, loss=0.000639]\n",
      "Epoch 10: 100%|██████████| 68/68 [00:00<00:00, 84.18batch/s, batch_accuracy=1, loss=0.000394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5217948717948718, 0.5145385587863464, 0.5181413112667091, None)\n",
      "AUC 0.6134759100793722\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-f485db9b4e054d20bc373bdc67a81ae5.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-f485db9b4e054d20bc373bdc67a81ae5.vega-embed details,\n",
       "  #altair-viz-f485db9b4e054d20bc373bdc67a81ae5.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-f485db9b4e054d20bc373bdc67a81ae5\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f485db9b4e054d20bc373bdc67a81ae5\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f485db9b4e054d20bc373bdc67a81ae5\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-97544d5ced9842af00d3b52321dfc0e4\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-97544d5ced9842af00d3b52321dfc0e4\": [{\"training_acc\": 0.6171875, \"training_loss\": 0.6931891441345215, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6915789246559143, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6905691623687744, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.691412091255188, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6891361474990845, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6861398816108704, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6806784272193909, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6794524788856506, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.685685396194458, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6823476552963257, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6764760613441467, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6794474720954895, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6788986325263977, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6687976121902466, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6583260297775269, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6967374086380005, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6852625012397766, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6422669291496277, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6815632581710815, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6865280866622925, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6449828147888184, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6784700751304626, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6796629428863525, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6727913618087769, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6627987623214722, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.53125, \"training_loss\": 0.6999916434288025, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6890906095504761, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.662370502948761, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6919423341751099, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6713184118270874, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.706464409828186, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6445758938789368, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6836209297180176, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6474723219871521, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6775927543640137, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6762518286705017, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6634047031402588, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6481215953826904, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6748987436294556, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6672657132148743, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6403485536575317, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6616399884223938, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6478977203369141, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6508472561836243, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.651612401008606, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6710397005081177, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6372953653335571, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6511508226394653, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6323286890983582, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6677600741386414, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6583722829818726, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 0.6893523931503296, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6457710862159729, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.688318133354187, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6220074892044067, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6429029107093811, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6372185945510864, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6440662741661072, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6129190921783447, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6168860197067261, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6666561961174011, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6095995306968689, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6317005157470703, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6159396767616272, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6211541891098022, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6762627363204956, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6420943140983582, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.7662337662337663, \"training_loss\": 0.5739021301269531, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.5719197392463684, \"iteration\": 69, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 0.6087956428527832, \"iteration\": 70, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 0.6087493300437927, \"iteration\": 71, \"epoch\": 2}, {\"training_acc\": 0.6640625, \"training_loss\": 0.5792647004127502, \"iteration\": 72, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 0.5893872976303101, \"iteration\": 73, \"epoch\": 2}, {\"training_acc\": 0.6875, \"training_loss\": 0.5876344442367554, \"iteration\": 74, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 0.542478084564209, \"iteration\": 75, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5743928551673889, \"iteration\": 76, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 0.575829029083252, \"iteration\": 77, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5639521479606628, \"iteration\": 78, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5633922815322876, \"iteration\": 79, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5504037141799927, \"iteration\": 80, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 0.5589470267295837, \"iteration\": 81, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.5223933458328247, \"iteration\": 82, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.5244127511978149, \"iteration\": 83, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 0.5899192690849304, \"iteration\": 84, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5245245695114136, \"iteration\": 85, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.5457518696784973, \"iteration\": 86, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.5139234066009521, \"iteration\": 87, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.5186884999275208, \"iteration\": 88, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.5626783967018127, \"iteration\": 89, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.5438629388809204, \"iteration\": 90, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.5552784204483032, \"iteration\": 91, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.5483704805374146, \"iteration\": 92, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.48748779296875, \"iteration\": 93, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.49420681595802307, \"iteration\": 94, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.5076557397842407, \"iteration\": 95, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5094548463821411, \"iteration\": 96, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.473745197057724, \"iteration\": 97, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5264962315559387, \"iteration\": 98, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5401532053947449, \"iteration\": 99, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.583097517490387, \"iteration\": 100, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.4722711741924286, \"iteration\": 101, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.42935711145401, \"iteration\": 102, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5013891458511353, \"iteration\": 103, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.43712663650512695, \"iteration\": 104, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4459548592567444, \"iteration\": 105, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5095430016517639, \"iteration\": 106, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4319802224636078, \"iteration\": 107, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4666830003261566, \"iteration\": 108, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4684388339519501, \"iteration\": 109, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4322027564048767, \"iteration\": 110, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4359785318374634, \"iteration\": 111, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4552278518676758, \"iteration\": 112, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.44647666811943054, \"iteration\": 113, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.41060012578964233, \"iteration\": 114, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.47478899359703064, \"iteration\": 115, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.46628332138061523, \"iteration\": 116, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.42469334602355957, \"iteration\": 117, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.45412755012512207, \"iteration\": 118, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4787563979625702, \"iteration\": 119, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.43330368399620056, \"iteration\": 120, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.4027738571166992, \"iteration\": 121, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.45042797923088074, \"iteration\": 122, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4518294334411621, \"iteration\": 123, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.416704922914505, \"iteration\": 124, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.47960180044174194, \"iteration\": 125, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.35744336247444153, \"iteration\": 126, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.39908263087272644, \"iteration\": 127, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4268258810043335, \"iteration\": 128, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4242098331451416, \"iteration\": 129, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.43250787258148193, \"iteration\": 130, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.437738299369812, \"iteration\": 131, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.43510451912879944, \"iteration\": 132, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4096043109893799, \"iteration\": 133, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4108876585960388, \"iteration\": 134, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.4647941291332245, \"iteration\": 135, \"epoch\": 2}, {\"training_acc\": 0.8311688311688312, \"training_loss\": 0.4817962050437927, \"iteration\": 136, \"epoch\": 2}, {\"training_acc\": 0.890625, \"training_loss\": 0.31482836604118347, \"iteration\": 137, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.24892455339431763, \"iteration\": 138, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.29329174757003784, \"iteration\": 139, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.25328734517097473, \"iteration\": 140, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.26732397079467773, \"iteration\": 141, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.24869538843631744, \"iteration\": 142, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.26006263494491577, \"iteration\": 143, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2951969504356384, \"iteration\": 144, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.2203984260559082, \"iteration\": 145, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2314063310623169, \"iteration\": 146, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.279204398393631, \"iteration\": 147, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.28484517335891724, \"iteration\": 148, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2057151347398758, \"iteration\": 149, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25244516134262085, \"iteration\": 150, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.22651039063930511, \"iteration\": 151, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.276973158121109, \"iteration\": 152, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.20228876173496246, \"iteration\": 153, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2588084638118744, \"iteration\": 154, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2842830717563629, \"iteration\": 155, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2055237740278244, \"iteration\": 156, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2636365592479706, \"iteration\": 157, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19256377220153809, \"iteration\": 158, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.25406593084335327, \"iteration\": 159, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23875531554222107, \"iteration\": 160, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23800839483737946, \"iteration\": 161, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1951424926519394, \"iteration\": 162, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2162201702594757, \"iteration\": 163, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.20814910531044006, \"iteration\": 164, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20676551759243011, \"iteration\": 165, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.15938417613506317, \"iteration\": 166, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.25573694705963135, \"iteration\": 167, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2539421021938324, \"iteration\": 168, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.26326504349708557, \"iteration\": 169, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.27187854051589966, \"iteration\": 170, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2465851753950119, \"iteration\": 171, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20163777470588684, \"iteration\": 172, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1744169443845749, \"iteration\": 173, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.312948614358902, \"iteration\": 174, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.1800312101840973, \"iteration\": 175, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.29514023661613464, \"iteration\": 176, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2049025595188141, \"iteration\": 177, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30036163330078125, \"iteration\": 178, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20044738054275513, \"iteration\": 179, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2129599004983902, \"iteration\": 180, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2534915804862976, \"iteration\": 181, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.29482194781303406, \"iteration\": 182, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2611082196235657, \"iteration\": 183, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.20403029024600983, \"iteration\": 184, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.24273675680160522, \"iteration\": 185, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2205287516117096, \"iteration\": 186, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.32718825340270996, \"iteration\": 187, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22876420617103577, \"iteration\": 188, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.19611388444900513, \"iteration\": 189, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.27796635031700134, \"iteration\": 190, \"epoch\": 3}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19386428594589233, \"iteration\": 191, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.28247469663619995, \"iteration\": 192, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2421378642320633, \"iteration\": 193, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2828071713447571, \"iteration\": 194, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.24321229755878448, \"iteration\": 195, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.22544704377651215, \"iteration\": 196, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.2507447302341461, \"iteration\": 197, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.30178773403167725, \"iteration\": 198, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.3099871873855591, \"iteration\": 199, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20346052944660187, \"iteration\": 200, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3474045395851135, \"iteration\": 201, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.30399587750434875, \"iteration\": 202, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21122393012046814, \"iteration\": 203, \"epoch\": 3}, {\"training_acc\": 0.935064935064935, \"training_loss\": 0.22787269949913025, \"iteration\": 204, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.12996786832809448, \"iteration\": 205, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1225367933511734, \"iteration\": 206, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.13452006876468658, \"iteration\": 207, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08757929503917694, \"iteration\": 208, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10788601636886597, \"iteration\": 209, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10053327679634094, \"iteration\": 210, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15422168374061584, \"iteration\": 211, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.13080500066280365, \"iteration\": 212, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08686963468790054, \"iteration\": 213, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1479676365852356, \"iteration\": 214, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.062289461493492126, \"iteration\": 215, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09887358546257019, \"iteration\": 216, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13631851971149445, \"iteration\": 217, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13204558193683624, \"iteration\": 218, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.13454791903495789, \"iteration\": 219, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11842817068099976, \"iteration\": 220, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08121119439601898, \"iteration\": 221, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09640772640705109, \"iteration\": 222, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11713568866252899, \"iteration\": 223, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07025742530822754, \"iteration\": 224, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11754538118839264, \"iteration\": 225, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09393878281116486, \"iteration\": 226, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11854468286037445, \"iteration\": 227, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09189219027757645, \"iteration\": 228, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10817432403564453, \"iteration\": 229, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10572270303964615, \"iteration\": 230, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0824984759092331, \"iteration\": 231, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.1081814169883728, \"iteration\": 232, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0982779711484909, \"iteration\": 233, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.05965690314769745, \"iteration\": 234, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12437398731708527, \"iteration\": 235, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11963227391242981, \"iteration\": 236, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09669189900159836, \"iteration\": 237, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0718945786356926, \"iteration\": 238, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0664048120379448, \"iteration\": 239, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.14525632560253143, \"iteration\": 240, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08501497656106949, \"iteration\": 241, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08952774107456207, \"iteration\": 242, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.09851604700088501, \"iteration\": 243, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14573533833026886, \"iteration\": 244, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12079647183418274, \"iteration\": 245, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09049376845359802, \"iteration\": 246, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.06614729017019272, \"iteration\": 247, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08002252876758575, \"iteration\": 248, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12448611855506897, \"iteration\": 249, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.051721345633268356, \"iteration\": 250, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09752413630485535, \"iteration\": 251, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09792851656675339, \"iteration\": 252, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06220627948641777, \"iteration\": 253, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11805889755487442, \"iteration\": 254, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.19310782849788666, \"iteration\": 255, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09710749983787537, \"iteration\": 256, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12004193663597107, \"iteration\": 257, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.1347379982471466, \"iteration\": 258, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06048164516687393, \"iteration\": 259, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1028883084654808, \"iteration\": 260, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11727540194988251, \"iteration\": 261, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11450879275798798, \"iteration\": 262, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11648023873567581, \"iteration\": 263, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10408628731966019, \"iteration\": 264, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.1085541695356369, \"iteration\": 265, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10591040551662445, \"iteration\": 266, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1496478021144867, \"iteration\": 267, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12087089568376541, \"iteration\": 268, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08156567811965942, \"iteration\": 269, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10769403725862503, \"iteration\": 270, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.15731869637966156, \"iteration\": 271, \"epoch\": 4}, {\"training_acc\": 0.987012987012987, \"training_loss\": 0.08430399000644684, \"iteration\": 272, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.052500613033771515, \"iteration\": 273, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.062253687530756, \"iteration\": 274, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06436993181705475, \"iteration\": 275, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05899481102824211, \"iteration\": 276, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03650563582777977, \"iteration\": 277, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.028817005455493927, \"iteration\": 278, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08064920455217361, \"iteration\": 279, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04927952587604523, \"iteration\": 280, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03217421472072601, \"iteration\": 281, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.037048839032649994, \"iteration\": 282, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02543412707746029, \"iteration\": 283, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.021599840372800827, \"iteration\": 284, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.045380849391222, \"iteration\": 285, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.025569694116711617, \"iteration\": 286, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027409033849835396, \"iteration\": 287, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04219895973801613, \"iteration\": 288, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03540932387113571, \"iteration\": 289, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05031127855181694, \"iteration\": 290, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02549917623400688, \"iteration\": 291, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.021472325548529625, \"iteration\": 292, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02815401554107666, \"iteration\": 293, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027131153270602226, \"iteration\": 294, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0327465645968914, \"iteration\": 295, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06844862550497055, \"iteration\": 296, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02792389690876007, \"iteration\": 297, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02798575907945633, \"iteration\": 298, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028518518432974815, \"iteration\": 299, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05793719366192818, \"iteration\": 300, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03237481415271759, \"iteration\": 301, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.025951499119400978, \"iteration\": 302, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.031143583357334137, \"iteration\": 303, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03444940596818924, \"iteration\": 304, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.029464151710271835, \"iteration\": 305, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02411133050918579, \"iteration\": 306, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.028317011892795563, \"iteration\": 307, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.025586221367120743, \"iteration\": 308, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06172903627157211, \"iteration\": 309, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06189371645450592, \"iteration\": 310, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08726891875267029, \"iteration\": 311, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.025416569784283638, \"iteration\": 312, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.051506053656339645, \"iteration\": 313, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04644308239221573, \"iteration\": 314, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.020372118800878525, \"iteration\": 315, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04260747879743576, \"iteration\": 316, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.019308310002088547, \"iteration\": 317, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.058831699192523956, \"iteration\": 318, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.018373671919107437, \"iteration\": 319, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.061042170971632004, \"iteration\": 320, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017508698627352715, \"iteration\": 321, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03584577888250351, \"iteration\": 322, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.024730736389756203, \"iteration\": 323, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.024530529975891113, \"iteration\": 324, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08985922485589981, \"iteration\": 325, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04646794870495796, \"iteration\": 326, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09893928468227386, \"iteration\": 327, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.025309622287750244, \"iteration\": 328, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.035666536539793015, \"iteration\": 329, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0388900451362133, \"iteration\": 330, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.028942488133907318, \"iteration\": 331, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.068262979388237, \"iteration\": 332, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.021664435043931007, \"iteration\": 333, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08990205079317093, \"iteration\": 334, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05090378224849701, \"iteration\": 335, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0179275032132864, \"iteration\": 336, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.038520850241184235, \"iteration\": 337, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.015005375258624554, \"iteration\": 338, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028373466804623604, \"iteration\": 339, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.025717630982398987, \"iteration\": 340, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.045185428112745285, \"iteration\": 341, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009719557128846645, \"iteration\": 342, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.042284756898880005, \"iteration\": 343, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011937555857002735, \"iteration\": 344, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010443991981446743, \"iteration\": 345, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028030764311552048, \"iteration\": 346, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008279948495328426, \"iteration\": 347, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011440185829997063, \"iteration\": 348, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013114920817315578, \"iteration\": 349, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.029746970161795616, \"iteration\": 350, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.031064653769135475, \"iteration\": 351, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.06548358500003815, \"iteration\": 352, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008852913975715637, \"iteration\": 353, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02327251434326172, \"iteration\": 354, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010844607837498188, \"iteration\": 355, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007630172651261091, \"iteration\": 356, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012775292620062828, \"iteration\": 357, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01090084295719862, \"iteration\": 358, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03740696981549263, \"iteration\": 359, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.030541352927684784, \"iteration\": 360, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007280332967638969, \"iteration\": 361, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010327287949621677, \"iteration\": 362, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014303170144557953, \"iteration\": 363, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010559110902249813, \"iteration\": 364, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010782117024064064, \"iteration\": 365, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009506418369710445, \"iteration\": 366, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010285401716828346, \"iteration\": 367, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010234053246676922, \"iteration\": 368, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02848273143172264, \"iteration\": 369, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00886869803071022, \"iteration\": 370, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009950829669833183, \"iteration\": 371, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0279424749314785, \"iteration\": 372, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01009826548397541, \"iteration\": 373, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012386836111545563, \"iteration\": 374, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009088297374546528, \"iteration\": 375, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007039434742182493, \"iteration\": 376, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007931970991194248, \"iteration\": 377, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007817678153514862, \"iteration\": 378, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0319606214761734, \"iteration\": 379, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01102641224861145, \"iteration\": 380, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007421919144690037, \"iteration\": 381, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009522912092506886, \"iteration\": 382, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.036689892411231995, \"iteration\": 383, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02892826683819294, \"iteration\": 384, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0056968312710523605, \"iteration\": 385, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006903605069965124, \"iteration\": 386, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00770089915022254, \"iteration\": 387, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008566402830183506, \"iteration\": 388, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009562189690768719, \"iteration\": 389, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0159964170306921, \"iteration\": 390, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008698909543454647, \"iteration\": 391, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.024607181549072266, \"iteration\": 392, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02578076720237732, \"iteration\": 393, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.019927753135561943, \"iteration\": 394, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.005281914956867695, \"iteration\": 395, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006802745163440704, \"iteration\": 396, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009099971503019333, \"iteration\": 397, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009095655754208565, \"iteration\": 398, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03461252897977829, \"iteration\": 399, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006532744504511356, \"iteration\": 400, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007059452123939991, \"iteration\": 401, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00724271684885025, \"iteration\": 402, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021375438198447227, \"iteration\": 403, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007043891586363316, \"iteration\": 404, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.054775089025497437, \"iteration\": 405, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0059470306150615215, \"iteration\": 406, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00890954490751028, \"iteration\": 407, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007704738527536392, \"iteration\": 408, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00388267170637846, \"iteration\": 409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009316207841038704, \"iteration\": 410, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011568192392587662, \"iteration\": 411, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03195721283555031, \"iteration\": 412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004264170303940773, \"iteration\": 413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003924000076949596, \"iteration\": 414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004025585018098354, \"iteration\": 415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008066141977906227, \"iteration\": 416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003706947434693575, \"iteration\": 417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0042404369451105595, \"iteration\": 418, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00414508581161499, \"iteration\": 419, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005092747043818235, \"iteration\": 420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005305658560246229, \"iteration\": 421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007348807528614998, \"iteration\": 422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005212814547121525, \"iteration\": 423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003886319464072585, \"iteration\": 424, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02708481438457966, \"iteration\": 425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00423484668135643, \"iteration\": 426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0044017089530825615, \"iteration\": 427, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01219975110143423, \"iteration\": 428, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.024461548775434494, \"iteration\": 429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005099105183035135, \"iteration\": 430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002829469507560134, \"iteration\": 431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0037248393055051565, \"iteration\": 432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004207969177514315, \"iteration\": 433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006179388612508774, \"iteration\": 434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004449549131095409, \"iteration\": 435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004631116054952145, \"iteration\": 436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004853080492466688, \"iteration\": 437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005375562701374292, \"iteration\": 438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003376963082700968, \"iteration\": 439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004487038590013981, \"iteration\": 440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0037480380851775408, \"iteration\": 441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004184907302260399, \"iteration\": 442, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004068514332175255, \"iteration\": 443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0059256055392324924, \"iteration\": 444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003779282094910741, \"iteration\": 445, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.010163255035877228, \"iteration\": 446, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01838763989508152, \"iteration\": 447, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0030148839578032494, \"iteration\": 448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003432396799325943, \"iteration\": 449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0034944459330290556, \"iteration\": 450, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.010310297831892967, \"iteration\": 451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004090094473212957, \"iteration\": 452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004136135336011648, \"iteration\": 453, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004222847055643797, \"iteration\": 454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0033582192845642567, \"iteration\": 455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003158392384648323, \"iteration\": 456, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01412152498960495, \"iteration\": 457, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.009095130488276482, \"iteration\": 458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0034839462023228407, \"iteration\": 459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005155118647962809, \"iteration\": 460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005016156937927008, \"iteration\": 461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0028357384726405144, \"iteration\": 462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002803960582241416, \"iteration\": 463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00258224387653172, \"iteration\": 464, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007970339618623257, \"iteration\": 465, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004540830384939909, \"iteration\": 466, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003382948227226734, \"iteration\": 467, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0033498231787234545, \"iteration\": 468, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0028792000375688076, \"iteration\": 469, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.027659425511956215, \"iteration\": 470, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00339594972319901, \"iteration\": 471, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.002549058059230447, \"iteration\": 472, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004180964548140764, \"iteration\": 473, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.03920350223779678, \"iteration\": 474, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0032892203889787197, \"iteration\": 475, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004158936440944672, \"iteration\": 476, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003047461621463299, \"iteration\": 477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002409059088677168, \"iteration\": 478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0028301032725721598, \"iteration\": 479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002065370324999094, \"iteration\": 480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003697414183989167, \"iteration\": 481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002413260517641902, \"iteration\": 482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001883197808638215, \"iteration\": 483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001992463134229183, \"iteration\": 484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0022490760311484337, \"iteration\": 485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0022503018844872713, \"iteration\": 486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002131573623046279, \"iteration\": 487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0030776674393564463, \"iteration\": 488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002271409844979644, \"iteration\": 489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0025150426663458347, \"iteration\": 490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004440526012331247, \"iteration\": 491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0011830077273771167, \"iteration\": 492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0025623005349189043, \"iteration\": 493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001819281023927033, \"iteration\": 494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.006307330448180437, \"iteration\": 495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001287521212361753, \"iteration\": 496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0018916141707450151, \"iteration\": 497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0019812616519629955, \"iteration\": 498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016426376532763243, \"iteration\": 499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0018063233001157641, \"iteration\": 500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002205298049375415, \"iteration\": 501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001437001978047192, \"iteration\": 502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015361064579337835, \"iteration\": 503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0020254640839993954, \"iteration\": 504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001521933008916676, \"iteration\": 505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017707799561321735, \"iteration\": 506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0019168140133842826, \"iteration\": 507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0018652001162990928, \"iteration\": 508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017113659996539354, \"iteration\": 509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015162397176027298, \"iteration\": 510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0017892029136419296, \"iteration\": 511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001172796473838389, \"iteration\": 512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015707886777818203, \"iteration\": 513, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.015524178743362427, \"iteration\": 514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002217399887740612, \"iteration\": 515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00190259818919003, \"iteration\": 516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005312364548444748, \"iteration\": 517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00208543729968369, \"iteration\": 518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004167544189840555, \"iteration\": 519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013884366489946842, \"iteration\": 520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001514885458163917, \"iteration\": 521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00265794456936419, \"iteration\": 522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002258794382214546, \"iteration\": 523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0012588523095473647, \"iteration\": 524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003154809819534421, \"iteration\": 525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001389500335790217, \"iteration\": 526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016546653350815177, \"iteration\": 527, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.014716163277626038, \"iteration\": 528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001555565046146512, \"iteration\": 529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0014875434571877122, \"iteration\": 530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0019979397766292095, \"iteration\": 531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0016007251106202602, \"iteration\": 532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013043732615187764, \"iteration\": 533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015878976555541158, \"iteration\": 534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015355302020907402, \"iteration\": 535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002157792914658785, \"iteration\": 536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001842069672420621, \"iteration\": 537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00403534434735775, \"iteration\": 538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0018825301667675376, \"iteration\": 539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001889080391265452, \"iteration\": 540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0025083499494940042, \"iteration\": 541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0010080698411911726, \"iteration\": 542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0015370158944278955, \"iteration\": 543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.001666406518779695, \"iteration\": 544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0013991807354614139, \"iteration\": 545, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016777410637587309, \"iteration\": 546, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010688431793823838, \"iteration\": 547, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001040964387357235, \"iteration\": 548, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012928894720971584, \"iteration\": 549, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011812187731266022, \"iteration\": 550, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012552894186228514, \"iteration\": 551, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008526772144250572, \"iteration\": 552, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008873000624589622, \"iteration\": 553, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002186232479289174, \"iteration\": 554, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012510215165093541, \"iteration\": 555, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00094820890808478, \"iteration\": 556, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009203071240335703, \"iteration\": 557, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013109637657180429, \"iteration\": 558, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014057564549148083, \"iteration\": 559, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007286853506229818, \"iteration\": 560, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010367498034611344, \"iteration\": 561, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009569573448970914, \"iteration\": 562, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009556400473229587, \"iteration\": 563, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014884922420606017, \"iteration\": 564, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006832826184108853, \"iteration\": 565, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010223439894616604, \"iteration\": 566, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005544269224628806, \"iteration\": 567, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004753172863274813, \"iteration\": 568, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014084092108532786, \"iteration\": 569, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012743743136525154, \"iteration\": 570, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008311833371408284, \"iteration\": 571, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006545830983668566, \"iteration\": 572, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011516015511006117, \"iteration\": 573, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0012983689084649086, \"iteration\": 574, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006704422412440181, \"iteration\": 575, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009023770689964294, \"iteration\": 576, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018499060533940792, \"iteration\": 577, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007935410249046981, \"iteration\": 578, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017095747170969844, \"iteration\": 579, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009027971536852419, \"iteration\": 580, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007400896865874529, \"iteration\": 581, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000878595223184675, \"iteration\": 582, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007151784957386553, \"iteration\": 583, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006888404604978859, \"iteration\": 584, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0010128329740837216, \"iteration\": 585, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007834199932403862, \"iteration\": 586, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000676228606607765, \"iteration\": 587, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006390039343386889, \"iteration\": 588, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009480693843215704, \"iteration\": 589, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006424707244150341, \"iteration\": 590, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006151812267489731, \"iteration\": 591, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006924695335328579, \"iteration\": 592, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0008500619442202151, \"iteration\": 593, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007879363838583231, \"iteration\": 594, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007527320994995534, \"iteration\": 595, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000672960770316422, \"iteration\": 596, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007387553341686726, \"iteration\": 597, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0005712850834243, \"iteration\": 598, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007493890589103103, \"iteration\": 599, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0016912836581468582, \"iteration\": 600, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001507901819422841, \"iteration\": 601, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006283915136009455, \"iteration\": 602, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0011354301823303103, \"iteration\": 603, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0007373910630121827, \"iteration\": 604, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006754036294296384, \"iteration\": 605, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000729922903701663, \"iteration\": 606, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0009503788314759731, \"iteration\": 607, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006541306502185762, \"iteration\": 608, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.000523183261975646, \"iteration\": 609, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0013135034823790193, \"iteration\": 610, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006657614721916616, \"iteration\": 611, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006393266376107931, \"iteration\": 612, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0006865327595733106, \"iteration\": 613, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007329559302888811, \"iteration\": 614, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0009326677536591887, \"iteration\": 615, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005842393729835749, \"iteration\": 616, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004470594576559961, \"iteration\": 617, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005756310420110822, \"iteration\": 618, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005873909103684127, \"iteration\": 619, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00047124657430686057, \"iteration\": 620, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003797120589297265, \"iteration\": 621, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004450644482858479, \"iteration\": 622, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014910469762980938, \"iteration\": 623, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006723534315824509, \"iteration\": 624, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007031197310425341, \"iteration\": 625, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004047542461194098, \"iteration\": 626, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008145070169121027, \"iteration\": 627, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007080361247062683, \"iteration\": 628, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00036835967330262065, \"iteration\": 629, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006088634836487472, \"iteration\": 630, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0022269687615334988, \"iteration\": 631, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005085711600258946, \"iteration\": 632, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00042392261093482375, \"iteration\": 633, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006567658856511116, \"iteration\": 634, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000449580344138667, \"iteration\": 635, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007321885786950588, \"iteration\": 636, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0007810998358763754, \"iteration\": 637, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000346918823197484, \"iteration\": 638, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0008927189046517015, \"iteration\": 639, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004212589410599321, \"iteration\": 640, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004892342840321362, \"iteration\": 641, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014842168893665075, \"iteration\": 642, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005130836507305503, \"iteration\": 643, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00036679094773717225, \"iteration\": 644, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00040270015597343445, \"iteration\": 645, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003966823860537261, \"iteration\": 646, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005134233506396413, \"iteration\": 647, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00035729294177144766, \"iteration\": 648, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005449867458082736, \"iteration\": 649, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005015160422772169, \"iteration\": 650, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00034387491177767515, \"iteration\": 651, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004908418050035834, \"iteration\": 652, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006829044432379305, \"iteration\": 653, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005233598058111966, \"iteration\": 654, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005299347103573382, \"iteration\": 655, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00035095150815322995, \"iteration\": 656, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003031257656402886, \"iteration\": 657, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006953804986551404, \"iteration\": 658, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004550018347799778, \"iteration\": 659, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005098303081467748, \"iteration\": 660, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000391351873986423, \"iteration\": 661, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004702783771790564, \"iteration\": 662, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00036311132134869695, \"iteration\": 663, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.000478145171655342, \"iteration\": 664, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004189108731225133, \"iteration\": 665, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005718620959669352, \"iteration\": 666, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0005464605637826025, \"iteration\": 667, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0006655678735114634, \"iteration\": 668, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003483979671727866, \"iteration\": 669, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00043005734914913774, \"iteration\": 670, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004574244376271963, \"iteration\": 671, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00044509657891467214, \"iteration\": 672, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003607374965213239, \"iteration\": 673, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00033662241185083985, \"iteration\": 674, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00041223171865567565, \"iteration\": 675, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002879065868910402, \"iteration\": 676, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0002450897009111941, \"iteration\": 677, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0004712724476121366, \"iteration\": 678, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00032233528327196836, \"iteration\": 679, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0003940432798117399, \"iteration\": 680, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters finding\n",
    "\n",
    "print(\"Train model\")\n",
    "models_dir = Path('models/hr')\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    "    optimizer_params= {\"lr\": 0.0001, \"weight_decay\": 0.001, }\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_data_nn, batch_size=128, shuffle=True)\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "model_nn = NeuralNetwork(\n",
    "    input_size=len(tfidf_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn.pt').exists() and USE_CACHE:\n",
    "    model_nn = load_model(model_nn, models_dir, 'model_nn')\n",
    "else:\n",
    "    model_nn.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn, models_dir, \"model_nn\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # X_test = torch.stack([dta[0] for dta in test])\n",
    "    X_test = torch.stack([test[0] for test in test_data_nn]).to(model_nn.device)\n",
    "    y_test = torch.stack([test[1] for test in test_data_nn]).to(model_nn.device)\n",
    "    y_pred = model_nn.predict(X_test)\n",
    "\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, y_pred, average='binary'))\n",
    "print(\"AUC\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn, train_config, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 68/68 [00:00<00:00, 78.29batch/s, batch_accuracy=0.494, loss=0.686]\n",
      "Epoch 2: 100%|██████████| 68/68 [00:00<00:00, 77.96batch/s, batch_accuracy=0.844, loss=0.453]\n",
      "Epoch 3: 100%|██████████| 68/68 [00:00<00:00, 72.24batch/s, batch_accuracy=0.922, loss=0.238]\n",
      "Epoch 4: 100%|██████████| 68/68 [00:00<00:00, 77.01batch/s, batch_accuracy=0.974, loss=0.0925]\n",
      "Epoch 5: 100%|██████████| 68/68 [00:00<00:00, 80.66batch/s, batch_accuracy=1, loss=0.0164]    \n",
      "Epoch 6: 100%|██████████| 68/68 [00:00<00:00, 73.72batch/s, batch_accuracy=1, loss=0.00961]   \n",
      "Epoch 7: 100%|██████████| 68/68 [00:00<00:00, 78.78batch/s, batch_accuracy=1, loss=0.00853]   \n",
      "Epoch 8: 100%|██████████| 68/68 [00:00<00:00, 79.81batch/s, batch_accuracy=1, loss=0.004]     \n",
      "Epoch 9: 100%|██████████| 68/68 [00:00<00:00, 74.57batch/s, batch_accuracy=1, loss=0.00379]  \n",
      "Epoch 10: 100%|██████████| 68/68 [00:00<00:00, 70.09batch/s, batch_accuracy=1, loss=0.0015]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5389876880984952, 0.49810366624525915, 0.5177398160315374, None)\n",
      "AUC 0.6191366442251739\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-d466ca3766874a6ca3374d6976227069.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-d466ca3766874a6ca3374d6976227069.vega-embed details,\n",
       "  #altair-viz-d466ca3766874a6ca3374d6976227069.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-d466ca3766874a6ca3374d6976227069\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-d466ca3766874a6ca3374d6976227069\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-d466ca3766874a6ca3374d6976227069\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-526d5a9554048efd81f4cf30408186e4\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-526d5a9554048efd81f4cf30408186e4\": [{\"training_acc\": 0.390625, \"training_loss\": 0.6971490979194641, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.4765625, \"training_loss\": 0.6973259449005127, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.693882942199707, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6922348737716675, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6878284811973572, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.515625, \"training_loss\": 0.6923301815986633, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6913532018661499, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6811867952346802, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6833988428115845, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6825551390647888, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.6621241569519043, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6694064736366272, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.6585888862609863, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6807509660720825, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6909161806106567, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6646491289138794, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6813082695007324, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.670465886592865, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6633257269859314, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6579303741455078, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6803487539291382, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6618096232414246, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6573833227157593, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6612449288368225, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6967509984970093, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6738705635070801, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6910059452056885, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6397380828857422, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6426881551742554, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6390751600265503, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6800830364227295, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6810295581817627, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6885582804679871, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6111913323402405, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6869101524353027, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6617835760116577, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6796892881393433, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6518917679786682, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 0.6967649459838867, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6522393226623535, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6742813587188721, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6526002287864685, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6630828976631165, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6806241869926453, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6779847145080566, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.660822868347168, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6461055874824524, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6540309190750122, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6366636157035828, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6556675434112549, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.5078125, \"training_loss\": 0.6957945823669434, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6552351713180542, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 0.6740429401397705, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.6484375, \"training_loss\": 0.6382004618644714, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.645257830619812, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6652693152427673, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6560956239700317, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6592363119125366, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6455392837524414, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.615881621837616, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6517760753631592, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.5234375, \"training_loss\": 0.6842698454856873, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6636978387832642, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6584008932113647, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6421734094619751, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.649956226348877, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6581937074661255, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.4935064935064935, \"training_loss\": 0.6856691837310791, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6135883927345276, \"iteration\": 69, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 0.6200869083404541, \"iteration\": 70, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 0.6288400292396545, \"iteration\": 71, \"epoch\": 2}, {\"training_acc\": 0.546875, \"training_loss\": 0.636343777179718, \"iteration\": 72, \"epoch\": 2}, {\"training_acc\": 0.5546875, \"training_loss\": 0.633253812789917, \"iteration\": 73, \"epoch\": 2}, {\"training_acc\": 0.65625, \"training_loss\": 0.5941327214241028, \"iteration\": 74, \"epoch\": 2}, {\"training_acc\": 0.640625, \"training_loss\": 0.5980802774429321, \"iteration\": 75, \"epoch\": 2}, {\"training_acc\": 0.546875, \"training_loss\": 0.6070758104324341, \"iteration\": 76, \"epoch\": 2}, {\"training_acc\": 0.6796875, \"training_loss\": 0.5581856966018677, \"iteration\": 77, \"epoch\": 2}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6204559803009033, \"iteration\": 78, \"epoch\": 2}, {\"training_acc\": 0.578125, \"training_loss\": 0.6132599711418152, \"iteration\": 79, \"epoch\": 2}, {\"training_acc\": 0.6015625, \"training_loss\": 0.5826340317726135, \"iteration\": 80, \"epoch\": 2}, {\"training_acc\": 0.5625, \"training_loss\": 0.5865795016288757, \"iteration\": 81, \"epoch\": 2}, {\"training_acc\": 0.625, \"training_loss\": 0.6035890579223633, \"iteration\": 82, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 0.537472128868103, \"iteration\": 83, \"epoch\": 2}, {\"training_acc\": 0.6328125, \"training_loss\": 0.5852174758911133, \"iteration\": 84, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 0.562098503112793, \"iteration\": 85, \"epoch\": 2}, {\"training_acc\": 0.703125, \"training_loss\": 0.5743587613105774, \"iteration\": 86, \"epoch\": 2}, {\"training_acc\": 0.71875, \"training_loss\": 0.5388140678405762, \"iteration\": 87, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 0.552404522895813, \"iteration\": 88, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5420252084732056, \"iteration\": 89, \"epoch\": 2}, {\"training_acc\": 0.734375, \"training_loss\": 0.5685324668884277, \"iteration\": 90, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.5187212824821472, \"iteration\": 91, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5469085574150085, \"iteration\": 92, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 0.5310555100440979, \"iteration\": 93, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5394227504730225, \"iteration\": 94, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5674806237220764, \"iteration\": 95, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.5220522880554199, \"iteration\": 96, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5634583234786987, \"iteration\": 97, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.4941602945327759, \"iteration\": 98, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.5230262279510498, \"iteration\": 99, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5210869312286377, \"iteration\": 100, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5184926986694336, \"iteration\": 101, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.5156327486038208, \"iteration\": 102, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.5228849649429321, \"iteration\": 103, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.5083842277526855, \"iteration\": 104, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4566210210323334, \"iteration\": 105, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.5346198081970215, \"iteration\": 106, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.48087233304977417, \"iteration\": 107, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.523460328578949, \"iteration\": 108, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.48092082142829895, \"iteration\": 109, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.501886248588562, \"iteration\": 110, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5239677429199219, \"iteration\": 111, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4563547372817993, \"iteration\": 112, \"epoch\": 2}, {\"training_acc\": 0.84375, \"training_loss\": 0.4650071859359741, \"iteration\": 113, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.44449296593666077, \"iteration\": 114, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.42127615213394165, \"iteration\": 115, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.40923190116882324, \"iteration\": 116, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 0.4971555769443512, \"iteration\": 117, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5260319709777832, \"iteration\": 118, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4828305244445801, \"iteration\": 119, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.46199941635131836, \"iteration\": 120, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4663432240486145, \"iteration\": 121, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4316479563713074, \"iteration\": 122, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.47047775983810425, \"iteration\": 123, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.4621463716030121, \"iteration\": 124, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.44533997774124146, \"iteration\": 125, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.40692338347435, \"iteration\": 126, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.44465798139572144, \"iteration\": 127, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5327622890472412, \"iteration\": 128, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4410383403301239, \"iteration\": 129, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.47346216440200806, \"iteration\": 130, \"epoch\": 2}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3418024778366089, \"iteration\": 131, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.45096355676651, \"iteration\": 132, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4299316108226776, \"iteration\": 133, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.35622674226760864, \"iteration\": 134, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.4029496908187866, \"iteration\": 135, \"epoch\": 2}, {\"training_acc\": 0.8441558441558441, \"training_loss\": 0.4532264173030853, \"iteration\": 136, \"epoch\": 2}, {\"training_acc\": 0.90625, \"training_loss\": 0.43539363145828247, \"iteration\": 137, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.31963568925857544, \"iteration\": 138, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.3185812830924988, \"iteration\": 139, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.286106139421463, \"iteration\": 140, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.25811347365379333, \"iteration\": 141, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.287598192691803, \"iteration\": 142, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.26010066270828247, \"iteration\": 143, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.25100189447402954, \"iteration\": 144, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.25116297602653503, \"iteration\": 145, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2659062147140503, \"iteration\": 146, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.25486981868743896, \"iteration\": 147, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.24240371584892273, \"iteration\": 148, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.23882059752941132, \"iteration\": 149, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.23334011435508728, \"iteration\": 150, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2558191418647766, \"iteration\": 151, \"epoch\": 3}, {\"training_acc\": 0.96875, \"training_loss\": 0.18454892933368683, \"iteration\": 152, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.26530921459198, \"iteration\": 153, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.251162052154541, \"iteration\": 154, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22134152054786682, \"iteration\": 155, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.2521425485610962, \"iteration\": 156, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.23800984025001526, \"iteration\": 157, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2706511616706848, \"iteration\": 158, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.32486552000045776, \"iteration\": 159, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2559840679168701, \"iteration\": 160, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.32074880599975586, \"iteration\": 161, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.30744752287864685, \"iteration\": 162, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.30196407437324524, \"iteration\": 163, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.2085312306880951, \"iteration\": 164, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.27661120891571045, \"iteration\": 165, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.23024389147758484, \"iteration\": 166, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27793675661087036, \"iteration\": 167, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2943562865257263, \"iteration\": 168, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2645540237426758, \"iteration\": 169, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.31178969144821167, \"iteration\": 170, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.250938355922699, \"iteration\": 171, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3386722207069397, \"iteration\": 172, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2462191879749298, \"iteration\": 173, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.16638463735580444, \"iteration\": 174, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2701812982559204, \"iteration\": 175, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27120140194892883, \"iteration\": 176, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2251865416765213, \"iteration\": 177, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3033382296562195, \"iteration\": 178, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.18209140002727509, \"iteration\": 179, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2524240016937256, \"iteration\": 180, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2703630030155182, \"iteration\": 181, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.21012771129608154, \"iteration\": 182, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.29947760701179504, \"iteration\": 183, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.21861106157302856, \"iteration\": 184, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.23722627758979797, \"iteration\": 185, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2542347013950348, \"iteration\": 186, \"epoch\": 3}, {\"training_acc\": 0.953125, \"training_loss\": 0.24196287989616394, \"iteration\": 187, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.25755658745765686, \"iteration\": 188, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2314990907907486, \"iteration\": 189, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2536689043045044, \"iteration\": 190, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22987163066864014, \"iteration\": 191, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.25936436653137207, \"iteration\": 192, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.30702418088912964, \"iteration\": 193, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2389642745256424, \"iteration\": 194, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23668727278709412, \"iteration\": 195, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.24700553715229034, \"iteration\": 196, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.26693764328956604, \"iteration\": 197, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.25192031264305115, \"iteration\": 198, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.24271681904792786, \"iteration\": 199, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.26928627490997314, \"iteration\": 200, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.27463558316230774, \"iteration\": 201, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2853286862373352, \"iteration\": 202, \"epoch\": 3}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22136500477790833, \"iteration\": 203, \"epoch\": 3}, {\"training_acc\": 0.922077922077922, \"training_loss\": 0.23846697807312012, \"iteration\": 204, \"epoch\": 3}, {\"training_acc\": 0.9921875, \"training_loss\": 0.32595404982566833, \"iteration\": 205, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11938406527042389, \"iteration\": 206, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.14785300195217133, \"iteration\": 207, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0885976105928421, \"iteration\": 208, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0961090624332428, \"iteration\": 209, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.07383057475090027, \"iteration\": 210, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09113252907991409, \"iteration\": 211, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07524754106998444, \"iteration\": 212, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10994663089513779, \"iteration\": 213, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09270872920751572, \"iteration\": 214, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09803610295057297, \"iteration\": 215, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11584251374006271, \"iteration\": 216, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08312590420246124, \"iteration\": 217, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08099411427974701, \"iteration\": 218, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.12788423895835876, \"iteration\": 219, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12371475249528885, \"iteration\": 220, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08525592088699341, \"iteration\": 221, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14225097000598907, \"iteration\": 222, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10508093237876892, \"iteration\": 223, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1826428920030594, \"iteration\": 224, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.15500830113887787, \"iteration\": 225, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11108330637216568, \"iteration\": 226, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.09871428459882736, \"iteration\": 227, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.12469945102930069, \"iteration\": 228, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.13727062940597534, \"iteration\": 229, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1276317834854126, \"iteration\": 230, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10188660770654678, \"iteration\": 231, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1387769877910614, \"iteration\": 232, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09921209514141083, \"iteration\": 233, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.10019249469041824, \"iteration\": 234, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.13456152379512787, \"iteration\": 235, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09425099194049835, \"iteration\": 236, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.08213922381401062, \"iteration\": 237, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05837593227624893, \"iteration\": 238, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.06392662227153778, \"iteration\": 239, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11814956367015839, \"iteration\": 240, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.06880907714366913, \"iteration\": 241, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17104852199554443, \"iteration\": 242, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.16600777208805084, \"iteration\": 243, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.061306796967983246, \"iteration\": 244, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13639295101165771, \"iteration\": 245, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0934828445315361, \"iteration\": 246, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.18658539652824402, \"iteration\": 247, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1277489811182022, \"iteration\": 248, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.167902410030365, \"iteration\": 249, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14095497131347656, \"iteration\": 250, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.13212475180625916, \"iteration\": 251, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.10601113736629486, \"iteration\": 252, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.11360527575016022, \"iteration\": 253, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11534922569990158, \"iteration\": 254, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.11268928647041321, \"iteration\": 255, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07505621761083603, \"iteration\": 256, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.06343575567007065, \"iteration\": 257, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13101811707019806, \"iteration\": 258, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11501973867416382, \"iteration\": 259, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.1052011102437973, \"iteration\": 260, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11733369529247284, \"iteration\": 261, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.13084948062896729, \"iteration\": 262, \"epoch\": 4}, {\"training_acc\": 1.0, \"training_loss\": 0.038227763026952744, \"iteration\": 263, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12178662419319153, \"iteration\": 264, \"epoch\": 4}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08001526445150375, \"iteration\": 265, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.09702097624540329, \"iteration\": 266, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.15137289464473724, \"iteration\": 267, \"epoch\": 4}, {\"training_acc\": 0.984375, \"training_loss\": 0.0968826487660408, \"iteration\": 268, \"epoch\": 4}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12185204029083252, \"iteration\": 269, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.0913148820400238, \"iteration\": 270, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.11603553593158722, \"iteration\": 271, \"epoch\": 4}, {\"training_acc\": 0.974025974025974, \"training_loss\": 0.09246918559074402, \"iteration\": 272, \"epoch\": 4}, {\"training_acc\": 0.9765625, \"training_loss\": 0.2972351014614105, \"iteration\": 273, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04398740455508232, \"iteration\": 274, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03151768818497658, \"iteration\": 275, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.06234206631779671, \"iteration\": 276, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027627378702163696, \"iteration\": 277, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08509338647127151, \"iteration\": 278, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0351107120513916, \"iteration\": 279, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03439304977655411, \"iteration\": 280, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.033894095569849014, \"iteration\": 281, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03496413305401802, \"iteration\": 282, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.08168671280145645, \"iteration\": 283, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.0750531256198883, \"iteration\": 284, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02862655743956566, \"iteration\": 285, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06296246498823166, \"iteration\": 286, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02970820665359497, \"iteration\": 287, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.056098807603120804, \"iteration\": 288, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03509689122438431, \"iteration\": 289, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07353875786066055, \"iteration\": 290, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.027058042585849762, \"iteration\": 291, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05186653137207031, \"iteration\": 292, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023661978542804718, \"iteration\": 293, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.019259553402662277, \"iteration\": 294, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1391950249671936, \"iteration\": 295, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05129937827587128, \"iteration\": 296, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.028525162488222122, \"iteration\": 297, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04758400470018387, \"iteration\": 298, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0492340587079525, \"iteration\": 299, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04754982888698578, \"iteration\": 300, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05130269005894661, \"iteration\": 301, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02226799912750721, \"iteration\": 302, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02433563582599163, \"iteration\": 303, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02714850939810276, \"iteration\": 304, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.025232303887605667, \"iteration\": 305, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.03323320299386978, \"iteration\": 306, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.031104063615202904, \"iteration\": 307, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.043643511831760406, \"iteration\": 308, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07089457660913467, \"iteration\": 309, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03983480483293533, \"iteration\": 310, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.058908313512802124, \"iteration\": 311, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.019197698682546616, \"iteration\": 312, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.05322132259607315, \"iteration\": 313, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.030699793249368668, \"iteration\": 314, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.06202804669737816, \"iteration\": 315, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.017838332802057266, \"iteration\": 316, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.054421354085206985, \"iteration\": 317, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.029860585927963257, \"iteration\": 318, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.02172509953379631, \"iteration\": 319, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023794058710336685, \"iteration\": 320, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05452565848827362, \"iteration\": 321, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.0201981533318758, \"iteration\": 322, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01806642860174179, \"iteration\": 323, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023646339774131775, \"iteration\": 324, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.019793745130300522, \"iteration\": 325, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.059280503541231155, \"iteration\": 326, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03363539278507233, \"iteration\": 327, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10284776240587234, \"iteration\": 328, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.033080752938985825, \"iteration\": 329, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.036596428602933884, \"iteration\": 330, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.023087404668331146, \"iteration\": 331, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03984302654862404, \"iteration\": 332, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.07949799299240112, \"iteration\": 333, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05966884270310402, \"iteration\": 334, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04041527211666107, \"iteration\": 335, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04953692480921745, \"iteration\": 336, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.030280206352472305, \"iteration\": 337, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.029478885233402252, \"iteration\": 338, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.03899313881993294, \"iteration\": 339, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.01638798415660858, \"iteration\": 340, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.3943028450012207, \"iteration\": 341, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013438539579510689, \"iteration\": 342, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.046080440282821655, \"iteration\": 343, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.031019562855362892, \"iteration\": 344, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.024207307025790215, \"iteration\": 345, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013472574763000011, \"iteration\": 346, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.015325983986258507, \"iteration\": 347, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.015106825157999992, \"iteration\": 348, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.036750517785549164, \"iteration\": 349, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014807270839810371, \"iteration\": 350, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02230827882885933, \"iteration\": 351, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011447505094110966, \"iteration\": 352, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.015545238740742207, \"iteration\": 353, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04925723373889923, \"iteration\": 354, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01736569032073021, \"iteration\": 355, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014191783964633942, \"iteration\": 356, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.018182285130023956, \"iteration\": 357, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00787571631371975, \"iteration\": 358, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.014328873716294765, \"iteration\": 359, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0144161032512784, \"iteration\": 360, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03672841563820839, \"iteration\": 361, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.020294804126024246, \"iteration\": 362, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012791091576218605, \"iteration\": 363, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009184065274894238, \"iteration\": 364, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011280725710093975, \"iteration\": 365, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03382747992873192, \"iteration\": 366, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013736383989453316, \"iteration\": 367, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009933579713106155, \"iteration\": 368, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.0078001623041927814, \"iteration\": 369, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011162285692989826, \"iteration\": 370, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009861555881798267, \"iteration\": 371, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.017880501225590706, \"iteration\": 372, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012531720101833344, \"iteration\": 373, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010328743606805801, \"iteration\": 374, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009385040029883385, \"iteration\": 375, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.059364140033721924, \"iteration\": 376, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.006597878411412239, \"iteration\": 377, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.06895419955253601, \"iteration\": 378, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008654704317450523, \"iteration\": 379, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010469774715602398, \"iteration\": 380, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.011513791047036648, \"iteration\": 381, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.013178764842450619, \"iteration\": 382, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03931812196969986, \"iteration\": 383, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04686405882239342, \"iteration\": 384, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.023398330435156822, \"iteration\": 385, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.027033289894461632, \"iteration\": 386, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.041194990277290344, \"iteration\": 387, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03182852268218994, \"iteration\": 388, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.016742544248700142, \"iteration\": 389, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.010053519159555435, \"iteration\": 390, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007079877890646458, \"iteration\": 391, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009194537065923214, \"iteration\": 392, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012835157103836536, \"iteration\": 393, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.00980024691671133, \"iteration\": 394, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021352136507630348, \"iteration\": 395, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01235004048794508, \"iteration\": 396, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.01055937446653843, \"iteration\": 397, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.023625383153557777, \"iteration\": 398, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02391812950372696, \"iteration\": 399, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.007740010507404804, \"iteration\": 400, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.06347596645355225, \"iteration\": 401, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.008406616747379303, \"iteration\": 402, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.012210015207529068, \"iteration\": 403, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.028375012800097466, \"iteration\": 404, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02992074377834797, \"iteration\": 405, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0215903427451849, \"iteration\": 406, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.0641680359840393, \"iteration\": 407, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.009606749750673771, \"iteration\": 408, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.4495002329349518, \"iteration\": 409, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006784455850720406, \"iteration\": 410, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006144840270280838, \"iteration\": 411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0058011244982481, \"iteration\": 412, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02324787527322769, \"iteration\": 413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.011796606704592705, \"iteration\": 414, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0066189831122756, \"iteration\": 415, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009173300117254257, \"iteration\": 416, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006040269508957863, \"iteration\": 417, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006550120189785957, \"iteration\": 418, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.023146172985434532, \"iteration\": 419, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.03543715924024582, \"iteration\": 420, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006776223890483379, \"iteration\": 421, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0062565733678638935, \"iteration\": 422, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010400624945759773, \"iteration\": 423, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007240645587444305, \"iteration\": 424, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.05089898407459259, \"iteration\": 425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0072829704731702805, \"iteration\": 426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005857513751834631, \"iteration\": 427, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010062148794531822, \"iteration\": 428, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0066674696281552315, \"iteration\": 429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00566704710945487, \"iteration\": 430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.013498489744961262, \"iteration\": 431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00745298620313406, \"iteration\": 432, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.023237979039549828, \"iteration\": 433, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008525095880031586, \"iteration\": 434, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.020787136629223824, \"iteration\": 435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007441013585776091, \"iteration\": 436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0062142908573150635, \"iteration\": 437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006189329084008932, \"iteration\": 438, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009479666128754616, \"iteration\": 439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007471697870641947, \"iteration\": 440, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007343553006649017, \"iteration\": 441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005014701280742884, \"iteration\": 442, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01565946452319622, \"iteration\": 443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005594994872808456, \"iteration\": 444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005618338938802481, \"iteration\": 445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00761649664491415, \"iteration\": 446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004548713099211454, \"iteration\": 447, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006040944252163172, \"iteration\": 448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005017426330596209, \"iteration\": 449, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006007438059896231, \"iteration\": 450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004291506949812174, \"iteration\": 451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006380872800946236, \"iteration\": 452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005431358702480793, \"iteration\": 453, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007828574627637863, \"iteration\": 454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010257674381136894, \"iteration\": 455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004738122224807739, \"iteration\": 456, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.03899143636226654, \"iteration\": 457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005734539125114679, \"iteration\": 458, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.003546318504959345, \"iteration\": 459, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03776269033551216, \"iteration\": 460, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03664546087384224, \"iteration\": 461, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.009552119299769402, \"iteration\": 462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008931919001042843, \"iteration\": 463, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008320885710418224, \"iteration\": 464, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0035296494606882334, \"iteration\": 465, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.005131412763148546, \"iteration\": 466, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008417516946792603, \"iteration\": 467, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.018484167754650116, \"iteration\": 468, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00940826628357172, \"iteration\": 469, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03118938021361828, \"iteration\": 470, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.004831535741686821, \"iteration\": 471, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.007771538104861975, \"iteration\": 472, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.008833076804876328, \"iteration\": 473, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.006880469620227814, \"iteration\": 474, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.011854615062475204, \"iteration\": 475, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.00853410642594099, \"iteration\": 476, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.1868542581796646, \"iteration\": 477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004898718558251858, \"iteration\": 478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003901462070643902, \"iteration\": 479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0031307346653193235, \"iteration\": 480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004640848841518164, \"iteration\": 481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00551396980881691, \"iteration\": 482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005276706535369158, \"iteration\": 483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005375234875828028, \"iteration\": 484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00541033735498786, \"iteration\": 485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010245826095342636, \"iteration\": 486, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03325233235955238, \"iteration\": 487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004602598026394844, \"iteration\": 488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003842294216156006, \"iteration\": 489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0034035881981253624, \"iteration\": 490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0038494481705129147, \"iteration\": 491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0030174031853675842, \"iteration\": 492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005752750672399998, \"iteration\": 493, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.010418696328997612, \"iteration\": 494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003562628524377942, \"iteration\": 495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0031844486948102713, \"iteration\": 496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004283080343157053, \"iteration\": 497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003521390724927187, \"iteration\": 498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003862051060423255, \"iteration\": 499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004154253285378218, \"iteration\": 500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004731224849820137, \"iteration\": 501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003896556328982115, \"iteration\": 502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0076046292670071125, \"iteration\": 503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00313297426328063, \"iteration\": 504, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004773289430886507, \"iteration\": 505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0034736657980829477, \"iteration\": 506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0026337734889239073, \"iteration\": 507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0026443128008395433, \"iteration\": 508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0028534920420497656, \"iteration\": 509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003978450316935778, \"iteration\": 510, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004286565352231264, \"iteration\": 511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005477267783135176, \"iteration\": 512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0031464085914194584, \"iteration\": 513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007799653802067041, \"iteration\": 514, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01702149212360382, \"iteration\": 515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002353247255086899, \"iteration\": 516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0023520118556916714, \"iteration\": 517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003472737967967987, \"iteration\": 518, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.021854223683476448, \"iteration\": 519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003104712814092636, \"iteration\": 520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002774858381599188, \"iteration\": 521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005140362307429314, \"iteration\": 522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0032198522239923477, \"iteration\": 523, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01244656927883625, \"iteration\": 524, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0036302623338997364, \"iteration\": 525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0026805419474840164, \"iteration\": 526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004008438438177109, \"iteration\": 527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003007124410942197, \"iteration\": 528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002994327340275049, \"iteration\": 529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0038322494365274906, \"iteration\": 530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003082630690187216, \"iteration\": 531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0025886010844260454, \"iteration\": 532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002350620459765196, \"iteration\": 533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007329780142754316, \"iteration\": 534, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.018379762768745422, \"iteration\": 535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.004363559652119875, \"iteration\": 536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008700198493897915, \"iteration\": 537, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.012993620708584785, \"iteration\": 538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.002323080087080598, \"iteration\": 539, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003249580040574074, \"iteration\": 540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.003524950472638011, \"iteration\": 541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0026777915190905333, \"iteration\": 542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0032898152712732553, \"iteration\": 543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0040046293288469315, \"iteration\": 544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.29020220041275024, \"iteration\": 545, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020417426712810993, \"iteration\": 546, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004944669082760811, \"iteration\": 547, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029879857320338488, \"iteration\": 548, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004286635667085648, \"iteration\": 549, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006744308862835169, \"iteration\": 550, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0053620790131390095, \"iteration\": 551, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0033316141925752163, \"iteration\": 552, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.018187524750828743, \"iteration\": 553, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023528961464762688, \"iteration\": 554, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003319178707897663, \"iteration\": 555, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023915416095405817, \"iteration\": 556, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002061669249087572, \"iteration\": 557, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0028627896681427956, \"iteration\": 558, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0031052178237587214, \"iteration\": 559, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003308479208499193, \"iteration\": 560, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0030488306656479836, \"iteration\": 561, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002790253609418869, \"iteration\": 562, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002118839882314205, \"iteration\": 563, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005671825725585222, \"iteration\": 564, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0028437983710318804, \"iteration\": 565, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00348463817499578, \"iteration\": 566, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003433598903939128, \"iteration\": 567, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003716513281688094, \"iteration\": 568, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0030960130970925093, \"iteration\": 569, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002386326901614666, \"iteration\": 570, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002074980642646551, \"iteration\": 571, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002182830823585391, \"iteration\": 572, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00685340678319335, \"iteration\": 573, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003021581331267953, \"iteration\": 574, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006353913340717554, \"iteration\": 575, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002828113501891494, \"iteration\": 576, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0025921487249433994, \"iteration\": 577, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0038363770581781864, \"iteration\": 578, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003037090413272381, \"iteration\": 579, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0028925237711519003, \"iteration\": 580, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0019145052647218108, \"iteration\": 581, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002778469119220972, \"iteration\": 582, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029110671021044254, \"iteration\": 583, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0018140278989449143, \"iteration\": 584, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0030046282336115837, \"iteration\": 585, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023875124752521515, \"iteration\": 586, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017365437233820558, \"iteration\": 587, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0070781465619802475, \"iteration\": 588, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006025190465152264, \"iteration\": 589, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0024476468097418547, \"iteration\": 590, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0017862566746771336, \"iteration\": 591, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00175850884988904, \"iteration\": 592, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029246644116938114, \"iteration\": 593, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006948045454919338, \"iteration\": 594, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.009845823049545288, \"iteration\": 595, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002112990478053689, \"iteration\": 596, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020789566915482283, \"iteration\": 597, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0023266293574124575, \"iteration\": 598, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0014757075114175677, \"iteration\": 599, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0022773602977395058, \"iteration\": 600, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003412880701944232, \"iteration\": 601, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0020970646291971207, \"iteration\": 602, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004628192633390427, \"iteration\": 603, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0026870854198932648, \"iteration\": 604, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002762449672445655, \"iteration\": 605, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003285227110609412, \"iteration\": 606, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002993193222209811, \"iteration\": 607, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0032189979683607817, \"iteration\": 608, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004487833008170128, \"iteration\": 609, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.001634571235626936, \"iteration\": 610, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0019101040670648217, \"iteration\": 611, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0037872621323913336, \"iteration\": 612, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.34172120690345764, \"iteration\": 613, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005374407861381769, \"iteration\": 614, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.010739893652498722, \"iteration\": 615, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.012547767721116543, \"iteration\": 616, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.02842254377901554, \"iteration\": 617, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.012773951515555382, \"iteration\": 618, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00643906882032752, \"iteration\": 619, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0026053297333419323, \"iteration\": 620, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018223782535642385, \"iteration\": 621, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0028596706688404083, \"iteration\": 622, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.008624129928648472, \"iteration\": 623, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.008207455277442932, \"iteration\": 624, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.010459408164024353, \"iteration\": 625, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.011693520471453667, \"iteration\": 626, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.006549452431499958, \"iteration\": 627, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00495401956140995, \"iteration\": 628, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0037704072892665863, \"iteration\": 629, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017870576120913029, \"iteration\": 630, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0029369336552917957, \"iteration\": 631, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005987228825688362, \"iteration\": 632, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015960922464728355, \"iteration\": 633, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004944504704326391, \"iteration\": 634, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0034717016387730837, \"iteration\": 635, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00909197423607111, \"iteration\": 636, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003354910295456648, \"iteration\": 637, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0038649647030979395, \"iteration\": 638, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0053123850375413895, \"iteration\": 639, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0030458809342235327, \"iteration\": 640, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003772999159991741, \"iteration\": 641, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002969388384371996, \"iteration\": 642, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0035569313913583755, \"iteration\": 643, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0027884747833013535, \"iteration\": 644, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005259108263999224, \"iteration\": 645, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.009620016440749168, \"iteration\": 646, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.003532437141984701, \"iteration\": 647, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002960438607260585, \"iteration\": 648, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004402058199048042, \"iteration\": 649, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004912899807095528, \"iteration\": 650, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018899735296145082, \"iteration\": 651, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004626078996807337, \"iteration\": 652, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002061129081994295, \"iteration\": 653, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0028831767849624157, \"iteration\": 654, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004468274302780628, \"iteration\": 655, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0042737689800560474, \"iteration\": 656, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0028932977002114058, \"iteration\": 657, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0037209163419902325, \"iteration\": 658, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0030371493194252253, \"iteration\": 659, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016030590049922466, \"iteration\": 660, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0011957953684031963, \"iteration\": 661, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0039409613236784935, \"iteration\": 662, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019924950320273638, \"iteration\": 663, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015307401772588491, \"iteration\": 664, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0021512287203222513, \"iteration\": 665, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0042509655468165874, \"iteration\": 666, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004914376884698868, \"iteration\": 667, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002263026311993599, \"iteration\": 668, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002668590983375907, \"iteration\": 669, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.005531961098313332, \"iteration\": 670, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.011473284102976322, \"iteration\": 671, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0027185510843992233, \"iteration\": 672, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001750495983287692, \"iteration\": 673, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019179468508809805, \"iteration\": 674, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002455343957990408, \"iteration\": 675, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0049179778434336185, \"iteration\": 676, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0033182722982019186, \"iteration\": 677, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002113667782396078, \"iteration\": 678, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.004116086754947901, \"iteration\": 679, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001498518162406981, \"iteration\": 680, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop out\n",
    "\n",
    "print(\"Train model\")\n",
    "models_dir = Path('models/hr')\n",
    "\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    "    optimizer_params= {\"lr\": 0.001, \"weight_decay\": 0.01, }\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_data_nn, batch_size=128, shuffle=True)\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "model_nn = NeuralNetwork(\n",
    "    input_size=len(tfidf_encoder.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    dropout=0.5,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn.pt').exists() and USE_CACHE:\n",
    "    model_nn = load_model(model_nn, models_dir, 'model_nn')\n",
    "else:\n",
    "    model_nn.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn, models_dir, \"model_nn\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # X_test = torch.stack([dta[0] for dta in test])\n",
    "    X_test = torch.stack([test[0] for test in test_data_nn]).to(model_nn.device)\n",
    "    y_test = torch.stack([test[1] for test in test_data_nn]).to(model_nn.device)\n",
    "    y_pred = model_nn.predict(X_test)\n",
    "\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, y_pred, average='binary'))\n",
    "print(\"AUC\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn, train_config, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other classifiers\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "\"Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC, SVM\n",
    "Effective in high dimensional spaces.\n",
    "\n",
    "Still effective in cases where number of dimensions is greater than the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC with TfIdf did good on balanced English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoanghapham/.pyenv/versions/3.11.5/envs/power-identification/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5743145743145743, 0.5031605562579013, 0.5363881401617251, None)\n",
      "AUC: 0.6378562997172315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "# LinearSVC, tfidf\n",
    "X_train = tfidf_encoder.transform(train_raw.texts)\n",
    "print(\"Fit model\")\n",
    "model_LinearSVC_tfidf = LinearSVC()\n",
    "model_LinearSVC_tfidf.fit(X_train, train_raw.labels)\n",
    "\n",
    "pred_LinearSVC_tfidf = model_LinearSVC_tfidf.predict(tfidf_encoder.transform(test_raw.texts))\n",
    "\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_LinearSVC_tfidf, average='binary'))\n",
    "print(\"AUC:\", roc_auc_score(y_test, pred_LinearSVC_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier\n",
    "SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "\n",
    "SGD is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6208695652173913, 0.45132743362831856, 0.5226939970717424, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6416236242929566"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model_SGDClassifier_tfidf = SGDClassifier()\n",
    "model_SGDClassifier_tfidf.fit(X_train, train_raw.labels)\n",
    "\n",
    "pred_SGDClassifier_tfidf = model_SGDClassifier_tfidf.predict(tfidf_encoder.transform(test_raw.texts))\n",
    "\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_SGDClassifier_tfidf, average='binary'))\n",
    "\n",
    "roc_auc_score(test_raw.labels, pred_SGDClassifier_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Overall bad performance, not worth pursuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4217687074829932, 0.5486725663716814, 0.47692307692307695, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5449608013045762"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model_GaussianNB_tfidf = GaussianNB()\n",
    "model_GaussianNB_tfidf.fit(X_train.toarray(), train_raw.labels)\n",
    "\n",
    "pred_GaussianNB_tfidf = model_GaussianNB_tfidf.predict(tfidf_encoder.transform(test_raw.texts).toarray())\n",
    "\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_GaussianNB_tfidf, average='binary'))\n",
    "roc_auc_score(test_raw.labels, pred_GaussianNB_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "- Neural network is still a good option\n",
    "- sklearn's SGD is also good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard count vectors & scale\n",
    "Not good on both LinearSVC and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoanghapham/.pyenv/versions/3.11.5/envs/power-identification/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/Users/hoanghapham/.pyenv/versions/3.11.5/envs/power-identification/lib/python3.11/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4963054187192118, 0.5094816687737042, 0.5028072364316906, None)\n",
      "AUC: 0.5970692846567055\n",
      "(0.48759305210918114, 0.4968394437420986, 0.49217282404508456, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5892061520946422"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "encoding_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('scaler', StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "encoding_pipeline.fit(train_raw.texts)\n",
    "\n",
    "X_train = encoding_pipeline.transform(train_raw.texts)\n",
    "\n",
    "\n",
    "print(\"Fit model\")\n",
    "model_LinearSVC_tfidf = LinearSVC()\n",
    "model_LinearSVC_tfidf.fit(X_train, train_raw.labels)\n",
    "pred_LinearSVC_tfidf = model_LinearSVC_tfidf.predict(encoding_pipeline.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_LinearSVC_tfidf, average='binary'))\n",
    "print(\"AUC:\", roc_auc_score(y_test, pred_LinearSVC_tfidf))\n",
    "\n",
    "model_SGDClassifier_tfidf = SGDClassifier()\n",
    "model_SGDClassifier_tfidf.fit(X_train, train_raw.labels)\n",
    "pred_SGDClassifier_tfidf = model_SGDClassifier_tfidf.predict(encoding_pipeline.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_SGDClassifier_tfidf, average='binary'))\n",
    "roc_auc_score(test_raw.labels, pred_SGDClassifier_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoanghapham/.pyenv/versions/3.11.5/envs/power-identification/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5041322314049587, 0.46270543615676357, 0.48253131179960446, None)\n",
      "AUC: 0.5925709139149277\n",
      "SGDClassifier\n",
      "(0.5388429752066116, 0.41213653603034134, 0.4670487106017192, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5985123697884936"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tfidf = TfidfVectorizer(sublinear_tf=True, analyzer=\"word\", ngram_range=(3,5), max_features=10000)\n",
    "\n",
    "X_train = word_tfidf.fit_transform(train_raw.texts)\n",
    "\n",
    "\n",
    "print(\"LinearSVC\")\n",
    "model_LinearSVC_tfidf = LinearSVC()\n",
    "model_LinearSVC_tfidf.fit(X_train, train_raw.labels)\n",
    "pred_LinearSVC_tfidf = model_LinearSVC_tfidf.predict(word_tfidf.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_LinearSVC_tfidf, average='binary'))\n",
    "print(\"AUC:\", roc_auc_score(y_test, pred_LinearSVC_tfidf))\n",
    "\n",
    "print(\"SGDClassifier\")\n",
    "model_SGDClassifier_tfidf = SGDClassifier()\n",
    "model_SGDClassifier_tfidf.fit(X_train, train_raw.labels)\n",
    "\n",
    "pred_SGDClassifier_tfidf = model_SGDClassifier_tfidf.predict(word_tfidf.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_SGDClassifier_tfidf, average='binary'))\n",
    "roc_auc_score(test_raw.labels, pred_SGDClassifier_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use more tfidf word (50000) features improve 1%, but takes much more time to transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoanghapham/.pyenv/versions/3.11.5/envs/power-identification/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5390505359877489, 0.4450063211125158, 0.48753462603878117, None)\n",
      "AUC: 0.6064661520751476\n",
      "SGDClassifier\n",
      "(0.5610169491525424, 0.41845764854614415, 0.47936278059377263, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6093830262776981"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tfidf = TfidfVectorizer(sublinear_tf=True, analyzer=\"word\", ngram_range=(3,7), max_features=50000)\n",
    "\n",
    "X_train = word_tfidf.fit_transform(train_raw.texts)\n",
    "\n",
    "import scipy\n",
    "scipy.sparse.save_npz(\"models/tfidf/ngram_word_3to7_50000.npz\", X_train)\n",
    "\n",
    "print(\"LinearSVC\")\n",
    "model_LinearSVC_tfidf = LinearSVC()\n",
    "model_LinearSVC_tfidf.fit(X_train, train_raw.labels)\n",
    "pred_LinearSVC_tfidf = model_LinearSVC_tfidf.predict(word_tfidf.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_LinearSVC_tfidf, average='binary'))\n",
    "print(\"AUC:\", roc_auc_score(y_test, pred_LinearSVC_tfidf))\n",
    "\n",
    "print(\"SGDClassifier\")\n",
    "model_SGDClassifier_tfidf = SGDClassifier()\n",
    "model_SGDClassifier_tfidf.fit(X_train, train_raw.labels)\n",
    "\n",
    "pred_SGDClassifier_tfidf = model_SGDClassifier_tfidf.predict(word_tfidf.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_SGDClassifier_tfidf, average='binary'))\n",
    "roc_auc_score(test_raw.labels, pred_SGDClassifier_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Char ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoanghapham/.pyenv/versions/3.11.5/envs/power-identification/lib/python3.11/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5706371191135734, 0.5208596713021492, 0.5446133509583608, None)\n",
      "AUC: 0.6409232820658781\n",
      "SGDClassifier\n",
      "(0.6167247386759582, 0.4475347661188369, 0.5186813186813187, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6389562805150854"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tfidf = TfidfVectorizer(sublinear_tf=True, analyzer=\"char\", ngram_range=(3,7), max_features=50000)\n",
    "\n",
    "X_train = char_tfidf.fit_transform(train_raw.texts)\n",
    "\n",
    "import scipy\n",
    "scipy.sparse.save_npz(\"models/tfidf/ngram_char_3to7_50000.npz\", X_train)\n",
    "\n",
    "print(\"LinearSVC\")\n",
    "model_LinearSVC_tfidf = LinearSVC()\n",
    "model_LinearSVC_tfidf.fit(X_train, train_raw.labels)\n",
    "pred_LinearSVC_tfidf = model_LinearSVC_tfidf.predict(char_tfidf.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_LinearSVC_tfidf, average='binary'))\n",
    "print(\"AUC:\", roc_auc_score(y_test, pred_LinearSVC_tfidf))\n",
    "\n",
    "print(\"SGDClassifier\")\n",
    "model_SGDClassifier_tfidf = SGDClassifier()\n",
    "model_SGDClassifier_tfidf.fit(X_train, train_raw.labels)\n",
    "\n",
    "pred_SGDClassifier_tfidf = model_SGDClassifier_tfidf.predict(char_tfidf.transform(test_raw.texts))\n",
    "print(precision_recall_fscore_support(test_raw.labels, pred_SGDClassifier_tfidf, average='binary'))\n",
    "roc_auc_score(test_raw.labels, pred_SGDClassifier_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 68/68 [00:01<00:00, 50.77batch/s, batch_accuracy=0.623, loss=0.663]\n",
      "Epoch 2: 100%|██████████| 68/68 [00:01<00:00, 57.77batch/s, batch_accuracy=0.87, loss=0.382] \n",
      "Epoch 3: 100%|██████████| 68/68 [00:01<00:00, 62.57batch/s, batch_accuracy=0.896, loss=0.286]\n",
      "Epoch 4: 100%|██████████| 68/68 [00:01<00:00, 62.21batch/s, batch_accuracy=0.948, loss=0.213]\n",
      "Epoch 5: 100%|██████████| 68/68 [00:01<00:00, 59.00batch/s, batch_accuracy=0.974, loss=0.126] \n",
      "Epoch 6: 100%|██████████| 68/68 [00:01<00:00, 61.50batch/s, batch_accuracy=0.987, loss=0.0883]\n",
      "Epoch 7: 100%|██████████| 68/68 [00:01<00:00, 59.31batch/s, batch_accuracy=1, loss=0.045]     \n",
      "Epoch 8: 100%|██████████| 68/68 [00:01<00:00, 59.67batch/s, batch_accuracy=1, loss=0.019]     \n",
      "Epoch 9: 100%|██████████| 68/68 [00:01<00:00, 63.09batch/s, batch_accuracy=1, loss=0.00419]   \n",
      "Epoch 10: 100%|██████████| 68/68 [00:01<00:00, 62.57batch/s, batch_accuracy=1, loss=0.00185]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.546975546975547, 0.5372945638432364, 0.5420918367346939, None)\n",
      "AUC 0.6329495178506852\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-b82ec4132347466f88f5bad128d267e4.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-b82ec4132347466f88f5bad128d267e4.vega-embed details,\n",
       "  #altair-viz-b82ec4132347466f88f5bad128d267e4.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-b82ec4132347466f88f5bad128d267e4\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-b82ec4132347466f88f5bad128d267e4\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-b82ec4132347466f88f5bad128d267e4\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"hconcat\": [{\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Accuracy\"}, \"field\": \"training_acc\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Accuracy\", \"width\": 600}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"epoch\", \"scale\": {\"scheme\": \"category20\"}, \"title\": \"Epoch\", \"type\": \"nominal\"}, \"x\": {\"axis\": {\"labelAngle\": -30, \"title\": \"Iteration\", \"values\": [0, 100, 200, 300, 400, 500, 600]}, \"field\": \"iteration\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"title\": \"Loss\"}, \"field\": \"training_loss\", \"type\": \"quantitative\"}}, \"height\": 400, \"title\": \"Training Loss\", \"width\": 600}], \"data\": {\"name\": \"data-b6f1f056a09e7fc2fd3d66668f6f7b91\"}, \"title\": \"Base Neural Network with Tf-Idf vectors\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-b6f1f056a09e7fc2fd3d66668f6f7b91\": [{\"training_acc\": 0.5625, \"training_loss\": 0.6932743787765503, \"iteration\": 1, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6901644468307495, \"iteration\": 2, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6814019680023193, \"iteration\": 3, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6833067536354065, \"iteration\": 4, \"epoch\": 1}, {\"training_acc\": 0.5859375, \"training_loss\": 0.6812025308609009, \"iteration\": 5, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.673845648765564, \"iteration\": 6, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6894456148147583, \"iteration\": 7, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6692498326301575, \"iteration\": 8, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6819270849227905, \"iteration\": 9, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6879208087921143, \"iteration\": 10, \"epoch\": 1}, {\"training_acc\": 0.5390625, \"training_loss\": 0.6931805610656738, \"iteration\": 11, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6522355079650879, \"iteration\": 12, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6602270007133484, \"iteration\": 13, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6805087327957153, \"iteration\": 14, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6540215611457825, \"iteration\": 15, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.6761745810508728, \"iteration\": 16, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6653459072113037, \"iteration\": 17, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.6642755270004272, \"iteration\": 18, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.6390883326530457, \"iteration\": 19, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6500515937805176, \"iteration\": 20, \"epoch\": 1}, {\"training_acc\": 0.546875, \"training_loss\": 0.6791224479675293, \"iteration\": 21, \"epoch\": 1}, {\"training_acc\": 0.625, \"training_loss\": 0.6337032318115234, \"iteration\": 22, \"epoch\": 1}, {\"training_acc\": 0.5625, \"training_loss\": 0.6675360202789307, \"iteration\": 23, \"epoch\": 1}, {\"training_acc\": 0.5546875, \"training_loss\": 0.6818902492523193, \"iteration\": 24, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6432337760925293, \"iteration\": 25, \"epoch\": 1}, {\"training_acc\": 0.578125, \"training_loss\": 0.6707736253738403, \"iteration\": 26, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6537583470344543, \"iteration\": 27, \"epoch\": 1}, {\"training_acc\": 0.5703125, \"training_loss\": 0.687864363193512, \"iteration\": 28, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6382536888122559, \"iteration\": 29, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.6352335214614868, \"iteration\": 30, \"epoch\": 1}, {\"training_acc\": 0.640625, \"training_loss\": 0.6547431945800781, \"iteration\": 31, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6336091160774231, \"iteration\": 32, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6515318751335144, \"iteration\": 33, \"epoch\": 1}, {\"training_acc\": 0.6015625, \"training_loss\": 0.6540376543998718, \"iteration\": 34, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6581780910491943, \"iteration\": 35, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6355414390563965, \"iteration\": 36, \"epoch\": 1}, {\"training_acc\": 0.609375, \"training_loss\": 0.6669118404388428, \"iteration\": 37, \"epoch\": 1}, {\"training_acc\": 0.59375, \"training_loss\": 0.671922504901886, \"iteration\": 38, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6112825870513916, \"iteration\": 39, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.6138001084327698, \"iteration\": 40, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6159404516220093, \"iteration\": 41, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6198060512542725, \"iteration\": 42, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6401335000991821, \"iteration\": 43, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.6046587824821472, \"iteration\": 44, \"epoch\": 1}, {\"training_acc\": 0.6953125, \"training_loss\": 0.6017237305641174, \"iteration\": 45, \"epoch\": 1}, {\"training_acc\": 0.6328125, \"training_loss\": 0.6421446204185486, \"iteration\": 46, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.5903939604759216, \"iteration\": 47, \"epoch\": 1}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5519573092460632, \"iteration\": 48, \"epoch\": 1}, {\"training_acc\": 0.65625, \"training_loss\": 0.5968178510665894, \"iteration\": 49, \"epoch\": 1}, {\"training_acc\": 0.6171875, \"training_loss\": 0.6341570019721985, \"iteration\": 50, \"epoch\": 1}, {\"training_acc\": 0.6796875, \"training_loss\": 0.6125327348709106, \"iteration\": 51, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.6036023497581482, \"iteration\": 52, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.6538833379745483, \"iteration\": 53, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.5997769236564636, \"iteration\": 54, \"epoch\": 1}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5834324955940247, \"iteration\": 55, \"epoch\": 1}, {\"training_acc\": 0.671875, \"training_loss\": 0.6351321935653687, \"iteration\": 56, \"epoch\": 1}, {\"training_acc\": 0.71875, \"training_loss\": 0.6447323560714722, \"iteration\": 57, \"epoch\": 1}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5684255361557007, \"iteration\": 58, \"epoch\": 1}, {\"training_acc\": 0.6875, \"training_loss\": 0.5936922430992126, \"iteration\": 59, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6170284748077393, \"iteration\": 60, \"epoch\": 1}, {\"training_acc\": 0.6640625, \"training_loss\": 0.6169261336326599, \"iteration\": 61, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.530825674533844, \"iteration\": 62, \"epoch\": 1}, {\"training_acc\": 0.7109375, \"training_loss\": 0.584532618522644, \"iteration\": 63, \"epoch\": 1}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5685907006263733, \"iteration\": 64, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5461153984069824, \"iteration\": 65, \"epoch\": 1}, {\"training_acc\": 0.703125, \"training_loss\": 0.6213070750236511, \"iteration\": 66, \"epoch\": 1}, {\"training_acc\": 0.75, \"training_loss\": 0.5571601986885071, \"iteration\": 67, \"epoch\": 1}, {\"training_acc\": 0.6233766233766234, \"training_loss\": 0.6632435917854309, \"iteration\": 68, \"epoch\": 1}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5101214647293091, \"iteration\": 69, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.48101806640625, \"iteration\": 70, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4786659777164459, \"iteration\": 71, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.54518061876297, \"iteration\": 72, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.5328422784805298, \"iteration\": 73, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.481607586145401, \"iteration\": 74, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4730672836303711, \"iteration\": 75, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.49311190843582153, \"iteration\": 76, \"epoch\": 2}, {\"training_acc\": 0.859375, \"training_loss\": 0.43179261684417725, \"iteration\": 77, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.5155584812164307, \"iteration\": 78, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.505156397819519, \"iteration\": 79, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4937230348587036, \"iteration\": 80, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 0.5086276531219482, \"iteration\": 81, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.4658680558204651, \"iteration\": 82, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.46115443110466003, \"iteration\": 83, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4716643691062927, \"iteration\": 84, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.45482420921325684, \"iteration\": 85, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.45425671339035034, \"iteration\": 86, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.43657511472702026, \"iteration\": 87, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.5119597911834717, \"iteration\": 88, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4521915912628174, \"iteration\": 89, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.40588831901550293, \"iteration\": 90, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3942950963973999, \"iteration\": 91, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.47713014483451843, \"iteration\": 92, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.43595564365386963, \"iteration\": 93, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5098240971565247, \"iteration\": 94, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.48122838139533997, \"iteration\": 95, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4209059178829193, \"iteration\": 96, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4439786374568939, \"iteration\": 97, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4739696979522705, \"iteration\": 98, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.5408997535705566, \"iteration\": 99, \"epoch\": 2}, {\"training_acc\": 0.7265625, \"training_loss\": 0.5314884185791016, \"iteration\": 100, \"epoch\": 2}, {\"training_acc\": 0.75, \"training_loss\": 0.5048731565475464, \"iteration\": 101, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.43163856863975525, \"iteration\": 102, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.46758395433425903, \"iteration\": 103, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.46971094608306885, \"iteration\": 104, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.4286746084690094, \"iteration\": 105, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.4649980664253235, \"iteration\": 106, \"epoch\": 2}, {\"training_acc\": 0.7734375, \"training_loss\": 0.6206891536712646, \"iteration\": 107, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.5152618885040283, \"iteration\": 108, \"epoch\": 2}, {\"training_acc\": 0.8203125, \"training_loss\": 0.45392489433288574, \"iteration\": 109, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.41184449195861816, \"iteration\": 110, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.45406633615493774, \"iteration\": 111, \"epoch\": 2}, {\"training_acc\": 0.8515625, \"training_loss\": 0.45457103848457336, \"iteration\": 112, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4924670457839966, \"iteration\": 113, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.4499770700931549, \"iteration\": 114, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.4744085967540741, \"iteration\": 115, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.48632919788360596, \"iteration\": 116, \"epoch\": 2}, {\"training_acc\": 0.8125, \"training_loss\": 0.42911431193351746, \"iteration\": 117, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.5375131368637085, \"iteration\": 118, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.5597174167633057, \"iteration\": 119, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.4883655905723572, \"iteration\": 120, \"epoch\": 2}, {\"training_acc\": 0.7421875, \"training_loss\": 0.5338535904884338, \"iteration\": 121, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.5020570755004883, \"iteration\": 122, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.47716015577316284, \"iteration\": 123, \"epoch\": 2}, {\"training_acc\": 0.8359375, \"training_loss\": 0.4041954576969147, \"iteration\": 124, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.5133074522018433, \"iteration\": 125, \"epoch\": 2}, {\"training_acc\": 0.7890625, \"training_loss\": 0.48520174622535706, \"iteration\": 126, \"epoch\": 2}, {\"training_acc\": 0.828125, \"training_loss\": 0.40006956458091736, \"iteration\": 127, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.5051937103271484, \"iteration\": 128, \"epoch\": 2}, {\"training_acc\": 0.78125, \"training_loss\": 0.48614367842674255, \"iteration\": 129, \"epoch\": 2}, {\"training_acc\": 0.796875, \"training_loss\": 0.4795393645763397, \"iteration\": 130, \"epoch\": 2}, {\"training_acc\": 0.8671875, \"training_loss\": 0.37179937958717346, \"iteration\": 131, \"epoch\": 2}, {\"training_acc\": 0.7578125, \"training_loss\": 0.5582044124603271, \"iteration\": 132, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.42853477597236633, \"iteration\": 133, \"epoch\": 2}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4335569143295288, \"iteration\": 134, \"epoch\": 2}, {\"training_acc\": 0.765625, \"training_loss\": 0.5126338005065918, \"iteration\": 135, \"epoch\": 2}, {\"training_acc\": 0.8701298701298701, \"training_loss\": 0.38168865442276, \"iteration\": 136, \"epoch\": 2}, {\"training_acc\": 0.875, \"training_loss\": 0.3849732279777527, \"iteration\": 137, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3617387115955353, \"iteration\": 138, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.3815420866012573, \"iteration\": 139, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.322134405374527, \"iteration\": 140, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.343558669090271, \"iteration\": 141, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.3338282108306885, \"iteration\": 142, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3312707841396332, \"iteration\": 143, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3067777454853058, \"iteration\": 144, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 0.37297323346138, \"iteration\": 145, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.3600671887397766, \"iteration\": 146, \"epoch\": 3}, {\"training_acc\": 0.7890625, \"training_loss\": 0.4489307403564453, \"iteration\": 147, \"epoch\": 3}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2806684970855713, \"iteration\": 148, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.35659560561180115, \"iteration\": 149, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 0.36255645751953125, \"iteration\": 150, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3311406373977661, \"iteration\": 151, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.37742894887924194, \"iteration\": 152, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.3645395338535309, \"iteration\": 153, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.34600475430488586, \"iteration\": 154, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.33285897970199585, \"iteration\": 155, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.35968902707099915, \"iteration\": 156, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.31454145908355713, \"iteration\": 157, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27698010206222534, \"iteration\": 158, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 0.350846529006958, \"iteration\": 159, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 0.4510536193847656, \"iteration\": 160, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 0.43038997054100037, \"iteration\": 161, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.28913769125938416, \"iteration\": 162, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 0.3684624433517456, \"iteration\": 163, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.33836326003074646, \"iteration\": 164, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.33983129262924194, \"iteration\": 165, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.33322349190711975, \"iteration\": 166, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.4119718670845032, \"iteration\": 167, \"epoch\": 3}, {\"training_acc\": 0.8359375, \"training_loss\": 0.37939000129699707, \"iteration\": 168, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.35570257902145386, \"iteration\": 169, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.309866726398468, \"iteration\": 170, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.375610888004303, \"iteration\": 171, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.31532052159309387, \"iteration\": 172, \"epoch\": 3}, {\"training_acc\": 0.890625, \"training_loss\": 0.309299498796463, \"iteration\": 173, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.31669217348098755, \"iteration\": 174, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3107452094554901, \"iteration\": 175, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 0.3998379409313202, \"iteration\": 176, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 0.4317623972892761, \"iteration\": 177, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.29478397965431213, \"iteration\": 178, \"epoch\": 3}, {\"training_acc\": 0.9375, \"training_loss\": 0.2553715109825134, \"iteration\": 179, \"epoch\": 3}, {\"training_acc\": 0.90625, \"training_loss\": 0.26253020763397217, \"iteration\": 180, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2798561751842499, \"iteration\": 181, \"epoch\": 3}, {\"training_acc\": 0.8984375, \"training_loss\": 0.28533583879470825, \"iteration\": 182, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.34858518838882446, \"iteration\": 183, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.2872394919395447, \"iteration\": 184, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.3171887993812561, \"iteration\": 185, \"epoch\": 3}, {\"training_acc\": 0.84375, \"training_loss\": 0.4142126441001892, \"iteration\": 186, \"epoch\": 3}, {\"training_acc\": 0.8671875, \"training_loss\": 0.32650890946388245, \"iteration\": 187, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3538949191570282, \"iteration\": 188, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.31259340047836304, \"iteration\": 189, \"epoch\": 3}, {\"training_acc\": 0.875, \"training_loss\": 0.3169793486595154, \"iteration\": 190, \"epoch\": 3}, {\"training_acc\": 0.8046875, \"training_loss\": 0.4338986873626709, \"iteration\": 191, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.30957913398742676, \"iteration\": 192, \"epoch\": 3}, {\"training_acc\": 0.9140625, \"training_loss\": 0.29631829261779785, \"iteration\": 193, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 0.36073392629623413, \"iteration\": 194, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.3822349011898041, \"iteration\": 195, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 0.3981419801712036, \"iteration\": 196, \"epoch\": 3}, {\"training_acc\": 0.828125, \"training_loss\": 0.380876362323761, \"iteration\": 197, \"epoch\": 3}, {\"training_acc\": 0.8125, \"training_loss\": 0.38131415843963623, \"iteration\": 198, \"epoch\": 3}, {\"training_acc\": 0.8828125, \"training_loss\": 0.3244757056236267, \"iteration\": 199, \"epoch\": 3}, {\"training_acc\": 0.859375, \"training_loss\": 0.3847210705280304, \"iteration\": 200, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 0.3944760859012604, \"iteration\": 201, \"epoch\": 3}, {\"training_acc\": 0.8203125, \"training_loss\": 0.41372668743133545, \"iteration\": 202, \"epoch\": 3}, {\"training_acc\": 0.8515625, \"training_loss\": 0.395089328289032, \"iteration\": 203, \"epoch\": 3}, {\"training_acc\": 0.8961038961038961, \"training_loss\": 0.28623998165130615, \"iteration\": 204, \"epoch\": 3}, {\"training_acc\": 0.921875, \"training_loss\": 0.21592947840690613, \"iteration\": 205, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.29624223709106445, \"iteration\": 206, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.23863795399665833, \"iteration\": 207, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2271537184715271, \"iteration\": 208, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.24753837287425995, \"iteration\": 209, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.2140730619430542, \"iteration\": 210, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.24180954694747925, \"iteration\": 211, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2089308202266693, \"iteration\": 212, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2751484215259552, \"iteration\": 213, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.24795950949192047, \"iteration\": 214, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2439989596605301, \"iteration\": 215, \"epoch\": 4}, {\"training_acc\": 0.875, \"training_loss\": 0.27226805686950684, \"iteration\": 216, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.19362910091876984, \"iteration\": 217, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.2333703488111496, \"iteration\": 218, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.1738426685333252, \"iteration\": 219, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.24997273087501526, \"iteration\": 220, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.21789202094078064, \"iteration\": 221, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.27937400341033936, \"iteration\": 222, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.266050785779953, \"iteration\": 223, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.22588106989860535, \"iteration\": 224, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.27183353900909424, \"iteration\": 225, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2420058697462082, \"iteration\": 226, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2518652677536011, \"iteration\": 227, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.21631452441215515, \"iteration\": 228, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2111506462097168, \"iteration\": 229, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.23858362436294556, \"iteration\": 230, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.24203361570835114, \"iteration\": 231, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.3073386549949646, \"iteration\": 232, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.2608657479286194, \"iteration\": 233, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.275344580411911, \"iteration\": 234, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2135094702243805, \"iteration\": 235, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.2755509912967682, \"iteration\": 236, \"epoch\": 4}, {\"training_acc\": 0.8828125, \"training_loss\": 0.2692914605140686, \"iteration\": 237, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22139431536197662, \"iteration\": 238, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.2143341451883316, \"iteration\": 239, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.22085995972156525, \"iteration\": 240, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.22196468710899353, \"iteration\": 241, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.30143997073173523, \"iteration\": 242, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.2235262542963028, \"iteration\": 243, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2257235050201416, \"iteration\": 244, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.25094929337501526, \"iteration\": 245, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.30699655413627625, \"iteration\": 246, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2740961015224457, \"iteration\": 247, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26983970403671265, \"iteration\": 248, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.27192699909210205, \"iteration\": 249, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2594777047634125, \"iteration\": 250, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.24047835171222687, \"iteration\": 251, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.2778998017311096, \"iteration\": 252, \"epoch\": 4}, {\"training_acc\": 0.859375, \"training_loss\": 0.4115520715713501, \"iteration\": 253, \"epoch\": 4}, {\"training_acc\": 0.9296875, \"training_loss\": 0.2555694878101349, \"iteration\": 254, \"epoch\": 4}, {\"training_acc\": 0.890625, \"training_loss\": 0.26998212933540344, \"iteration\": 255, \"epoch\": 4}, {\"training_acc\": 0.875, \"training_loss\": 0.37246525287628174, \"iteration\": 256, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.28830793499946594, \"iteration\": 257, \"epoch\": 4}, {\"training_acc\": 0.953125, \"training_loss\": 0.2158486396074295, \"iteration\": 258, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.25792476534843445, \"iteration\": 259, \"epoch\": 4}, {\"training_acc\": 0.8671875, \"training_loss\": 0.2753819227218628, \"iteration\": 260, \"epoch\": 4}, {\"training_acc\": 0.875, \"training_loss\": 0.3329574763774872, \"iteration\": 261, \"epoch\": 4}, {\"training_acc\": 0.8515625, \"training_loss\": 0.37042921781539917, \"iteration\": 262, \"epoch\": 4}, {\"training_acc\": 0.84375, \"training_loss\": 0.4284033477306366, \"iteration\": 263, \"epoch\": 4}, {\"training_acc\": 0.8359375, \"training_loss\": 0.36123180389404297, \"iteration\": 264, \"epoch\": 4}, {\"training_acc\": 0.9375, \"training_loss\": 0.2637079358100891, \"iteration\": 265, \"epoch\": 4}, {\"training_acc\": 0.859375, \"training_loss\": 0.3118065297603607, \"iteration\": 266, \"epoch\": 4}, {\"training_acc\": 0.9140625, \"training_loss\": 0.206180140376091, \"iteration\": 267, \"epoch\": 4}, {\"training_acc\": 0.921875, \"training_loss\": 0.22671422362327576, \"iteration\": 268, \"epoch\": 4}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26817768812179565, \"iteration\": 269, \"epoch\": 4}, {\"training_acc\": 0.9453125, \"training_loss\": 0.23496827483177185, \"iteration\": 270, \"epoch\": 4}, {\"training_acc\": 0.90625, \"training_loss\": 0.2514277398586273, \"iteration\": 271, \"epoch\": 4}, {\"training_acc\": 0.948051948051948, \"training_loss\": 0.21338145434856415, \"iteration\": 272, \"epoch\": 4}, {\"training_acc\": 0.96875, \"training_loss\": 0.13553616404533386, \"iteration\": 273, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1853604018688202, \"iteration\": 274, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17131376266479492, \"iteration\": 275, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1757287085056305, \"iteration\": 276, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11506199836730957, \"iteration\": 277, \"epoch\": 5}, {\"training_acc\": 1.0, \"training_loss\": 0.11190631240606308, \"iteration\": 278, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11648822575807571, \"iteration\": 279, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.17030349373817444, \"iteration\": 280, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10030073672533035, \"iteration\": 281, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.14091841876506805, \"iteration\": 282, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1747509241104126, \"iteration\": 283, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.11911900341510773, \"iteration\": 284, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.12985779345035553, \"iteration\": 285, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.12441091239452362, \"iteration\": 286, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.16187743842601776, \"iteration\": 287, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.14229315519332886, \"iteration\": 288, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.17561545968055725, \"iteration\": 289, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.16763950884342194, \"iteration\": 290, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.14175817370414734, \"iteration\": 291, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20414899289608002, \"iteration\": 292, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.13657227158546448, \"iteration\": 293, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.16212894022464752, \"iteration\": 294, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1679551601409912, \"iteration\": 295, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.14882902801036835, \"iteration\": 296, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12363488972187042, \"iteration\": 297, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.09941083937883377, \"iteration\": 298, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.1890532225370407, \"iteration\": 299, \"epoch\": 5}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10898831486701965, \"iteration\": 300, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.13284330070018768, \"iteration\": 301, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.16223952174186707, \"iteration\": 302, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12609580159187317, \"iteration\": 303, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.17410840094089508, \"iteration\": 304, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.21037611365318298, \"iteration\": 305, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.14884111285209656, \"iteration\": 306, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.13808174431324005, \"iteration\": 307, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1354195773601532, \"iteration\": 308, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.27540430426597595, \"iteration\": 309, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.1824929118156433, \"iteration\": 310, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.12491616606712341, \"iteration\": 311, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1814175695180893, \"iteration\": 312, \"epoch\": 5}, {\"training_acc\": 0.9765625, \"training_loss\": 0.1944107860326767, \"iteration\": 313, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.1377168744802475, \"iteration\": 314, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1584295779466629, \"iteration\": 315, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.23422960937023163, \"iteration\": 316, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.13428370654582977, \"iteration\": 317, \"epoch\": 5}, {\"training_acc\": 0.9140625, \"training_loss\": 0.190909281373024, \"iteration\": 318, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.22078070044517517, \"iteration\": 319, \"epoch\": 5}, {\"training_acc\": 0.953125, \"training_loss\": 0.22093072533607483, \"iteration\": 320, \"epoch\": 5}, {\"training_acc\": 0.9140625, \"training_loss\": 0.17614445090293884, \"iteration\": 321, \"epoch\": 5}, {\"training_acc\": 0.921875, \"training_loss\": 0.23332609236240387, \"iteration\": 322, \"epoch\": 5}, {\"training_acc\": 0.9140625, \"training_loss\": 0.3340538442134857, \"iteration\": 323, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.12416551262140274, \"iteration\": 324, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.17483529448509216, \"iteration\": 325, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.22477176785469055, \"iteration\": 326, \"epoch\": 5}, {\"training_acc\": 0.984375, \"training_loss\": 0.1591503918170929, \"iteration\": 327, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.10302005708217621, \"iteration\": 328, \"epoch\": 5}, {\"training_acc\": 0.8984375, \"training_loss\": 0.26337453722953796, \"iteration\": 329, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.1561943143606186, \"iteration\": 330, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19464077055454254, \"iteration\": 331, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.19240745902061462, \"iteration\": 332, \"epoch\": 5}, {\"training_acc\": 0.921875, \"training_loss\": 0.19363521039485931, \"iteration\": 333, \"epoch\": 5}, {\"training_acc\": 0.9453125, \"training_loss\": 0.20061014592647552, \"iteration\": 334, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.18668049573898315, \"iteration\": 335, \"epoch\": 5}, {\"training_acc\": 0.96875, \"training_loss\": 0.10134174674749374, \"iteration\": 336, \"epoch\": 5}, {\"training_acc\": 0.9296875, \"training_loss\": 0.22805047035217285, \"iteration\": 337, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.19870012998580933, \"iteration\": 338, \"epoch\": 5}, {\"training_acc\": 0.9375, \"training_loss\": 0.21671971678733826, \"iteration\": 339, \"epoch\": 5}, {\"training_acc\": 0.974025974025974, \"training_loss\": 0.12612134218215942, \"iteration\": 340, \"epoch\": 5}, {\"training_acc\": 0.9609375, \"training_loss\": 0.11622670292854309, \"iteration\": 341, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.14814071357250214, \"iteration\": 342, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.1260170042514801, \"iteration\": 343, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08442054688930511, \"iteration\": 344, \"epoch\": 6}, {\"training_acc\": 0.953125, \"training_loss\": 0.11525797843933105, \"iteration\": 345, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.15005835890769958, \"iteration\": 346, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10068732500076294, \"iteration\": 347, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09527640789747238, \"iteration\": 348, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.049461208283901215, \"iteration\": 349, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.12028619647026062, \"iteration\": 350, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08962592482566833, \"iteration\": 351, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.10171905159950256, \"iteration\": 352, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08968451619148254, \"iteration\": 353, \"epoch\": 6}, {\"training_acc\": 0.96875, \"training_loss\": 0.11271220445632935, \"iteration\": 354, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.10210713744163513, \"iteration\": 355, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0821789801120758, \"iteration\": 356, \"epoch\": 6}, {\"training_acc\": 0.9609375, \"training_loss\": 0.1123802587389946, \"iteration\": 357, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09654919058084488, \"iteration\": 358, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0952482521533966, \"iteration\": 359, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06563806533813477, \"iteration\": 360, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09228991717100143, \"iteration\": 361, \"epoch\": 6}, {\"training_acc\": 0.96875, \"training_loss\": 0.12645389139652252, \"iteration\": 362, \"epoch\": 6}, {\"training_acc\": 0.9609375, \"training_loss\": 0.17711615562438965, \"iteration\": 363, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07838702201843262, \"iteration\": 364, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.0646018460392952, \"iteration\": 365, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08543197810649872, \"iteration\": 366, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.09515310823917389, \"iteration\": 367, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05725488439202309, \"iteration\": 368, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08042720705270767, \"iteration\": 369, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08172036707401276, \"iteration\": 370, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.10615652054548264, \"iteration\": 371, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.07276838272809982, \"iteration\": 372, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07260455936193466, \"iteration\": 373, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08592654764652252, \"iteration\": 374, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.14477062225341797, \"iteration\": 375, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0969216376543045, \"iteration\": 376, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05911136418581009, \"iteration\": 377, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06038825958967209, \"iteration\": 378, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.07341749221086502, \"iteration\": 379, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07708606868982315, \"iteration\": 380, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.058706820011138916, \"iteration\": 381, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.04080914333462715, \"iteration\": 382, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.049912504851818085, \"iteration\": 383, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08239606767892838, \"iteration\": 384, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.09351532906293869, \"iteration\": 385, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0733703151345253, \"iteration\": 386, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07376352697610855, \"iteration\": 387, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.08998431265354156, \"iteration\": 388, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.09724893420934677, \"iteration\": 389, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.07352156937122345, \"iteration\": 390, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.06593402475118637, \"iteration\": 391, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.11873185634613037, \"iteration\": 392, \"epoch\": 6}, {\"training_acc\": 0.96875, \"training_loss\": 0.12608186900615692, \"iteration\": 393, \"epoch\": 6}, {\"training_acc\": 0.96875, \"training_loss\": 0.13759198784828186, \"iteration\": 394, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.09950916469097137, \"iteration\": 395, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07256723195314407, \"iteration\": 396, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.14367882907390594, \"iteration\": 397, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.06249886751174927, \"iteration\": 398, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.10961642861366272, \"iteration\": 399, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.07934138923883438, \"iteration\": 400, \"epoch\": 6}, {\"training_acc\": 0.984375, \"training_loss\": 0.10949521511793137, \"iteration\": 401, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.059196073561906815, \"iteration\": 402, \"epoch\": 6}, {\"training_acc\": 0.96875, \"training_loss\": 0.14648431539535522, \"iteration\": 403, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.10261628031730652, \"iteration\": 404, \"epoch\": 6}, {\"training_acc\": 0.9765625, \"training_loss\": 0.08836387097835541, \"iteration\": 405, \"epoch\": 6}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06656702607870102, \"iteration\": 406, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.1067354679107666, \"iteration\": 407, \"epoch\": 6}, {\"training_acc\": 0.987012987012987, \"training_loss\": 0.08832533657550812, \"iteration\": 408, \"epoch\": 6}, {\"training_acc\": 1.0, \"training_loss\": 0.028578953817486763, \"iteration\": 409, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.033774279057979584, \"iteration\": 410, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04733498394489288, \"iteration\": 411, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.055152393877506256, \"iteration\": 412, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.047761231660842896, \"iteration\": 413, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.026257943361997604, \"iteration\": 414, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04013880342245102, \"iteration\": 415, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.07059404253959656, \"iteration\": 416, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.047444187104701996, \"iteration\": 417, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.04641147330403328, \"iteration\": 418, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.02937776781618595, \"iteration\": 419, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04226645082235336, \"iteration\": 420, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.041401248425245285, \"iteration\": 421, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03894379734992981, \"iteration\": 422, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05162258818745613, \"iteration\": 423, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04959343373775482, \"iteration\": 424, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03627852350473404, \"iteration\": 425, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03167394548654556, \"iteration\": 426, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.030826471745967865, \"iteration\": 427, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.05850869417190552, \"iteration\": 428, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03761542588472366, \"iteration\": 429, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.033759210258722305, \"iteration\": 430, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02754424326121807, \"iteration\": 431, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.017891384661197662, \"iteration\": 432, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.022159064188599586, \"iteration\": 433, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06493004411458969, \"iteration\": 434, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03725329414010048, \"iteration\": 435, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.027327558025717735, \"iteration\": 436, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.024415642023086548, \"iteration\": 437, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02653127908706665, \"iteration\": 438, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.044984471052885056, \"iteration\": 439, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.037832122296094894, \"iteration\": 440, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0390472412109375, \"iteration\": 441, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.021695053204894066, \"iteration\": 442, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03798842430114746, \"iteration\": 443, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.04192512854933739, \"iteration\": 444, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.022913560271263123, \"iteration\": 445, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02448957972228527, \"iteration\": 446, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.028674382716417313, \"iteration\": 447, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03264837712049484, \"iteration\": 448, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03669218346476555, \"iteration\": 449, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.03949582204222679, \"iteration\": 450, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03340672701597214, \"iteration\": 451, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.041645873337984085, \"iteration\": 452, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0464579239487648, \"iteration\": 453, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03594883903861046, \"iteration\": 454, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03018096461892128, \"iteration\": 455, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03899316489696503, \"iteration\": 456, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.07044394314289093, \"iteration\": 457, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.010132474824786186, \"iteration\": 458, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.06323418021202087, \"iteration\": 459, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0532374307513237, \"iteration\": 460, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.04906396195292473, \"iteration\": 461, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.053031135350465775, \"iteration\": 462, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.023948699235916138, \"iteration\": 463, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.0637248158454895, \"iteration\": 464, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.044248923659324646, \"iteration\": 465, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.06009231507778168, \"iteration\": 466, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.08936652541160583, \"iteration\": 467, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.022653494030237198, \"iteration\": 468, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.02385881170630455, \"iteration\": 469, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0502081923186779, \"iteration\": 470, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04248351603746414, \"iteration\": 471, \"epoch\": 7}, {\"training_acc\": 0.984375, \"training_loss\": 0.030383866280317307, \"iteration\": 472, \"epoch\": 7}, {\"training_acc\": 0.9921875, \"training_loss\": 0.03225291520357132, \"iteration\": 473, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.0832795798778534, \"iteration\": 474, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.04168810695409775, \"iteration\": 475, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.04500287026166916, \"iteration\": 476, \"epoch\": 7}, {\"training_acc\": 1.0, \"training_loss\": 0.03326135128736496, \"iteration\": 477, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.022172847762703896, \"iteration\": 478, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.013390496373176575, \"iteration\": 479, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010094745084643364, \"iteration\": 480, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011211364530026913, \"iteration\": 481, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009859021753072739, \"iteration\": 482, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.022887174040079117, \"iteration\": 483, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.016186347231268883, \"iteration\": 484, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012535667046904564, \"iteration\": 485, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010606726631522179, \"iteration\": 486, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009406759403645992, \"iteration\": 487, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012040624395012856, \"iteration\": 488, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01860106736421585, \"iteration\": 489, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01027559582144022, \"iteration\": 490, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.015974627807736397, \"iteration\": 491, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0111142722889781, \"iteration\": 492, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009654117748141289, \"iteration\": 493, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011963077820837498, \"iteration\": 494, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011257297359406948, \"iteration\": 495, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011204831302165985, \"iteration\": 496, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007882971316576004, \"iteration\": 497, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00887945108115673, \"iteration\": 498, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01336605567485094, \"iteration\": 499, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014323536306619644, \"iteration\": 500, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007342732045799494, \"iteration\": 501, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011191087774932384, \"iteration\": 502, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011577829718589783, \"iteration\": 503, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009409010410308838, \"iteration\": 504, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.05927875265479088, \"iteration\": 505, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.013222455978393555, \"iteration\": 506, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008400357328355312, \"iteration\": 507, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011737399734556675, \"iteration\": 508, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009491708129644394, \"iteration\": 509, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011930922977626324, \"iteration\": 510, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01779751107096672, \"iteration\": 511, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011552887037396431, \"iteration\": 512, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01592777855694294, \"iteration\": 513, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010165130719542503, \"iteration\": 514, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.00846143439412117, \"iteration\": 515, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.017445258796215057, \"iteration\": 516, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01309035625308752, \"iteration\": 517, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005496368743479252, \"iteration\": 518, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.0075986143201589584, \"iteration\": 519, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011643558740615845, \"iteration\": 520, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014397445134818554, \"iteration\": 521, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011260595172643661, \"iteration\": 522, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01860148459672928, \"iteration\": 523, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.009397084824740887, \"iteration\": 524, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.045131865888834, \"iteration\": 525, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011901461519300938, \"iteration\": 526, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.02103067748248577, \"iteration\": 527, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.017283642664551735, \"iteration\": 528, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.005823325365781784, \"iteration\": 529, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011788713745772839, \"iteration\": 530, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.016045086085796356, \"iteration\": 531, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.011962981894612312, \"iteration\": 532, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.013040555641055107, \"iteration\": 533, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.02760983631014824, \"iteration\": 534, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.008128207176923752, \"iteration\": 535, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.012252329848706722, \"iteration\": 536, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.022443721070885658, \"iteration\": 537, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.014481155201792717, \"iteration\": 538, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.010617751628160477, \"iteration\": 539, \"epoch\": 8}, {\"training_acc\": 0.9921875, \"training_loss\": 0.01602652110159397, \"iteration\": 540, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.013055342249572277, \"iteration\": 541, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.020165173336863518, \"iteration\": 542, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.016824116930365562, \"iteration\": 543, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.01896575465798378, \"iteration\": 544, \"epoch\": 8}, {\"training_acc\": 1.0, \"training_loss\": 0.007282414007931948, \"iteration\": 545, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.008883146569132805, \"iteration\": 546, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.010176174342632294, \"iteration\": 547, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.007013477385044098, \"iteration\": 548, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0072359852492809296, \"iteration\": 549, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004826064687222242, \"iteration\": 550, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0044514089822769165, \"iteration\": 551, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004181440453976393, \"iteration\": 552, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005245002917945385, \"iteration\": 553, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.008689027279615402, \"iteration\": 554, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.01138423290103674, \"iteration\": 555, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0050151655450463295, \"iteration\": 556, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004269247408956289, \"iteration\": 557, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.008089249953627586, \"iteration\": 558, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005551001988351345, \"iteration\": 559, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004066047724336386, \"iteration\": 560, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005313802510499954, \"iteration\": 561, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.010276583023369312, \"iteration\": 562, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003670490114018321, \"iteration\": 563, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005521746817976236, \"iteration\": 564, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00408139917999506, \"iteration\": 565, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002953754970803857, \"iteration\": 566, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003205730812624097, \"iteration\": 567, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00415209261700511, \"iteration\": 568, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004268042743206024, \"iteration\": 569, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004714942071586847, \"iteration\": 570, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.008214278146624565, \"iteration\": 571, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0037946179509162903, \"iteration\": 572, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0026863778475672007, \"iteration\": 573, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004612121731042862, \"iteration\": 574, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.006791356019675732, \"iteration\": 575, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0040375543758273125, \"iteration\": 576, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004954060539603233, \"iteration\": 577, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0037884018383920193, \"iteration\": 578, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004494879860430956, \"iteration\": 579, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0029767954256385565, \"iteration\": 580, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005488279741257429, \"iteration\": 581, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.005738021805882454, \"iteration\": 582, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0027632415294647217, \"iteration\": 583, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004547678865492344, \"iteration\": 584, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0037746166344732046, \"iteration\": 585, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0053331730887293816, \"iteration\": 586, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003356861649081111, \"iteration\": 587, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002740665804594755, \"iteration\": 588, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004607237875461578, \"iteration\": 589, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0034706988371908665, \"iteration\": 590, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004777079913765192, \"iteration\": 591, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003195097204297781, \"iteration\": 592, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.010414057411253452, \"iteration\": 593, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003857304574921727, \"iteration\": 594, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0036298383492976427, \"iteration\": 595, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0032229735516011715, \"iteration\": 596, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.00346518331207335, \"iteration\": 597, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0034404227044433355, \"iteration\": 598, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003577803261578083, \"iteration\": 599, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0028313074726611376, \"iteration\": 600, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004196723457425833, \"iteration\": 601, \"epoch\": 9}, {\"training_acc\": 0.9921875, \"training_loss\": 0.04036179557442665, \"iteration\": 602, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003539511701092124, \"iteration\": 603, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0044722240418195724, \"iteration\": 604, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0026931827887892723, \"iteration\": 605, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004026858601719141, \"iteration\": 606, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.003580666845664382, \"iteration\": 607, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0041235703974962234, \"iteration\": 608, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002841124776750803, \"iteration\": 609, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.002696520183235407, \"iteration\": 610, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0035413354635238647, \"iteration\": 611, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.004185439087450504, \"iteration\": 612, \"epoch\": 9}, {\"training_acc\": 1.0, \"training_loss\": 0.0030708936974406242, \"iteration\": 613, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0028220429085195065, \"iteration\": 614, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0021453923545777798, \"iteration\": 615, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0022251582704484463, \"iteration\": 616, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019569199066609144, \"iteration\": 617, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00176526908762753, \"iteration\": 618, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002588040428236127, \"iteration\": 619, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0023851327132433653, \"iteration\": 620, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0022123027592897415, \"iteration\": 621, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002134832786396146, \"iteration\": 622, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001846736646257341, \"iteration\": 623, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018007154576480389, \"iteration\": 624, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001722150482237339, \"iteration\": 625, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0024197676684707403, \"iteration\": 626, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0025843523908406496, \"iteration\": 627, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019164331024512649, \"iteration\": 628, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019121121149510145, \"iteration\": 629, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0025898627936840057, \"iteration\": 630, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0020025107078254223, \"iteration\": 631, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002441770164296031, \"iteration\": 632, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015673507004976273, \"iteration\": 633, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0021614879369735718, \"iteration\": 634, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0020725822541862726, \"iteration\": 635, \"epoch\": 10}, {\"training_acc\": 0.9921875, \"training_loss\": 0.011770367622375488, \"iteration\": 636, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0028023403137922287, \"iteration\": 637, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018406221643090248, \"iteration\": 638, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00270779337733984, \"iteration\": 639, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019794905092567205, \"iteration\": 640, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017925468273460865, \"iteration\": 641, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015745465643703938, \"iteration\": 642, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00245942291803658, \"iteration\": 643, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00279820105060935, \"iteration\": 644, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014446990098804235, \"iteration\": 645, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0017686665523797274, \"iteration\": 646, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019553140737116337, \"iteration\": 647, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019916133023798466, \"iteration\": 648, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002508973702788353, \"iteration\": 649, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014995405217632651, \"iteration\": 650, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018728472059592605, \"iteration\": 651, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002194991335272789, \"iteration\": 652, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014349017292261124, \"iteration\": 653, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00248921662569046, \"iteration\": 654, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018928342033177614, \"iteration\": 655, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002578021725639701, \"iteration\": 656, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016331274528056383, \"iteration\": 657, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014264187775552273, \"iteration\": 658, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001811091206036508, \"iteration\": 659, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001986742950975895, \"iteration\": 660, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.001976028084754944, \"iteration\": 661, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0025918614119291306, \"iteration\": 662, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00228226650506258, \"iteration\": 663, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0020514437928795815, \"iteration\": 664, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016327716875821352, \"iteration\": 665, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.00173846073448658, \"iteration\": 666, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016394328558817506, \"iteration\": 667, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0020647714845836163, \"iteration\": 668, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0021300569642335176, \"iteration\": 669, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.002028808696195483, \"iteration\": 670, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0019250874174758792, \"iteration\": 671, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014653257094323635, \"iteration\": 672, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016385512426495552, \"iteration\": 673, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0020292720291763544, \"iteration\": 674, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0027594580315053463, \"iteration\": 675, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0014744627987965941, \"iteration\": 676, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0016242264537140727, \"iteration\": 677, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0021263319067656994, \"iteration\": 678, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0015318428631871939, \"iteration\": 679, \"epoch\": 10}, {\"training_acc\": 1.0, \"training_loss\": 0.0018453901866450906, \"iteration\": 680, \"epoch\": 10}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test char tfidf feature on NN\n",
    "train_data_nn = encode_data(train_raw, char_tfidf)\n",
    "test_data_nn = encode_data(test_raw, char_tfidf)\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    num_epochs      = 10,\n",
    "    early_stop      = False,\n",
    "    violation_limit = 5,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(train_data_nn, batch_size=128, shuffle=True)\n",
    "\n",
    "USE_CACHE = False\n",
    "\n",
    "\n",
    "model_nn = NeuralNetwork(\n",
    "    input_size=len(char_tfidf.vocabulary_),\n",
    "    hidden_size=128,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "if (models_dir / 'model_nn.pt').exists() and USE_CACHE:\n",
    "    model_nn = load_model(model_nn, models_dir, 'model_nn')\n",
    "else:\n",
    "    model_nn.fit(dataloader, train_config, disable_progress_bar=False)\n",
    "    save_model(model_nn, models_dir, \"model_nn\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # X_test = torch.stack([dta[0] for dta in test])\n",
    "    X_test = torch.stack([test[0] for test in test_data_nn]).to(model_nn.device)\n",
    "    y_test = torch.stack([test[1] for test in test_data_nn]).to(model_nn.device)\n",
    "    y_pred = model_nn.predict(X_test)\n",
    "\n",
    "\n",
    "print(precision_recall_fscore_support(y_test, y_pred, average='binary'))\n",
    "print(\"AUC\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Plot training accuracy and loss side-by-side\n",
    "plot_results(model_nn, train_config, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-identification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

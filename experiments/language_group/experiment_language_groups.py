# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12pp5yJFBvR5D0Ib50PfvbhFCzte9k1W2
"""

from typing import Optional, Callable
from typing_extensions import Self
from collections.abc import Iterable
from collections import defaultdict
from pathlib import Path

import csv
import torch
from torch.utils.data import Dataset
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from tqdm import tqdm

from scipy.sparse import csr_matrix


class PositionalEncoder(BaseEstimator, TransformerMixin):
    def __init__(
            self,
            vocabulary: Optional[Iterable] = None,
            tokenizer: Optional[Callable] = None,
            no_progress_bar: bool = True,
        ) -> None:
        self.vocabulary = vocabulary
        self.tokenizer = tokenizer or self.build_tokenizer()
        self.max_sentence_length_ = 0
        self.no_progress_bar = no_progress_bar

    def token_to_index(self, token):
        if token in self.vocabulary:
            return self.vocabulary[token]
        else:
            return self.vocabulary['<UNK>']

    def index_to_token(self, index):
        return list(self.vocabulary.keys())[index]

    def fit(self, X: list[str], y: Optional[list] = None):
        """Learn vocabulary from provided sentences and labels
        May need a sentence splitter?
        """
        vocabulary: set = set()
        max_sentence_length = 0

        # Iterate through sentences in the input, tokenize, and update vocabulary
        for sentence in tqdm(X, total=len(X), desc="Fit\t\t", unit="sample", disable=self.no_progress_bar):
            tokens = self.tokenizer(sentence)
            if len(tokens) > max_sentence_length:
                max_sentence_length = len(tokens)

            if self.vocabulary is None:
                for token in tokens:
                    vocabulary.add(token)

        if self.vocabulary is None:
            vocab_list = ['<UNK>'] + list(vocabulary) + ['<PAD>']
            self.vocabulary = {word: idx for idx, word in enumerate(vocab_list)}

        self.max_sentence_length_ = max_sentence_length

        return self

    def transform(self, X, y: Optional[list] = None):

        crow, col, token_val = [], [], []

        # Iterate through sentences, construct the necessary arrays to create CSR sparse matrix
        for i, text in tqdm(enumerate(X), total=len(X), desc="Transform\t", unit="sample", disable=self.no_progress_bar):
            tokens = self.tokenizer(text)
            crow.append(i * self.max_sentence_length_)

            for j, token in enumerate(tokens):
                col.append(j)

                # Append index of tokens and tags to the val arrays
                if token in self.vocabulary:
                    token_val.append(self.token_to_index(token))
                else:
                    token_val.append(self.token_to_index("<UNK>"))

            # Add padding to make all sentences have the same length
            padding_amt = self.max_sentence_length_ - len(tokens)
            token_val += [self.token_to_index("<PAD>")] * padding_amt

            # Column index of the paddings runs from (len of tokens) to max_sentence_length
            col += list(range(len(tokens), self.max_sentence_length_))

        assert len(token_val) == self.max_sentence_length_ * len(X), \
            f"Length of token_val is incorrect: {len(token_val)} != {self.max_sentence_length_ * len(X)}"

        # Construct sparse matrices
        mat_size = (len(X), self.max_sentence_length_)
        tokens_sparse = torch.sparse_csr_tensor(crow, col, token_val, size=mat_size, dtype=torch.long)

        return tokens_sparse.to_dense()

    #TODO: use spacy tokenizer

    def build_tokenizer(self):
        def simple_tokenizer(sentence):
            words: list = sentence.split(' ')
            new_words = []

            for word in words:
                # If there is no punctuation in the word, add to the final list
                # Else, split the words further at the punctuations
                if all([char.isalnum() for char in word]):
                    new_words.append(word)

                else:
                    tmp = ''
                    # Iterate through characters. When encounter a punctuation,
                    # add the previous characters as a word, then add the punctuation
                    for char_idx, char in enumerate(word):
                        if char.isalnum():
                            tmp += char
                            if char_idx == len(word) - 1:
                                new_words.append(tmp)
                        else:
                            if char_idx > 0:
                                new_words.append(tmp)
                            new_words.append(char)
                            tmp = ''
            return new_words
        return simple_tokenizer

    def fit_transform(self, X, y: Optional[list] = None):
        self.fit(X, y)
        tokens = self.transform(X, y)
        return tokens


class RawDataset():
    """Class to hold raw data load directly from the tsv files.
    """
    def __init__(self, ids: list[str], speakers: list[str], texts: list[str], labels: list[int]) -> None:
        assert len(ids) == len(speakers) == len(texts) == len(labels), "All arrays must have the same length"
        self.ids = ids
        self.speakers = speakers
        self.texts = texts
        self.labels = labels

    def subset(self, index_list: list[int]):

        data = RawDataset(
            [self.ids[idx] for idx in index_list],
            [self.speakers[idx] for idx in index_list],
            [self.texts[idx] for idx in index_list],
            [self.labels[idx] for idx in index_list],
        )

        return data

    def __getitem__(self, index: int):
        return (self.ids[index], self.speakers[index], self.texts[index], self.labels[index])

    def __add__(self, other: Self):
        return RawDataset(
            self.ids + other.ids,
            self.speakers + other.speakers,
            self.texts + other.texts,
            self.labels + other.labels
        )

    def __iter__(self):
        for data in zip(self.ids, self.speakers, self.texts, self.labels):
            yield data

    def __len__(self):
        return len(self.ids)


class EncodedDataset(Dataset):
    """Custom Dataset object to hold parliament debate data. Each item in the dataset
    is a tuple of (input tensor, label)
    """
    def __init__(
            self,
            inputs: torch.Tensor,
            labels: torch.Tensor,
        ) -> None:
        super().__init__()
        assert len(inputs) == len(labels), "Inputs and labels have different length"
        self.data_ = list(zip(inputs, labels))

    def __len__(self):
        return len(self.data_)

    def __getitem__(self, index):
        return self.data_[index]

    def __iter__(self):
        for data in self.data_:
            yield data


def load_data(folder_path: str | Path, file_list: list, text_head: str = 'text') -> RawDataset:
    """Load the Parliament Debate dataset.

    Parameters
    ----------
    folder_path : str | Path
        Parent folder containing the text files
    file_list : list
        List of files you want to load
    text_head : str, optional
        Name of the text column, either 'text' or 'text_en', by default 'text'

    Returns
    -------
    list[tuple]
        Returns a list of tuples containing: text ID, speaker ID, text, label
    """
    if isinstance(folder_path, str):
        folder_path = Path(folder_path)

    data = RawDataset([], [], [], [])

    for fname in file_list:
        print(f"Load {fname}...")
        tmp_data = _load_one(file_path=folder_path / fname, text_head=text_head)

        data += tmp_data

    return data


def _load_one(file_path, encoding: str = 'utf-8', text_head: str = 'text') -> RawDataset:
    """Load one file and return """

    ids         : list[str] = []
    speakers    : list[str] = []
    texts       : list[str] = []
    labels      : list[int] = []

    with open(file_path, "rt", encoding=encoding) as f:
        csv_r = csv.DictReader(f, delimiter="\t")

        for row in csv_r:
            ids.append(row.get('id'))
            speakers.append(row.get('speaker'))
            texts.append(row.get(text_head))
            labels.append(int(row.get('label', -1)))

    return RawDataset(ids, speakers, texts, labels)


def split_data(data: RawDataset, test_size=0.2, random_state=None) -> tuple[RawDataset, RawDataset]:
    """Return train-test sets divided by speakers, so that speakers of the train and test set do not overlap"""
    speaker_indices = defaultdict(list)

    # Inverted index: {speaker: [idx1, idx2]}
    for idx, speaker in enumerate(data.speakers):
        speaker_indices[speaker].append(idx)

    # Split list of (indices list)
    train_indices_lst, test_indices_lst = train_test_split(
        list(speaker_indices.values()),
        test_size=test_size,
        random_state=random_state
    )

    train_indices = [idx for lst in train_indices_lst for idx in lst]
    test_indices = [idx for lst in test_indices_lst for idx in lst]

    return data.subset(train_indices), data.subset(test_indices)


def encode_data(raw_data: RawDataset, encoder: PositionalEncoder | TfidfVectorizer):
    """Convenience function to create the final encoded dataset"""
    # Encode text
    enc_texts_csr = encoder.transform(raw_data.texts)

    if isinstance(enc_texts_csr, csr_matrix):
        inputs = torch.from_numpy(enc_texts_csr.todense()).float()
    else:
        inputs = enc_texts_csr.to_dense()

    # Convert labels to tensor
    labels = torch.tensor(raw_data.labels)

    return EncodedDataset(inputs, labels)

import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from sklearn.metrics import precision_recall_fscore_support
from tqdm import tqdm
#from utils import EncodedDataset, PositionalEncoder

from typing import Optional

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Check if CUDA is available

class TrainConfig():
    def __init__(
            self,
            num_epochs: int,
            early_stop: bool,
            violation_limit: int,
            optimizer_params: Optional[dict] = None,
        ) -> None:
        self.optimizer_params   = optimizer_params
        self.num_epochs         = num_epochs
        self.violation_limit    = violation_limit
        self.early_stop        = early_stop


class NeuralNetwork(nn.Module):
    def __init__(self, input_size, num_classes, hidden_size = 64, device = 'cpu'):
        assert device in ['cpu', 'cuda'], "device must be 'cpu' or 'cuda'"

        super().__init__()

        # Define the layers of the neural network
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)

        if device == 'cuda':
            if torch.cuda.is_available():
                self.to('cuda')
                self.device = 'cuda'
            else:
                print("CUDA not available. Run model on CPU")
                self.device = 'cpu'
        else:
            self.device = 'cpu'

    def forward(self, x):
        # Define the forward pass of the neural network
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

    def fit(
        self,
        train_dataloader: DataLoader,
        dev_dataloader: DataLoader,
        train_config: TrainConfig,
    ):
        """Train a neural network model

        Parameters
        ----------
        train_data : EncodedDataset
        dev_data : EncodedDataset
        num_classes : int, optional
            Number of classes to be predicted, by default 2
        hidden_size : int, optional
            Number of hidden nodes, by default 64
        batch_size : int, optional
            Number of samples in a data batch, by default 64
        num_epochs : int, optional
            Number of epochs to train, by default 20
        train_config.violation_limit : int, optional
            Number of epochs to wait when the loss cannot be reduced further, by default 5
        device: str, optional
            Can be either 'cpu' or 'cuda'. Only use `cuda` if your machine has a graphic card supporting CUDA.
        Returns
        -------
        NeuralNetwork
        """
        best_val_loss = float('inf')
        violation_counter = 0
        loss_function = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.parameters(), lr=0.001)  # Adjust learning rate

        for epoch in range(train_config.num_epochs):
            self.train()

            with tqdm(train_dataloader, desc=f"Epoch {epoch + 1}", unit="batch") as train_batches:
                for X_train, y_train in train_batches:
                    X_train = X_train.to(self.device)
                    y_train = y_train.to(self.device)
                    optimizer.zero_grad()
                    outputs = self(X_train)
                    loss = loss_function(outputs, y_train)
                    loss.backward()
                    optimizer.step()

            # Evaluate on validation set for early stopping
            self.eval()
            X_dev = torch.concat([tup[0] for tup in list(dev_dataloader)]).to(self.device)
            y_dev = torch.concat([tup[1] for tup in list(dev_dataloader)]).to(self.device)
            output = self(X_dev)
            val_loss = loss_function(output, y_dev).item()

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                violation_counter = 0
            else:
                violation_counter += 1

            if train_config.early_stop:
                if violation_counter >= train_config.violation_limit:
                    print(f"Validation loss did not improve for {train_config.violation_limit} epochs. Stopping early.")
                    break


def evaluate_nn_model(model: NeuralNetwork, test_dataset: EncodedDataset):
    with torch.no_grad():
        # X_test = torch.stack([dta[0] for dta in test_dataset])
        X_test = torch.stack([test[0] for test in test_dataset]).to(model.device)
        y_test = torch.stack([test[1] for test in test_dataset]).to(model.device)
        y_out = model(X_test)
        y_pred = y_out.argmax(dim=1)
        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
    return precision, recall, f1


class RNNClassifier(nn.Module):
    def __init__(
        self,
        encoder: PositionalEncoder,
        rnn_network: nn.Module = nn.LSTM,
        word_embedding_dim: int = 32,
        class_num: int = 2,
        hidden_dim: int = 64,
        bidirectional: bool = False,
        dropout: float = 0.0,
        device: str = 'cpu'
    ):
        """An RNN based classifier

        Parameters
        ----------
        encoder : PositionalEncoder
        rnn_network : nn.Module, optional
            The network type to be used, can be either nn.LSTM or nn.GRU. By default nn.LSTM
        word_embedding_dim : int, optional
            The dimensionality of the word embedding, by default 32
        hidden_dim : int, optional
            The dimensionality of the hidden state in the RNN, by default 64
        bidirectional : bool, optional
            Specify if the RNN is bi-directional or not, by default False
        dropout : float, optional
            Ratio of random weight drop-out while training, by default 0.0
        device : str, optional
            Device to train the model on, can be either 'cuda' or 'cpu'. By default 'cpu'
        """

        assert rnn_network in [nn.LSTM, nn.GRU], "rnn_network must be nn.LSTM or nn.GRU"
        assert device in ['cpu', 'cuda'], "device must be 'cpu' or 'cuda'"

        super().__init__()
        self.hidden_dim_        = hidden_dim
        self.vocabulary_size_   = len(encoder.vocabulary)
        self.class_num_         = class_num
        self.pad_token_idx_     = encoder.token_to_index('<PAD>')
        self.encoder_           = encoder

        if device == 'cuda':
            if torch.cuda.is_available():
                self.to('cuda')
                self.device = 'cuda'
            else:
                print("CUDA not available. Run model on CPU.")
                self.device = 'cpu'
        else:
            self.device = 'cpu'

        # Initiate the word embedder.
        # It is actually a nn.Linear module with a look up table to return the embedding
        # corresponding to the token's positional index
        self._get_word_embedding = nn.Embedding(
            num_embeddings=self.vocabulary_size_,
            embedding_dim=word_embedding_dim,
            padding_idx=self.pad_token_idx_
        ).to(self.device)

        # Initiate the network
        self._rnn_network = rnn_network(
            input_size=word_embedding_dim,
            hidden_size=hidden_dim,
            batch_first=True,
            bidirectional=bidirectional,
            dropout=dropout
        ).to(self.device)

        # Initiate a linear layer to transform output of _rnn_network to the class space
        # Direction: 1 if uni-directional, 2 if bi-directional
        # This is a binary classification, so only need 1 output unit
        directions = bidirectional + 1
        self._fc = nn.Linear(hidden_dim * directions, 1).to(self.device)

        # Sigmoid to convert output to probability between 0 and 1
        self._sigmoid = nn.Sigmoid()

        # Store loss and accuracy to plot
        self.training_loss_ = list()
        self.training_accuracy_ = list()


    def forward(self, padded_sentences):
        """The forward pass through the network"""
        batch_size, max_sentence_length = padded_sentences.size()
        embedded_sentences = self._get_word_embedding(padded_sentences)

        # Prepare a PackedSequence object, and pass data through the RNN
        # TODO: Why do we need to pack the data this way?
        sentence_lengths = (padded_sentences != self.pad_token_idx_).sum(dim=1)
        sentence_lengths = sentence_lengths.long().cpu()

        packed_input = nn.utils.rnn.pack_padded_sequence(
            input=embedded_sentences,
            lengths=sentence_lengths,
            batch_first=True,
            enforce_sorted=False
        )
        rnn_output, _ = self._rnn_network(packed_input)  # Returned another PackedSequence

        # Unpack the PackedSequence
        unpacked_sequence, _ = nn.utils.rnn.pad_packed_sequence(sequence=rnn_output, batch_first=True)
        unpacked_sequence = unpacked_sequence.contiguous().view(-1, unpacked_sequence.shape[2])

        # Pass data through the fully-connected linear layer
        class_space = self._fc(unpacked_sequence)

        # Reshape data Example size: (64, 2000, 1)
        reshaped = class_space.view(batch_size, max_sentence_length, 1)

        # With RNN, need to collapse the soft prediction (logit) into a one-dimension vector
        # TODO: Ask Fredrik about how to collapse the prediction
        collapsed = torch.stack([reshaped[i, j-1] for i, j in enumerate(sentence_lengths)]).squeeze()

        # sigmoid applied to convert to value between 0 - 1
        # Use sigmoid as output for nn.CrossEntropyLoss()
        scores = self._sigmoid(collapsed)

        return scores.to(self.device)


    def fit(self, train_dataloader: DataLoader, train_config: TrainConfig, no_progress_bar: bool = True) -> None:
        """Training loop for the RNN model. The loop will modify the model itself and returns nothing

        Parameters
        ----------
        train_dataloader : DataLoader
        train_config: TrainConfig
            An object containing various configs for the training loop
        train_encoder : PositionalEncoder
            The encoder providing the vocabulary and tagset for an internal batch_encoder
        Returns
        -------
        """
        # Make sure that the training process do not modify the initial model

        best_lost = float('inf')
        violations = 0
        optimizer = torch.optim.Adam(self.parameters(), **train_config.optimizer_params)
        loss_function = nn.CrossEntropyLoss()

        for epoch in range(train_config.num_epochs):
            with tqdm(
                train_dataloader,
                total   = len(train_dataloader),
                unit    = "batch",
                desc    = f"Epoch {epoch + 1}",
                disable = no_progress_bar,
            ) as batches:

                for ids, speakers, raw_inputs, raw_targets in batches:

                    # Initiate a batch-specific encoder that inherits the vocabulary from the pre-trained encoder
                    # to transform data in the batch.
                    batch_encoder = PositionalEncoder(vocabulary=self.encoder_.vocabulary,)

                    # max_sentence_length_ of each batch are allowed to be varied since it is learned here -> more memory-efficient
                    #
                    train_inputs = batch_encoder.fit_transform(raw_inputs).to(self.device)
                    train_targets = torch.as_tensor(raw_targets, dtype=torch.float).to(self.device)  # nn.CrossEntropyLoss() require target to be float

                    # Reset gradients, then run forward pass
                    self.zero_grad()
                    scores = self(train_inputs)

                    # Calc loss
                    loss = loss_function(scores.view(-1), train_targets.view(-1))

                    # Backward propagation. After each iteration through the batches,
                    # accumulate the gradient for each theta
                    # Run the optimizer to update the parameters
                    loss.backward()
                    optimizer.step()

                    # Evaluate with training batch accuracy
                    pred = scores >= 0.5
                    correct = (pred * 1.0 == train_targets).sum().item()
                    accuracy = correct / len(train_targets)

                    # Save accuracy and loss for plotting
                    self.training_accuracy_.append(accuracy)
                    self.training_loss_.append(loss.item())

                    # Add loss and accuracy info to tqdm's progress bar
                    batches.set_postfix(loss=loss.item(), batch_accuracy=accuracy)

                    # Early stop:
                    if train_config.early_stop:
                        if loss < best_lost:
                            best_lost = loss
                            violations = 0
                        else:
                            violations += 1

                        if violations == train_config.violation_limit:
                            print(f"No improvement for {train_config.violation_limit} epochs. Stop early.")
                            break

import torch
from torch.utils.data import DataLoader
from sklearn.feature_extraction.text import TfidfVectorizer
#from models import NeuralNetwork, TrainConfig, evaluate_nn_model
#from utils import load_data, split_data, encode_data

if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print("Device: cuda")
        print(torch.cuda.get_device_name(i))
else:
    print("Device: cpu")

file_list = [
    #'power-hr-train.tsv',
     #'power-ua-train.tsv',
     #'power-ba-train.tsv',
    #'power-rs-train.tsv'
    #'power-si-train.tsv'
    # 'power-nl-train.tsv',
    'power-fi-train.tsv',
    'power-hu-train.tsv'
]

train_raw = load_data(folder_path="/content/sample_data/", file_list=['power-fi-train.tsv', 'power-hu-train.tsv'],text_head='text_en')
#dev_raw = load_data(folder_path="/content/sample_data/", file_list=['power-ua-train.tsv',],text_head='text_en')
test_raw = load_data(folder_path="/content/sample_data/", file_list=['power-fi-train.tsv', 'power-hu-train.tsv'],text_head='text_en')
full_data = load_data(folder_path="/content/sample_data/", file_list=file_list,text_head='text')

print("Prepare data encoder...")
train_encoder = TfidfVectorizer(sublinear_tf=True, analyzer="char", ngram_range=(1,3))
train_encoder.fit(train_raw.texts)

print("Prepare data...")
train_dataset = encode_data(train_raw, train_encoder)
#dev_dataset = encode_data(dev_raw, train_encoder)
test_dataset = encode_data(test_raw, train_encoder)

print("Prepare data encoder...")
train_encoder = PositionalEncoder()
train_encoder.fit(train_raw.texts)

train_dataloader = DataLoader(train_raw, batch_size=50, shuffle=True)
test_dataloader = DataLoader(test_raw, batch_size=50, shuffle=True)

# Prepare baseline config
train_config = TrainConfig(
    optimizer_params = {'lr': 0.01},
    num_epochs       = 5,
    early_stop       = False,
    violation_limit  = 5
)

# Train baseline model
baseline_lstm = RNNClassifier(
    rnn_network         = nn.LSTM,
    word_embedding_dim  = 32,
    hidden_dim          = 64,
    bidirectional       = True,
    dropout             = 0,
    encoder             = train_encoder,
    device              = 'cuda'
)

# TODO: Ask Fredrik if this is truly the case
# This is slow because LSTM reads in one word in the sentence at a time. The maximum "sentence" length of a batch can be 1000,
# so it does at least 1000 matrix multiplication per batch
baseline_lstm.fit(train_dataloader, train_config, no_progress_bar=False)

baseline_gru = RNNClassifier(
    rnn_network         = nn.GRU,
    word_embedding_dim  = 32,
    hidden_dim          = 64,
    bidirectional       = False,
    dropout             = 0,
    encoder             = train_encoder,
    device              = 'cuda'
)

baseline_gru.fit(train_dataloader, train_config, no_progress_bar=False)

def evaluate_model(
        model: nn.Module | RNNClassifier,
        test_dataloader,
        train_encoder
    ) -> float:
    """Evaluate the model on an inputs-targets set, using accuracy metric.

    Parameters
    ----------
    model : nn.Module
        Should be one of the two custom RNN taggers we defined.
    inputs : torch.Tensor
    targets : torch.Tensor
    pad_tag_idx : int
        Index of the <PAD> tag in the tagset to be ignored when calculating accuracy

    Returns
    -------
    float
        Accuracy metric (ignored the <PAD> tag)
    """
    corrects = []
    total_dpoints = 0
    for ids, speakers, raw_inputs, raw_targets in tqdm(test_dataloader, unit="batch"):

        batch_encoder = PositionalEncoder(vocabulary=train_encoder.vocabulary)
        inputs = batch_encoder.fit_transform(raw_inputs)
        targets = torch.as_tensor(raw_targets, dtype=torch.float).to(model.device)  # nn.CrossEntropyLoss() require target to be float

        # Make prediction
        scores = model(inputs.to(model.device))
        pred = scores > 0.5
        correct = (pred == targets).sum().item()
        corrects.append(correct)
        total_dpoints += len(inputs)

    accuracy = sum(corrects) / total_dpoints

    return accuracy

# Evaluate  model
baseline_lstm_acc = evaluate_model(baseline_lstm, test_dataloader, train_encoder)
print(f"Last train accuracy: {baseline_lstm.training_accuracy_[-1] * 100:.1f}%. Test accuracy {baseline_lstm_acc * 100:.1f}%")

baseline_lstm_gru = evaluate_model(baseline_gru, test_dataloader, train_encoder)
print(f"Last train accuracy: {baseline_gru.training_accuracy_[-1] * 100:.1f}%. Test accuracy {baseline_lstm_gru * 100:.1f}%")

train_config = TrainConfig(
    num_epochs      = 1,
    early_stop      = True,
    violation_limit = 5,
)

model = NeuralNetwork(
    input_size=len(train_encoder.vocabulary_),
    num_classes=2,
    hidden_size=128,
    device='cpu'
)

train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)
dev_dataloader = DataLoader(dev_dataset, batch_size=128, shuffle=True)

model.fit(
    train_dataloader   = train_dataloader,
    dev_dataloader     = dev_dataloader,
    train_config       = train_config
)

precision, recall, f1 = evaluate_nn_model(model, test_dataset)
print(precision, recall, f1)

